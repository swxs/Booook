{"./":{"url":"./","title":"简介","keywords":"","body":"技术笔记 记录一些通识与思考 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/":{"url":"common/","title":"项目","keywords":"","body":"项目 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/":{"url":"common/Linux/","title":"Linux","keywords":"","body":"Linux Linux 文档: 文档: http://www.kernel.org http://filezillaproject.org/download.php http://www.linux-laptop.net/ powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/install.html":{"url":"common/Linux/install.html","title":"安装","keywords":"","body":"安装 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/command.html":{"url":"common/Linux/command.html","title":"命令","keywords":"","body":"命令 命令 系统日志 查看信息 系统资源 磁盘和分区 网络 进程 用户 服务 程序 配置用户 挂载硬盘 删除文件 修改时间 DNS链路 任务调度 系统日志 系统日志是由一个名为syslog的服务管理的，如以下日志文件都是由syslog日志服务驱动的 url 描述 /var/log/boot.log 录了系统在引导过程中发生的事件，就是Linux系统开机自检过程显示的信息 /var/log/lastlog 记录最后一次用户成功登陆的时间、登陆IP等信息 /var/log/messages 记录Linux操作系统常见的系统和服务错误信息 /var/log/secure Linux系统安全日志，记录用户和工作组变坏情况、用户登陆认证情况 /var/log/btmp 记录Linux登陆失败的用户、时间以及远程IP地址 /var/log/syslog 只记录警告信息，常常是系统出问题的信息，使用lastlog查看 /var/log/wtmp 该日志文件永久记录每个用户登录、注销及系统的启动、停机的事件，使用last命令查看 /var/run/utmp 该日志文件记录有关当前登录的每个用户的信息。如 who、w、users、finger等就需要访问这个文件 查看信息 系统资源 uname -a # 查看内核/操作系统/CPU信息 head -n 1 /etc/issue # 查看操作系统版本 rpm -q centos-release # Centos 版本 rpm -q redhat-release # Redhat 版本 cat /proc/version # Linux内核版本 cat /proc/cpuinfo # 查看CPU信息 cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c cat /proc/cpuinfo | grep physical | uniq -c dmidecode | grep 'Processor Information' # 完整查看cpu详细信息 dmidecode | grep \"Product Name\" # 查看机器型号 hostname # 查看计算机名 lspci -tv # 列出所有PCI设备 lsusb -tv # 列出所有USB设备 lsmod # 列出加载的内核模块 env # 查看环境变量 free -m # 查看内存使用量和交换区使用量 df -h # 查看各分区使用情况 du -sh # 查看指定目录的大小 cat /proc/meminfo # 查看内存信息 grep MemTotal /proc/meminfo # 查看内存总量 grep MemFree /proc/meminfo # 查看空闲内存量 uptime # 查看系统运行时间、用户数、负载 cat /proc/loadavg # 查看系统负载 磁盘和分区 mount | column -t # 查看挂接的分区状态 fdisk -l # 查看所有分区 swapon -s # 查看所有交换分区 hdparm -i /dev/hda # 查看磁盘参数(仅适用于IDE设备) dmesg | grep IDE # 查看启动时IDE设备检测状况 网络 dmesg | grep -i eth # 查看网卡信息 ifconfig # 查看所有网络接口的属性 iptables -L # 查看防火墙设置 route -n # 查看路由表 netstat -lntp # 查看所有监听端口 netstat -antp # 查看所有已经建立的连接 netstat -s # 查看网络统计信息 awk '{print $7}' access.log|sort | uniq -c |sort -n -k 1 -r|more ## 查询访问最频繁的URL awk '{print $1}' access.log|sort | uniq -c |sort -n -k 1 -r|more ## ## 查询访问最频繁的IP 进程 ps -ef # 查看所有进程 top # 实时显示进程状态 用户 w # 查看活动用户 id # 查看指定用户信息 last # 查看用户登录日志 cut -d: -f1 /etc/passwd # 查看系统所有用户 cut -d: -f1 /etc/group # 查看系统所有组 crontab -l # 查看当前用户的计划任务 服务 chkconfig --list # 列出所有系统服务 chkconfig --list | grep on # 列出所有启动的系统服务 程序 rpm -qa # 查看所有安装的软件包 配置用户 # 配置www用户 groupadd www useradd --shell /sbin/nologin -g www www usermod -s /bin/bash www mkdir -p /data/www chown www:www /data/www su www vim ~/.bashrc cd /data/www 挂载硬盘 fdisk -l mkfs.ext4 /dev/sdc mkdir -p /home/data mount /dev/sdc /home/data vi /etc/fstab /dev/sdc /home/data ext4 defaults 0 0 删除文件 # 保留指定数量的文件 TOTAL_COUNT=`ls|wc -l` LEFT_COUNT=20 OLD_VERSION=`ll -rt | head -n $[$TOTAL_COUNT-$LEFT_COUNT]` rm -rf $OLD_VERSION # 删除7天前的文件 find /mnt/tmp_bi/ -type f -mtime +7 -exec rm -f {} \\; ll | awk '{if($7 修改时间 # 修改时间 cp /etc/localtime /etc/localtime.bak ln -svf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime vim /etc/sysconfig/clock ZONE=\"Asia/Shanghai\" UTC=false ARC=false ntpdate 0.centos.pool.ntp.org /etc/init.d/crond restart date -R DNS链路 #! /bin/bash set -x set -e dig www.baidu.com dig +short www.baidu.com dig +trace www.baidu.com dig -x www.baidu.com dig ns com host www.baidu.com 任务调度 # 查看任务 jobs -l ps aux # 任务前后台切换 fg % bg % # 防止任务在退出命令行时被自动kill nohup &1> & disown -h % # 停止进程 kill -9 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/":{"url":"common/Linux/tool/","title":"工具","keywords":"","body":"工具 工具 文档 文档 https://wangchujiang.com/linux-command/ powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/apt-get.html":{"url":"common/Linux/tool/apt-get.html","title":"apt-get","keywords":"","body":"apt-get apt-get 补充说明 语法 选项 参数 实例 更新软件包列表 安装一个新软件包： 卸载一个已安装的软件包（保留配置文件）： 卸载一个已安装的软件包（删除配置文件）： 这个命令会把安装的软件的备份也删除，不过这样不会影响软件的使用的： 更新所有已安装的软件包： 将系统升级到新版本： 清除卸载的软件包 Debian Linux发行版中的APT软件包管理工具 补充说明 apt-get命令 是Debian Linux发行版中的APT软件包管理工具。所有基于Debian的发行都使用这个包管理系统。deb包可以把一个应用的文件包在一起，大体就如同Windows上的安装文件。 语法 apt-get [OPTION] PACKAGE 选项 apt-get install # 安装新包 apt-get remove # 卸载已安装的包（保留配置文件） apt-get purge # 卸载已安装的包（删除配置文件） apt-get update # 更新软件包列表 apt-get upgrade # 更新所有已安装的包 apt-get autoremove # 卸载已不需要的包依赖 apt-get dist-upgrade # 自动处理依赖包升级 apt-get autoclean # 将已经删除了的软件包的.deb安装文件从硬盘中删除掉 apt-get clean # 删除软件包的安装包 -c：指定配置文件。 参数 管理指令：对APT软件包的管理操作； 软件包：指定要操纵的软件包。 实例 使用apt-get命令的第一步就是引入必需的软件库，Debian的软件库也就是所有Debian软件包的集合，它们存在互联网上的一些公共站点上。把它们的地址加入，apt-get就能搜索到我们想要的软件。/etc/apt/sources.list是存放这些地址列表的配置文件，其格式如下： deb web或[ftp地址] [发行版名字] main/contrib/non-[free] 我们常用的Ubuntu就是一个基于Debian的发行，我们使用apt-get命令获取这个列表，以下是我整理的常用命令： 更新软件包列表 在修改/etc/apt/sources.list或者/etc/apt/preferences之后运行该命令。此外您需要定期运行这一命令以确保您的软件包列表是最新的： apt-get update 安装一个新软件包： apt-get install 卸载一个已安装的软件包（保留配置文件）： apt-get remove 卸载一个已安装的软件包（删除配置文件）： apt-get –purge remove 这个命令会把安装的软件的备份也删除，不过这样不会影响软件的使用的： apt-get clean 更新所有已安装的软件包： apt-get upgrade 将系统升级到新版本： apt-get dist-upgrade 清除卸载的软件包 定期运行这个命令来清除那些已经卸载的软件包的.deb文件。通过这种方式，您可以释放大量的磁盘空间。如果您的需求十分迫切，可以使用apt-get clean以释放更多空间。这个命令会将已安装软件包裹的.deb文件一并删除。大多数情况下您不会再用到这些.debs文件，因此如果您为磁盘空间不足 而感到焦头烂额，这个办法也许值得一试： apt-get autoclean 会把已装或已卸的软件都备份在硬盘上，所以如果需要空间的话，可以让这个命令来删除你已经删掉的软件： apt-get autoclean apt powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/awk.html":{"url":"common/Linux/tool/awk.html","title":"awk","keywords":"","body":"awk awk 简介 基本语法 内建变量 模式 示例 简介 awk是一种编程语言，用于在linux/unix下对文本和数据进行处理。 基本语法 awk 'pattern {action}' 内建变量 条件 解释 $0 当前记录（这个变量中存放着整个行的内容） $n 当前记录的第n个字段，字段间由FS分隔 FS 输入字段分隔符 默认是空格或Tab NF 当前记录中的字段个数，就是有多少列 NR 已经读出的记录数，就是行号，从1开始，如果有多个文件话，这个值也是不断累加中。 FNR 当前记录数，与NR不同的是，这个值会是各个文件自己的行号 RS 输入的记录分隔符， 默认为换行符 OFS 输出字段分隔符， 默认也是空格 ORS 输出的记录分隔符，默认为换行符 FILENAME 当前输入文件的名字 模式 模式 解释 举例 正则表达式 使用通配符的扩展集 /abc/ ， $1 ~ /abc/ 比较表达式 满足条件的行， $2>$1 常量表达式 0为假，非0为真 1 BEGIN 开始 END 结束 模式范围 begpat, endpat 模式1~模式2 示例 # 条件聚合 cat 1.csv | awk -F ',' '$1~/[ab]/ {g[$1]+=$2*$3} END {for(l in g) printf \"%s = %d\\n\",l,g[l]}' # ls awk 'BEGIN{while( \"ls\" | getline) print}' powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/curl.html":{"url":"common/Linux/tool/curl.html","title":"curl","keywords":"","body":"curl curl header header 我们可以通过传递 -i 参数给 curl 命令，该参数 能够显示响应的头部 curl -i -XGET http://localhost:9200/website/blog/124?pretty powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/du.html":{"url":"common/Linux/tool/du.html","title":"du","keywords":"","body":"du du 补充说明 语法 选项 实例 文件从大到小排序 只显示当前目录下子目录的大小。 查看指定目录下文件所占的空间： 只显示总和的大小: 显示每个文件和目录的磁盘使用空间 补充说明 du命令 也是查看使用空间的，但是与df命令不同的是Linux du命令是对文件和目录磁盘使用的空间的查看，还是和df命令有一些区别的。 语法 du [选项][文件] 选项 -a, --all 显示目录中个别文件的大小。 -B, --block-size=大小 使用指定字节数的块 -b, --bytes 显示目录或文件大小时，以byte为单位。 -c, --total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和。 -D, --dereference-args 显示指定符号链接的源文件大小。 -H, --si 与-h参数相同，但是K，M，G是以1000为换算单位。 -h, --human-readable 以K，M，G为单位，提高信息的可读性。 -k, --kilobytes 以KB(1024bytes)为单位输出。 -l, --count-links 重复计算硬件链接的文件。 -m, --megabytes 以MB为单位输出。 -L, --dereference 显示选项中所指定符号链接的源文件大小。 -P, --no-dereference 不跟随任何符号链接(默认) -0, --null 将每个空行视作0 字节而非换行符 -S, --separate-dirs 显示个别目录的大小时，并不含其子目录的大小。 -s, --summarize 仅显示总计，只列出最后加总的值。 -x, --one-file-xystem 以一开始处理时的文件系统为准，若遇上其它不同的文件系统目录则略过。 -X, --exclude-from= 在指定目录或文件。 --apparent-size 显示表面用量，而并非是磁盘用量；虽然表面用量通常会小一些，但有时它会因为稀疏文件间的\"洞\"、内部碎片、非直接引用的块等原因而变大。 --files0-from=F 计算文件F中以NUL结尾的文件名对应占用的磁盘空间如果F的值是\"-\"，则从标准输入读入文件名 --exclude= 略过指定的目录或文件。 --max-depth=N 显示目录总计(与--all 一起使用计算文件)当N为指定数值时计算深度为N，等于0时等同--summarize --si 类似-h，但在计算时使用1000 为基底而非1024 --time 显示目录或该目录子目录下所有文件的最后修改时间 --time=WORD 显示WORD时间，而非修改时间：atime，access，use，ctime 或status --time-style=样式 按照指定样式显示时间(样式解释规则同\"date\"命令)：full-iso，long-iso，iso，+FORMAT --help 显示此帮助信息并退出 --version 显示版本信息并退出 实例 文件从大到小排序 du -sh * |sort -rh 2.9M command 1.9M assets 148K template 72K package-lock.json 52K dist 28K build 16K README.md 4.0K renovate.json 4.0K package.json 4.0K LICENSE 只显示当前目录下子目录的大小。 du -sh ./*/ 1.9M ./assets/ 28K ./build/ 2.9M ./command/ 52K ./dist/ 148K ./template/ 查看指定目录下文件所占的空间： du ./* 144 ./alfred.png 452 ./chrome-extensions.gif 4 ./dash-icon.png 1312 ./Linux.gif 16 ./qr.png 只显示总和的大小: du -s . 1932 . powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/find.html":{"url":"common/Linux/tool/find.html","title":"find","keywords":"","body":"find find 场景 命令 实例 查找指定名字的文件并删除 删除7天前的文件 文档 场景 寻找指定文件 命令 find [-H] [-L] [-P] [-D debugopts] [-Olevel] [path...] [expression] 实例 查找指定名字的文件并删除 find . -maxdepth 1 -regex \"\\.\\/[0-9a-f]*_[0-9]*_[0-9a-f]*\\.h5\" -exec rm -f {} +; 删除7天前的文件 find . -type f -mtime +7 -exec rm -f {} +; 文档 正则语法说明: https://www.gnu.org/software/findutils/manual/html_node/find_html/Regular-Expressions.html powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/imail.html":{"url":"common/Linux/tool/imail.html","title":"imail","keywords":"","body":"imail imail 格式 格式 telnet mail.bpsinopec.com 25 cem@bpsinopec.com helo wenjuan.com mail from: [email] rcpt to: [email] rcpt to: [email] data subject:Hello, there! hello, this is a test. . powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/install.html":{"url":"common/Linux/tool/install.html","title":"install","keywords":"","body":"install install 补充说明 语法 选项 实例 安装或升级软件或备份数据 补充说明 install命令的作用是安装或升级软件或备份数据，它的使用权限是所有用户。install命令和cp命令类似，都可以将文件/目录拷贝到指定的地点。但是，install允许你控制目标文件的属性。install通常用于程序的makefile，使用它来将程序拷贝到目标（安装）目录。 语法 install [OPTION]... [-T] SOURCE DEST install [OPTION]... SOURCE... DIRECTORY install [OPTION]... -t DIRECTORY SOURCE... install [OPTION]... -d DIRECTORY... 在前两种格式中，会将复制至或将多个文件复制至已存在的，同时设定权限模式及所有者/所属组。在第三种格式中，会创建所有指定的目录及它们的主目录。长选项必须用的参数在使用短选项时也是必须的。 选项 --backup[=CONTROL]：为每个已存在的目的地文件进行备份。 -b：类似 --backup，但不接受任何参数。 -d，--directory：所有参数都作为目录处理，而且会创建指定目录的所有主目录。 -D：创建前的所有主目录，然后将复制至 ；在第一种使用格式中有用。 -g，--group=组：自行设定所属组，而不是进程目前的所属组。 -m，--mode=模式：自行设定权限模式 (像chmod)，而不是rwxr-xr-x。 -o，--owner=所有者：自行设定所有者 (只适用于超级用户)。 -p，--preserve-timestamps：以文件的访问/修改时间作为相应的目的地文件的时间属性。 -s，--strip：用strip命令删除symbol table，只适用于第一及第二种使用格式。 -S，--suffix=后缀：自行指定备份文件的。 -v，--verbose：处理每个文件/目录时印出名称。 --help：显示此帮助信息并离开。 --version：显示版本信息并离开。 实例 install -d [option] DIRECTORY [DIRECTORY...] 支持多个，类似mkdir -p支持递归。 例如：install -d a/b/c e/f结果和mkdir -p a/b/c e/f一样。 install [option] SOURCE DEST 复制SOURCE（文件）到 DEST（文件）： 有用选项-D： install -D x a/b/c 效果类似： mkdir -p a/b && cp x a/b/c install [option] SOURCE [SOURCE...] DIRECTORY 复制多个SOURCE（文件）到 DIRECTORY（目录）： install a/* d 其中d是目录。 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/iptables.html":{"url":"common/Linux/tool/iptables.html","title":"iptables","keywords":"","body":"iptables iptables 流程 安全配置 流程 修改/etc/sysconfig/iptables 重运行iptables服务 防火墙开启 vi /usr/local/sbin/firewall #!/bin/bash iptables -P INPUT ACCEPT iptables -F iptables -X iptables -Z iptables -A INPUT -i lo -j ACCEPT iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT iptables -A INPUT -p icmp -j ACCEPT iptables -A INPUT -p tcp --dport 22 -j ACCEPT iptables -A INPUT -p tcp --dport 80 -j ACCEPT iptables -A INPUT -p tcp --dport 443 -j ACCEPT iptables -P INPUT DROP iptables -P OUTPUT ACCEPT iptables -P FORWARD ACCEPT iptables -t nat -F iptables -t nat -X iptables -t nat -Z iptables -t nat -P PREROUTING ACCEPT iptables -t nat -P POSTROUTING ACCEPT iptables -t nat -P OUTPUT ACCEPT service iptables save service iptables restart chkconfig iptables on 安全配置 vi /etc/init.d/iptables （set_policy() { 段） security) $IPTABLES -t filter -P INPUT $policy \\ && $IPTABLES -t filter -P OUTPUT $policy \\ && $IPTABLES -t filter -P FORWARD $policy \\ || let ret+=1 ;; firewall-cmd --add-port=5000/tcp --zone=public --permanent firewall-cmd --reload 配置logrotate /data/log/nginx/*.log { daily rotate 365 compress dateext missingok notifempty sharedscripts postrotate /bin/kill -USR1 `cat /usr/local/nginx/logs/nginx.pid 2>/dev/null` 2>/dev/null || ture endscript } service iptables restart powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/kafkacat.html":{"url":"common/Linux/tool/kafkacat.html","title":"kafkacat","keywords":"","body":"kafkacat kafkacat 安装 常用命令 查询信息列表 起一个消费端 起一个发送端 源码 安装 apt-get install kafkacat 常用命令 查询信息列表 kafkacat -b cdh-worker001:9092 -L 起一个消费端 kafkacat -C -b cdh-worker001:9092 -t [topic] 起一个发送端 kafkacat -P -b cdh-worker001:9092 -t [topic] 源码 https://github.com/edenhill/kafkacat powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/kill.html":{"url":"common/Linux/tool/kill.html","title":"kill","keywords":"","body":"kill kill 强制关闭进程 强制关闭进程 ps aux|grep $2|grep -v grep|awk '{print $2}'|xargs kill -9 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/lftp.html":{"url":"common/Linux/tool/lftp.html","title":"lftp","keywords":"","body":"lftp lftp 示例 示例 # SYNTAX # lftp [-d] [-e cmd] [-p port] [-u user[,pass]] [site] # lftp -f script_file # lftp -c commands # lftp --version # lftp --help lftp -p 990 -u , ftps://host set ftp:ssl-auth TLS set ftp:ssl-force true set ftp:ssl-protect-list yes set ftp:ssl-protect-data yes set ftp:ssl-protect-fxp yes set ftp:passive-mode off set ssl:verify-certificate no mput filename; powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/lsof.html":{"url":"common/Linux/tool/lsof.html","title":"lsof","keywords":"","body":"lsof lsof 安装 命令 示例 查询一个文件的打开状态 查询当前文件下所有文件的状态 安装 yum install -y lsof 命令 来自lsof -h 示例 查询一个文件的打开状态 lsof {path} 查询当前文件下所有文件的状态 lsof +D ./ powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/rpm.html":{"url":"common/Linux/tool/rpm.html","title":"RPM","keywords":"","body":"RPM RPM 介绍 APT-GET 介绍 在Linux 操作系统中，有一个系统软件包，它的功能类似于Windows里面的“添加/删除程序”，但是功能又比“添加/删除程序”强很多，它就是 Red Hat Package Manager(简称RPM)。此工具包最先是由Red Hat公司推出的，后来被其他Linux开发商所借用。由于它为Linux使用者省去了很多时间，所以被广泛应用于在Linux下安装、删除软件。下面就 给大家介绍一下它的具体使用方法。 我们得到一个新软件，在安装之前，一般都要先查看一下这个软件包里有什么内容，假设这个文件是：Linux-1.4-6.i368.rpm，我们可以用这条命令查看：rpm -qpi Linux-1.4-6.i368.rpm系统将会列出这个软件包的详细资料，包括含有多少个文件、各文件名称、文件大小、创建时间、编译日期等信息。 上面列出的所有文件在安装时不一定全部安装，就像Windows下程序的安装方式分为典型、完全、自定义一样，Linux也会让你选择安装方式，此时我们可以用下面这条命令查看软件包将会在系统里安装哪些部分，以方便我们的选择：rpm -qpl Linux-1.4-6.i368.rpm 选择安装方式后，开始安装。我们可以用rpm-ivh Linux-1.4-6.i368.rpm命令安装此软件。在安装过程中，若系统提示此软件已安装过或因其他原因无法继续安装，但若我们确实想执行安装命 令，可以在 -ivh后加一参数“-replacepkgs”：rpm -ivh -replacepkgs Linux-1.4-6.i368.rpm 有时我们卸载某个安装过的软件，只需执行rpm-e ;命令即可。 对低版本软件进行升级是提高其功能的好办法，这样可以省去我们卸载后再安装新软件的麻烦，要升级某个软件，只须执行如下命令：rpm -uvh ;，注意：此时的文件名必须是要升级软件的升级补丁 另外一个安装软件的方法可谓是Linux的独到之处，同时也是RMP强大功能的一个表现：通过FTP站点直接在线安装软件。当找到含有你所需软件的站点并 与此网站连接后，执行下面的命令即可实现在线安装，譬如在线安装Linux-1.4-6.i368.rpm，可以用命令：rpm -i ftp://ftp.pht.com/pub/linux/redhat/...-1.4-6.i368.rpm 在我们使用电脑过程中，难免会有误操作，若我们误删了几个文件而影响了系统的性能时，怎样查找到底少了哪些文件呢?RPM软件包提供了一个查找损坏文件的 功能，执行此命令：rpm -Va即可，Linux将为你列出所有损坏的文件。你可以通过Linux的安装光盘进行修复。 Linux系统中文件繁多，在使用过程中，难免会碰到我们不认识的文件，在Windows下我们可以用“开始/查找”菜单快速判断某个文件属于哪个文件夹，在Linux中，下面这条命令行可以帮助我们快速判定某个文件属于哪个软件包：rpm -qf ; 当每个软件包安装在Linux系统后，安装文件都会到RPM数据库中“报到”，所以，我们要查询某个已安装软件的属性时，只需到此数据库中查找即可。注意：此时的查询命令不同于1和8介绍的查询，这种方法只适用于已安装过的软件包！命令格式：rpm -参数　; APT-GET apt-get update——在修改/etc/apt/sources.list或者/etc/apt/preferences之后运行该命令。此外您需要定期运行这一命令以确保您的软件包列表是最新的。 apt-get install packagename——安装一个新软件包（参见下文的aptitude） apt-get remove packagename——卸载一个已安装的软件包（保留配置文件） apt-get --purge remove packagename——卸载一个已安装的软件包（删除配置文件） dpkg --force-all --purge packagename 有些软件很难卸载，而且还阻止了别的软件的应用，就可以用这个，不过有点冒险。 apt-get autoclean apt会把已装或已卸的软件都备份在硬盘上，所以如果需要空间的话，可以让这个命令来删除你已经删掉的软件 apt-get clean 这个命令会把安装的软件的备份也删除，不过这样不会影响软件的使用的。 apt-get upgrade——更新所有已安装的软件包 apt-get dist-upgrade——将系统升级到新版本 apt-cache search string——在软件包列表中搜索字符串 dpkg -l package-name-pattern——列出所有与模式相匹配的软件包。如果您不知道软件包的全名，您可以使用“*package-name-pattern*”。 aptitude——详细查看已安装或可用的软件包。与apt-get类似，aptitude可以通过命令行方式调用，但仅限于某些命令——最常见的有安装和卸载命令。由于aptitude比apt-get了解更多信息，可以说它更适合用来进行安装和卸载。 apt-cache showpkg pkgs——显示软件包信息。 apt-cache dumpavail——打印可用软件包列表。 apt-cache show pkgs——显示软件包记录，类似于dpkg –print-avail。 apt-cache pkgnames——打印软件包列表中所有软件包的名称。 dpkg -S file——这个文件属于哪个已安装软件包。 dpkg -L package——列出软件包中的所有文件。 apt-file search filename——查找包含特定文件的软件包（不一定是已安装的），这些文件的文件名中含有指定的字符串。apt-file是一个独立的软件包。您必须 先使用apt-get install来安装它，然后运行apt-file update。如果apt-file search filename输出的内容太多，您可以尝试使用apt-file search filename | grep -w filename（只显示指定字符串作为完整的单词出现在其中的那些文件名）或者类似方法，例如：apt-file search filename | grep /bin/（只显示位于诸如/bin或/usr/bin这些文件夹中的文件，如果您要查找的是某个特定的执行文件的话，这样做是有帮助的）。 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/ssh.html":{"url":"common/Linux/tool/ssh.html","title":"SSH","keywords":"","body":"SSH SSH 免密登录 多私钥配置 免密登录 生成私钥 ssh-keygen -t rsa 复制公钥到远程服务器 ssh-copy-id -i ~/.ssh/id_rsa.pub root@127.0.0.1 重启ssh服务systemctl restart sshd.service 多私钥配置 ssh-agent bash # ssh-add 私钥名字 ssh-add id_rsa # 查询私钥列表 ssh-add -l # 清空私钥列表 ssh-add -D powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/system.html":{"url":"common/Linux/tool/system.html","title":"system","keywords":"","body":"system system 命令 实例 命令 # 脚本注册 systemctl daemon-reload # 启动 systemctl start cem.service # 停止 systemctl stop cem.service # 重启 systemctl reload cem.service # 注册开机自启动 systemctl enable cem.service 实例 cem.service [Unit] Description=cem-base service [Service] Type=forking ExecStart=/data/www/cem-base/restart_all.sh ExecStop=/data/www/cem-base/kill_all.sh ExecReload=/data/www/cem-base/restart_all.sh User=www Group=www [Install] WantedBy=multi-user.target powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/tcpdump.html":{"url":"common/Linux/tool/tcpdump.html","title":"tcpdump","keywords":"","body":"tcpdump powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/tmux.html":{"url":"common/Linux/tool/tmux.html","title":"tmux","keywords":"","body":"tmux tmux 命令 常用配置 命令 快捷键 命令 效果 C-b Send the prefix key (C-b) through to the application. C-o Rotate the panes in the current window forwards. C-z Suspend the tmux client. ! Break the current pane out of the window. \" Split the current pane into two, top and bottom. # List all paste buffers. $ Rename the current session. % Split the current pane into two, left and right. & Kill the current window. ' Prompt for a window index to select. ( Switch the attached client to the previous session. ) Switch the attached client to the next session. , Rename the current window. - Delete the most recently copied buffer of text. . Prompt for an index to move the current window. 0 to 9 Select windows 0 to 9. : Enter the tmux command prompt. ; Move to the previously active pane. = Choose which buffer to paste interactively from a list. ? List all key bindings. D Choose a client to detach. L Switch the attached client back to the last session. [ Enter copy mode to copy text or view the history. ] Paste the most recently copied buffer of text. c Create a new window. d Detach the current client. f Prompt to search for text in open windows. i Display some information about the current window. l Move to the previously selected window. n Change to the next window. o Select the next pane in the current window. p Change to the previous window. q Briefly display pane indexes. r Force redraw of the attached client. m Mark the current pane (see select-pane -m). M Clear the marked pane. s Select a new session for the attached client interactively. t Show the time. w Choose the current window interactively. x Kill the current pane. z Toggle zoom state of the current pane. { Swap the current pane with the previous pane. } Swap the current pane with the next pane. ~ Show previous messages from tmux, if any. Page Up Enter copy mode and scroll one page up. Up, Down, Left, Right Change to the pane above, below, to the left, or to the right of the current pane. 常用配置 vim ~/.bashrc alias tmuxl='tmux select-pane -L' alias tmuxr='tmux select-pane -R' alias tmuxu='tmux select-pane -U' alias tmuxd='tmux select-pane -D' alias tmuxs='tmux switch -t ' alias tmuxa='tmux attach -t ' vim ~/.tmux.conf # 设置鼠标滚动 set -g mouse on powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/ps.html":{"url":"common/Linux/tool/ps.html","title":"ps","keywords":"","body":"ps powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/vim.html":{"url":"common/Linux/tool/vim.html","title":"vim","keywords":"","body":"vim vim 替换 tab替换 vimrc 替换 :[%]s/parttern/replace/[range] tab替换 :set ts=4 :set expandtab :set noexpandtab :%retab! vimrc \" must be first, changes behaviour of other settings set nocompatible \" 256 colors set t_Co=256 \" sane text files set fileformat=unix set encoding=utf-8 \" sane tabs set tabstop=4 set shiftwidth=4 set softtabstop=4 \" convert all typed tabs to spaces set expandtab \" syntax highlighting syntax on color koehler \"make sure highlighting works all the way down long files autocmd BufEnter * :syntax sync fromstart \" allow cursor to be positioned one char past end of line \" and apply operations to all of selection including last char set selection=exclusive \" allow backgrounding buffers without writing them \" and remember marks/undo for backgrounded buffers set hidden \" Keep more context when scrolling off the end of a buffer set scrolloff=3 \" allow cursor keys to go right off end of one line, onto start of next set whichwrap+=,[,] \" allow backspacing over everything in insert mode set backspace=indent,eol,start \" no line wrapping set nowrap \" line numbers set number \" when joining lines, don't insert two spaces after punctuation set nojoinspaces \" Make searches case-sensitive only if they contain upper-case characters set ignorecase set smartcase \" show search matches as the search pattern is typed set incsearch \" search-next wraps back to start of file set wrapscan \" highlight last search matches set hlsearch \" map key to dismiss search highlightedness map :noh \" grep for word under cursor noremap g :grep -rw '' . \" aliases for window switching (browser captures ctrl-w) noremap l noremap h noremap k noremap j \" similarly ctrl-q doesnt work, so use leader-q for block visual mode nnoremap q \" make tab completion for files/buffers act like bash set wildmenu \" display cursor co-ords at all times set ruler set cursorline \" display number of selected chars, lines, or size of blocks. set showcmd \" show matching brackets, etc, for 1/10th of a second set showmatch set matchtime=1 \" enables filetype specific plugins filetype plugin on \" enables filetype detection filetype on if has(\"autocmd\") \" Enable file type detection. \" Use the default filetype settings, so that mail gets 'tw' set to 72, \" 'cindent' is on in C files, etc. \" Also load indent files, to automatically do language-dependent indenting. filetype plugin indent on \" When editing a file, always jump to the last known cursor position. \" Don't do it when the position is invalid or when inside an event handler \" (happens when dropping a file on gvim). autocmd BufReadPost * \\ if line(\"'\\\"\") > 0 && line(\"'\\\"\") inoremap \" =====STATUS LINE OF DEATH!!===== set statusline= \" filename, relative to cwd set statusline+=%f \" separator set statusline+=\\ \" modified flag set statusline+=%#wildmenu# set statusline+=%m set statusline+=%* \"Display a warning if file encoding isnt utf-8 set statusline+=%#question# set statusline+=%{(&fenc!='utf-8'&&&fenc!='')?'['.&fenc.']':''} set statusline+=%* \"display a warning if fileformat isnt unix set statusline+=%#directory# set statusline+=%{&ff!='unix'?'['.&ff.']':''} set statusline+=%* \"display a warning if files contains tab chars set statusline+=%#warningmsg# set statusline+=%{StatuslineTabWarning()} set statusline+=%* \" read-only set statusline+=%r set statusline+=%* \" right-align set statusline+=%= \" filetype set statusline+=%{strlen(&ft)?&ft:'none'} \" separator set statusline+=\\ \" current char set statusline+=%3b,0x%02B \" separator set statusline+=\\ \" column, set statusline+=%2c, \" current line / lines in file set statusline+=%l/%L \" always show status line set laststatus=2 \" return '[tabs]' if tab chars in file, or empty string function! StatuslineTabWarning() if !exists(\"b:statusline_tab_warning\") let tabs = search('^\\t', 'nw') != 0 if tabs let b:statusline_tab_warning = '[tabs]' else let b:statusline_tab_warning = '' endif endif return b:statusline_tab_warning endfunction \"recalculate the tab warning flag when idle and after writing autocmd cursorhold,bufwritepost * unlet! b:statusline_tab_warning powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/wget.html":{"url":"common/Linux/tool/wget.html","title":"wget","keywords":"","body":"wget wget 补充说明 语法 选项 参数 实例 使用wget下载单个文件 下载并以不同的文件名保存 wget限速下载 使用wget断点续传 使用wget后台下载 测试下载链接 增加重试次数 下载多个文件 镜像网站 过滤指定格式下载 把下载信息存入日志文件 限制总下载文件大小 下载指定格式文件 FTP下载 Linux系统下载文件工具 补充说明 wget命令 用来从指定的URL下载文件。wget非常稳定，它在带宽很窄的情况下和不稳定网络中有很强的适应性，如果是由于网络的原因下载失败，wget会不断的尝试，直到整个文件下载完毕。如果是服务器打断下载过程，它会再次联到服务器上从停止的地方继续下载。这对从那些限定了链接时间的服务器上下载大文件非常有用。 wget支持HTTP，HTTPS和FTP协议，可以使用HTTP代理。所谓的自动下载是指，wget可以在用户退出系统的之后在后台执行。这意味这你可以登录系统，启动一个wget下载任务，然后退出系统，wget将在后台执行直到任务完成，相对于其它大部分浏览器在下载大量数据时需要用户一直的参与，这省去了极大的麻烦。 用于从网络上下载资源，没有指定目录，下载资源回默认为当前目录。wget虽然功能强大，但是使用起来还是比较简单： 支持断点下传功能 这一点，也是网络蚂蚁和FlashGet当年最大的卖点，现在，Wget也可以使用此功能，那些网络不是太好的用户可以放心了； 同时支持FTP和HTTP下载方式 尽管现在大部分软件可以使用HTTP方式下载，但是，有些时候，仍然需要使用FTP方式下载软件； 支持代理服务器 对安全强度很高的系统而言，一般不会将自己的系统直接暴露在互联网上，所以，支持代理是下载软件必须有的功能； 设置方便简单 可能，习惯图形界面的用户已经不是太习惯命令行了，但是，命令行在设置上其实有更多的优点，最少，鼠标可以少点很多次，也不要担心是否错点鼠标； 程序小，完全免费 程序小可以考虑不计，因为现在的硬盘实在太大了；完全免费就不得不考虑了，即使网络上有很多所谓的免费软件，但是，这些软件的广告却不是我们喜欢的。 语法 wget [参数] [URL地址] 选项 启动参数： -V, –-version 显示wget的版本后退出 -h, –-help 打印语法帮助 -b, –-background 启动后转入后台执行 -e, –-execute=COMMAND 执行 .wgetrc 格式的命令，wgetrc格式参见/etc/wgetrc或~/.wgetrc 记录和输入文件参数： -o, –-output-file=FILE 把记录写到FILE文件中 -a, –-append-output=FILE 把记录追加到FILE文件中 -d, –-debug 打印调试输出 -q, –-quiet 安静模式(没有输出) -v, –-verbose 冗长模式(这是缺省设置) -nv, –-non-verbose 关掉冗长模式，但不是安静模式 -i, –-input-file=FILE 下载在FILE文件中出现的URLs -F, –-force-html 把输入文件当作HTML格式文件对待 -B, –-base=URL 将URL作为在-F -i参数指定的文件中出现的相对链接的前缀 –-sslcertfile=FILE 可选客户端证书 –-sslcertkey=KEYFILE 可选客户端证书的KEYFILE –-egd-file=FILE 指定EGD socket的文件名 下载参数： –-bind-address=ADDRESS 指定本地使用地址(主机名或IP，当本地有多个IP或名字时使用) -t, –-tries=NUMBER 设定最大尝试链接次数(0 表示无限制). -O –-output-document=FILE 把文档写到FILE文件中 -nc, –-no-clobber 不要覆盖存在的文件或使用.#前缀 -c, –-continue 接着下载没下载完的文件 –progress=TYPE 设定进程条标记 -N, –-timestamping 不要重新下载文件除非比本地文件新 -S, –-server-response 打印服务器的回应 –-spider 不下载任何东西 -T, –-timeout=SECONDS 设定响应超时的秒数 -w, –-wait=SECONDS 两次尝试之间间隔SECONDS秒 –waitretry=SECONDS 在重新链接之间等待1…SECONDS秒 –random-wait 在下载之间等待0…2*WAIT秒 -Y, –-proxy=on/off 打开或关闭代理 -Q, –-quota=NUMBER 设置下载的容量限制 -–limit-rate=RATE 限定下载输率 目录参数： -nd –-no-directories 不创建目录 -x, –-force-directories 强制创建目录 -nH, –-no-host-directories 不创建主机目录 -P, –-directory-prefix=PREFIX 将文件保存到目录 PREFIX/… –cut-dirs=NUMBER 忽略 NUMBER层远程目录 HTTP 选项参数： -–http-user=USER 设定HTTP用户名为 USER. -–http-passwd=PASS 设定http密码为 PASS -C, –-cache=on/off 允许/不允许服务器端的数据缓存 (一般情况下允许) -E, –-html-extension 将所有text/html文档以.html扩展名保存 -–ignore-length 忽略 Content-Length 头域 -–header=STRING 在headers中插入字符串 STRING -–proxy-user=USER 设定代理的用户名为 USER -–proxy-passwd=PASS 设定代理的密码为 PASS -–referer=URL 在HTTP请求中包含 Referer: URL 头 -s, –-save-headers 保存HTTP头到文件 -U, –-user-agent=AGENT 设定代理的名称为 AGENT而不是 Wget/VERSION -–no-http-keep-alive 关闭 HTTP活动链接 (永远链接) –-cookies=off 不使用 cookies –-load-cookies=FILE 在开始会话前从文件 FILE中加载cookie -–save-cookies=FILE 在会话结束后将 cookies保存到 FILE文件中 FTP 选项参数： -nr, -–dont-remove-listing 不移走 .listing 文件 -g, -–glob=on/off 打开或关闭文件名的 globbing机制 -–passive-ftp 使用被动传输模式 (缺省值). -–active-ftp 使用主动传输模式 -–retr-symlinks 在递归的时候，将链接指向文件(而不是目录) 递归下载参数： -r, -–recursive 递归下载－－慎用! -l, -–level=NUMBER 最大递归深度 (inf 或 0 代表无穷) –-delete-after 在现在完毕后局部删除文件 -k, –-convert-links 转换非相对链接为相对链接 -K, –-backup-converted 在转换文件X之前，将之备份为 X.orig -m, –-mirror 等价于 -r -N -l inf -nr -p, –-page-requisites 下载显示HTML文件的所有图片 递归下载中的包含和不包含(accept/reject)： -A, –-accept=LIST 分号分隔的被接受扩展名的列表 -R, –-reject=LIST 分号分隔的不被接受的扩展名的列表 -D, –-domains=LIST 分号分隔的被接受域的列表 –-exclude-domains=LIST 分号分隔的不被接受的域的列表 –-follow-ftp 跟踪HTML文档中的FTP链接 –-follow-tags=LIST 分号分隔的被跟踪的HTML标签的列表 -G, –-ignore-tags=LIST 分号分隔的被忽略的HTML标签的列表 -H, –-span-hosts 当递归时转到外部主机 -L, –-relative 仅仅跟踪相对链接 -I, –-include-directories=LIST 允许目录的列表 -X, –-exclude-directories=LIST 不被包含目录的列表 -np, –-no-parent 不要追溯到父目录 wget -S –-spider url 不下载只显示过程 参数 URL：下载指定的URL地址。 实例 使用wget下载单个文件 wget http://www.jsdig.com/testfile.zip 以下的例子是从网络下载一个文件并保存在当前目录，在下载的过程中会显示进度条，包含（下载完成百分比，已经下载的字节，当前下载速度，剩余下载时间）。 下载并以不同的文件名保存 wget -O wordpress.zip http://www.jsdig.com/download.aspx?id=1080 wget默认会以最后一个符合/的后面的字符来命令，对于动态链接的下载通常文件名会不正确。 错误：下面的例子会下载一个文件并以名称download.aspx?id=1080保存: wget http://www.jsdig.com/download?id=1 即使下载的文件是zip格式，它仍然以download.php?id=1080命名。 正确：为了解决这个问题，我们可以使用参数-O来指定一个文件名： wget -O wordpress.zip http://www.jsdig.com/download.aspx?id=1080 wget限速下载 wget --limit-rate=300k http://www.jsdig.com/testfile.zip 当你执行wget的时候，它默认会占用全部可能的宽带下载。但是当你准备下载一个大文件，而你还需要下载其它文件时就有必要限速了。 使用wget断点续传 wget -c http://www.jsdig.com/testfile.zip 使用wget -c重新启动下载中断的文件，对于我们下载大文件时突然由于网络等原因中断非常有帮助，我们可以继续接着下载而不是重新下载一个文件。需要继续中断的下载时可以使用-c参数。 使用wget后台下载 wget -b http://www.jsdig.com/testfile.zip Continuing in background, pid 1840. Output will be written to 'wget-log'. 对于下载非常大的文件的时候，我们可以使用参数-b进行后台下载，你可以使用以下命令来察看下载进度： tail -f wget-log 伪装代理名称下载 wget --user-agent=\"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16\" http://www.jsdig.com/testfile.zip 有些网站能通过根据判断代理名称不是浏览器而拒绝你的下载请求。不过你可以通过--user-agent参数伪装。 测试下载链接 当你打算进行定时下载，你应该在预定时间测试下载链接是否有效。我们可以增加--spider参数进行检查。 wget --spider URL 如果下载链接正确，将会显示: Spider mode enabled. Check if remote file exists. HTTP request sent, awaiting response... 200 OK Length: unspecified [text/html] Remote file exists and could contain further links, but recursion is disabled -- not retrieving. 这保证了下载能在预定的时间进行，但当你给错了一个链接，将会显示如下错误: wget --spider url Spider mode enabled. Check if remote file exists. HTTP request sent, awaiting response... 404 Not Found Remote file does not exist -- broken link!!! 你可以在以下几种情况下使用--spider参数： 定时下载之前进行检查 间隔检测网站是否可用 检查网站页面的死链接 增加重试次数 wget --tries=40 URL 如果网络有问题或下载一个大文件也有可能失败。wget默认重试20次连接下载文件。如果需要，你可以使用--tries增加重试次数。 下载多个文件 wget -i filelist.txt 首先，保存一份下载链接文件： cat > filelist.txt url1 url2 url3 url4 接着使用这个文件和参数-i下载。 镜像网站 wget --mirror -p --convert-links -P ./LOCAL URL 下载整个网站到本地。 --miror开户镜像下载。 -p下载所有为了html页面显示正常的文件。 --convert-links下载后，转换成本地的链接。 -P ./LOCAL保存所有文件和目录到本地指定目录。 过滤指定格式下载 wget --reject=gif url 下载一个网站，但你不希望下载图片，可以使用这条命令。 把下载信息存入日志文件 wget -o download.log URL 不希望下载信息直接显示在终端而是在一个日志文件，可以使用。 限制总下载文件大小 wget -Q5m -i filelist.txt 当你想要下载的文件超过5M而退出下载，你可以使用。注意：这个参数对单个文件下载不起作用，只能递归下载时才有效。 下载指定格式文件 wget -r -A.pdf url 可以在以下情况使用该功能： 下载一个网站的所有图片。 下载一个网站的所有视频。 下载一个网站的所有PDF文件。 FTP下载 wget ftp-url wget --ftp-user=USERNAME --ftp-password=PASSWORD url 可以使用wget来完成ftp链接的下载。 使用wget匿名ftp下载： wget ftp-url 使用wget用户名和密码认证的ftp下载： wget --ftp-user=USERNAME --ftp-password=PASSWORD url powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/yum.html":{"url":"common/Linux/tool/yum.html","title":"yum","keywords":"","body":"yum yum 命令 示例 安装cmake 命令 来自yum -h 命令 解释 check Check for problems in the rpmdb check-update Check for available package updates clean Remove cached data deplist List a package's dependencies distribution-synchronization Synchronize installed packages to the latest available versions downgrade downgrade a package erase Remove a package or packages from your system fs Acts on the filesystem data of the host, mainly for removing docs/lanuages for minimal hosts. fssnapshot Creates filesystem snapshots, or lists/deletes current snapshots. groups Display, or use, the groups information help Display a helpful usage message history Display, or use, the transaction history info Display details about a package or group of packages install Install a package or packages on your system list List a package or groups of packages load -transaction load a saved transaction from filename makecache Generate the metadata cache provides Find what package provides the given value reinstall reinstall a package repo-pkgs Treat a repo. as a group of packages, so we can install/remove all of them repolist Display the configured software repositories search Search package details for the given string shell Run an interactive yum shell swap Simple way to swap packages, instead of using shell update Update a package or packages on your system update-minimal Works like upgrade, but goes to the 'newest' package match which fixes a problem that affects your system updateinfo Acts on repository update information upgrade Update packages taking obsoletes into account version Display a version for the machine and/or available repos. 示例 安装cmake 查询一下yum源中的相关包信息 yum search cmake yum list cmake | grep cmake 看一下安装状态, 可以知道一下安装之后的命令入口 yum provides */cmake 安装 yum install -y cmake powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/tool/zsh.html":{"url":"common/Linux/tool/zsh.html","title":"ZSH","keywords":"","body":"ZSH ZSH 安装 安装 安装脚本 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/project/":{"url":"common/Linux/project/","title":"专题","keywords":"","body":"专题 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/project/check.html":{"url":"common/Linux/project/check.html","title":"问题排查","keywords":"","body":"问题排查 问题排查 2023/03/20 kafka-python 消息发送失败 2023/02/10 内存不足 2022/08/08 磁盘不足 2022/07/18 继续内存问题 2022/04/11 安装gdb失败 2022/01/52 OSS图片没有Access-Control-Allow-Origin: *请求头 2021/12/20 尝试排查内存泄露问题 timeout 问题 top看到的内存占用与heap不一致 2020/10/28 延长ssh $'\\r': command not found 2020/05/11 服务器磁盘占用过高 2020/05/12 账号服务错误 2020/06/19 .h5文件读写报错 2023/03/20 kafka-python 消息发送失败 kafka-python可以创建topic，但是无法发送消息 消息是正常flush的producer.flush()， 而且老版本是正常发送 最后发现加了机器host就可以正常访问了， 原因如下： site-packages\\kafka\\client_async.py def _maybe_connect(self, node_id): \"\"\"Idempotent non-blocking connection attempt to the given node id.\"\"\" with self._lock: conn = self._conns.get(node_id) if conn is None: # 这边会尝试获取kafka元信息 broker = self.cluster.broker_metadata(node_id) assert broker, 'Broker id %s not in current metadata' % (node_id,) log.debug(\"Initiating connection to node %s at %s:%s\", node_id, broker.host, broker.port) # 这边会使用元信息的host来维持连接 host, port, afi = get_ip_port_afi(broker.host) cb = WeakMethod(self._conn_state_change) conn = BrokerConnection(host, broker.port, afi, state_change_callback=cb, node_id=node_id, **self.config) self._conns[node_id] = conn # Check if existing connection should be recreated because host/port changed elif self._should_recycle_connection(conn): self._conns.pop(node_id) return False elif conn.connected(): return True conn.connect() return conn.connected() kafka配置的host是机器名 get /brokers/ids/222 # 这边的host导致无法解析 {\"listener_security_protocol_map\":{\"PLAINTEXT\":\"PLAINTEXT\"},\"endpoints\":[\"PLAINTEXT://:9092\"],\"jmx_port\":9393,\"host\":\"\",\"timestamp\":\"1679283287285\",\"port\":9092,\"version\":4} 调整kafka配置的host后修复 2023/02/10 内存不足 yum update yum install -y gdb yum install -y python-psutil python-debuginfo python-pycallgraph pygobject3 webkitgtk3 python-meliae python3 -m pip install pyrasite cd /usr/local/python3/lib/python3.8/site-packages/pyrasite/tools python3 -m pip install urwid meliae objgraph guppy3 python3 shell.py 11 >>> import objgraph >>> objgraph.show_most_common_types(limit=20) 2022/08/08 磁盘不足 20G磁盘用完了，看下情况 清理日志差不多1G journalctl --disk-usage journalctl --vacuum-time=1w journalctl --vacuum-size=10M 清理snap差不多4G docker磁盘7G 清理不需要的资源差不多1G docker system prune -a 剩下5G，但是看镜像只有600M，可能要研究下 # 看着实际占用的磁盘远比这个大 docker system df -v 2022/07/18 继续内存问题 gdb --pid 30019 # 查看堆栈 >>> t a a py-bt >>> t a a bt # 查看虚拟内存占用情况 >>> i proc m 安装perf apt-get install -y linux-source binutils-dev libdw-dev python-dev libnewt-dev cd /usr/src tar -xf linux-source-4.19.tar.xz cd linux-source-4.19/tools/perf make make install 修改docker # docker docker run --cap-add sys_admin # docker-compose 添加配置 cap_add: - sys_admin ./perf probe -x /lib/x86_64-linux-gnu/libc-2.28.so --add malloc ./perf record -e probe_libc:malloc -aR sleep 10 报错 Error: The sys_perf_event_open() syscall returned with 22 (Invalid argument) for event (probe_libc:malloc). /bin/dmesg | grep -i perf may provide additional information. 没找到解决方案 git clone https://github.com/brendangregg/FlameGraph.git ./perf/perf record -p 34326 -e syscalls:sys_enter_brk -a -g -- sleep 1200 ./perf/perf record -p 34326 -e syscalls:sys_enter_mmap -a -g -- sleep 1200 ./perf/perf script -i perf.data &> perf.unfold ./FlameGraph/stackcollapse-perf.pl perf.unfold &> perf.folded ./FlameGraph/flamegraph.pl perf.folded > perf.svg rm -f perf.data perf.data.old perf.unfold perf.folded perf.svg ./perf/perf record -e page-fault -a -g -- sleep 120 2022/04/11 安装gdb失败 清华源将[glibc]文件删除了, 已提issue https://github.com/tuna/issues/issues/725 已经可以了，先apt-get update 先换源安装 https://blog.csdn.net/qq_21095573/article/details/99736630 用的阿里源 查看操作系统版本 head -n 1 /etc/issue 替换文件 /etc/apt/sources.list 然后 apt-get upgrade 2022/01/52 OSS图片没有Access-Control-Allow-Origin: *请求头 # curl访问资源 curl -voa '{img-url}' -H 'Origin:{host-url}' 相关资料 2021/12/20 尝试排查内存泄露问题 修改docker配置 # docker docker run --cap-add sys_ptrace # docker-compose 添加配置 cap_add: - sys_ptrace 安装gdb apt-get install -y gdb apt-get install -y python3-dbg gdb >>> source /usr/share/gdb/auto-load/usr/bin/python3.7-gdb.py 教程：https://wiki.python.org/moin/DebuggingWithGdb 利用pyrasite访问进程 安装pyrasite pip install pyrasite pyrasite-shell [pid] timeout 问题 修改vim /usr/local/lib/python3.8/site-packages/pyrasite/ipc.py self.sock.settimeout(5) -> self.sock.settimeout(50) 安装guppy3 pip install guppy3 >>> from guppy import hpy >>> h = hpy() >>> h.setref() >>> heap = h.heap() # 无依赖的 >>> byclodo = heap.byclodo >>> byclodo[0].sp >>> byclodo[0].get_rp(10) >>> ids = heap.byid >>> idset = heap.byidset >>> bymodule = heap.bymodule >>> byrcs = heap.byrcs >>> bysize = heap.bysize >>> bytype = heap.bytype >>> byunity = heap.byunity >>> byvia = heap.byvia 'biper', 'brief', 'by', 'byclodo', 'byid', 'byidset', 'bymodule', 'byprod', 'byrcs', 'bysize', 'bytype', 'byunity', 'byvia', 'count', 'dictof', 'diff', 'disjoint', 'doc', 'dominos', 'domisize', 'dump', 'er', 'fam', 'get_ckc', 'get_examples', 'get_render', 'get_rp', 'get_shpaths', 'imdom', 'indisize', 'kind', 'maprox', 'more', 'nodes', 'owners', 'partition', 'parts', 'pathsin', 'pathsout', 'prod', 'referents', 'referrers', 'rp', 'shpaths', 'size', 'sp', 'stat', 'test_contains', 'theone' import random h.heap().byid[random.randint(0, 494765)].sp 检查垃圾数据 >>> import gc >>> gc.collect() >>> gc.garbage 打印堆栈 # 需要gdb pip install pystack-debugger pystack [pid] top看到的内存占用与heap不一致 gcore 生成失败了 root@xm-biapi:/XM# gcore 98 warning: process 98 is already traced by process 15209 ptrace: Operation not permitted. You can't do that without a process to debug. The program is not being run. gcore: failed to create core.98 清除相关监听 ps -ef | grep gdb 2020/10/28 延长ssh 修改配置 #（每30秒往客户端发送会话请求，保持连接） ClientAliveInterval 30 #（去掉注释即可，3表示重连3次失败后，重启SSH会话） ClientAliveCountMax 3 重启 service sshd restart 没有的话就重装 yum -y install openssh-server openssh-clients $'\\r': command not found vi :set ff-unix 2020/05/11 服务器磁盘占用过高 df -h # 99G占用了90G左右 find . -type f -size +800M find . -type f -size +800M -print0 | xargs -0 ls -l find . -type f -size +800M -print0 | xargs -0 du -h find . -type f -size +800M -print0 | xargs -0 du -hm | sort -nr # 没有 find . -type f -size +500M # 有一个数据库导出文件， 删除 # 应该不是大文件导致的 du -h --max-depth=1 du -h --max-depth=2 # 总计只占用了4G左右，考虑僵尸进程导致文件回收失败的情况 lsof | grep delete lsof -n | grep deleted # 确实存在异常数据，占了60G, 强制关闭解决 kill -9 {pid} 2020/05/12 账号服务错误 # 电脑端访问失败、手机端可以访问 tail -f /data/docker/mount/nginx/logs/access_home.log # 所有请求都正常200返回 # 查看另一个服务，发现很慢，差不多1s才返回，可能是接口太慢被拒绝了 # 看了下日志，最近有攻击记录，看看连接情况 netstat -nat # 发现有一个美国地址， 想起来梯子开的全局模式， 关闭搞定... 2020/06/19 .h5文件读写报错 Traceback (most recent call last): File \"/XM/apps/bi/mq_handlers.py\", line 167, in rule_invalidate_by_survey_respondent_submit await df_utils.save_dataframe_by_file(file_path, df, key=key, mode=\"a\", format=\"table\", append=True) File \"/XM/commons/df_utils.py\", line 139, in save_dataframe_by_file df.to_hdf(abs_filename, key, **kwargs) File \"/usr/local/lib/python3.6/site-packages/pandas/core/generic.py\", line 2531, in to_hdf pytables.to_hdf(path_or_buf, key, self, **kwargs) File \"/usr/local/lib/python3.6/site-packages/pandas/io/pytables.py\", line 276, in to_hdf path_or_buf, mode=mode, complevel=complevel, complib=complib File \"/usr/local/lib/python3.6/site-packages/pandas/io/pytables.py\", line 505, in __init__ self.open(mode=mode, **kwargs) File \"/usr/local/lib/python3.6/site-packages/pandas/io/pytables.py\", line 627, in open self._handle = tables.open_file(self._path, self._mode, **kwargs) File \"/usr/local/lib/python3.6/site-packages/tables/file.py\", line 320, in open_file return File(filename, mode, title, root_uep, filters, **kwargs) File \"/usr/local/lib/python3.6/site-packages/tables/file.py\", line 784, in __init__ self._g_new(filename, mode, **params) File \"tables/hdf5extension.pyx\", line 492, in tables.hdf5extension.File._g_new tables.exceptions.HDF5ExtError: HDF5 error back trace File \"H5F.c\", line 511, in H5Fopen unable to open file File \"H5Fint.c\", line 1519, in H5F_open unable to lock the file File \"H5FD.c\", line 1650, in H5FD_lock driver lock request failed File \"H5FDsec2.c\", line 941, in H5FD_sec2_lock unable to lock file, errno = 11, error message = 'Resource temporarily unavailable' End of HDF5 error back trace Unable to open/create file '/XM/data/5d70d351aace70000afe0d1c_20.h5' 尝试使用读写锁，保证文件操作串行执行 读写锁正常执行，但错误没有解决 # 安装lsof yum install lsof lsof 5d70d351aace70000afe0d1c_20.h5 lsof +D ./ 查询日志， 发现有问题的文件都有过相关报错 [I 200620 09:37:47 aioredislock:143 pid=11] 获得/XM/data/5edf5b19aace70000cbfccec_3.h5读锁 [E 200620 09:37:47 df_utils:186 pid=11] PID: 11, /XM/data/5edf5b19aace70000cbfccec_3.h5 key[/df_1591804800] exception [Unknown]! Traceback (most recent call last): File \"/XM/commons/df_utils.py\", line 172, in get_dataframe_by_file df = pd.read_hdf(abs_filename, key=key, **kwargs) File \"/usr/local/lib/python3.6/site-packages/pandas/io/pytables.py\", line 407, in read_hdf return store.select(key, auto_close=auto_close, **kwargs) File \"/usr/local/lib/python3.6/site-packages/pandas/io/pytables.py\", line 761, in select s = self._create_storer(group) File \"/usr/local/lib/python3.6/site-packages/pandas/io/pytables.py\", line 1457, in _create_storer fields = group.table._v_attrs.fields File \"/usr/local/lib/python3.6/site-packages/tables/group.py\", line 839, in __getattr__ return self._f_get_child(name) File \"/usr/local/lib/python3.6/site-packages/tables/group.py\", line 711, in _f_get_child self._g_check_has_child(childname) File \"/usr/local/lib/python3.6/site-packages/tables/group.py\", line 398, in _g_check_has_child % (self._v_pathname, name)) tables.exceptions.NoSuchNodeError: group ``/df_1591804800`` does not have a child named ``table`` [I 200620 09:37:47 aioredislock:155 pid=11] 释放/XM/data/5edf5b19aace70000cbfccec_3.h5读锁， 完全释放 通过该问题去查询，发现相关issue: Build libhdf5 with the --enable-threadsafe flag 尝试根据该方法编译一个线程安全的 libhdf5.so 下载hdf5源码 hdf5项目 cd /opt/ wget -i https://hdf-wordpress-1.s3.amazonaws.com/wp-content/uploads/manual/HDF5/HDF5_1_12_0/source/hdf5-1.12.0.tar.gz ./configure --prefix=/usr/local/hdf5 --disable-hl --enable-threadsafe make install 看了下版本对不上， 当前版本3.5.1会有一个指向tables自己编译的 ldd hdf5extension.cpython-36m-x86_64-linux-gnu.so，而不是系统定义的 ldd hdf5extension.cpython-36m-x86_64-linux-gnu.so linux-vdso.so.1 (0x00007fff94b0a000) libhdf5-1b021ebd.so.101.1.0 => /usr/local/lib/python3.6/site-packages/tables/./.libs/libhdf5-1b021ebd.so.101.1.0 (0x00007ff7dd99c000) libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007ff7dd61a000) libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007ff7dd316000) libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007ff7dd0ff000) libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007ff7dcee2000) libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007ff7dcb43000) librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007ff7dc93b000) libsz-1c7dd0cf.so.2.0.1 => /usr/local/lib/python3.6/site-packages/tables/./.libs/./libsz-1c7dd0cf.so.2.0.1 (0x00007ff7dc737000) libaec-2147abcd.so.0.0.4 => /usr/local/lib/python3.6/site-packages/tables/./.libs/./libaec-2147abcd.so.0.0.4 (0x00007ff7dc52e000) libz-a147dcb0.so.1.2.3 => /usr/local/lib/python3.6/site-packages/tables/./.libs/./libz-a147dcb0.so.1.2.3 (0x00007ff7dc319000) libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007ff7dc115000) /lib64/ld-linux-x86-64.so.2 (0x00007ff7de323000) 先尝试修改tables/__init__.py中的依赖路径, 看了下代码只有windows平台是基于配置的， unix系需要在安装时就指定路径 参考 安装文档 # 尝试使用进行安装， 但是失败 pip3 install --install-option='--hdf5=/usr/local/hdf5/' tables==3.5.1 相关issue 很老的版本需要系统自带的hdf5，现在会打在wheel里，就不维护了。但是打出来的有问题啊，唉，得继续找办法了 # 尝试下载源码， python setup.py install -hdf5=/usr/local/hdf5/ # 装不了， 查了一圈 export DYLD_LIBRARY_PATH=/usr/local/hdf5/lib # 运行测试失败 Traceback (most recent call last): File \"test_tables.py\", line 1, in import tables File \"/usr/local/lib/python3.6/dist-packages/tables-3.5.1-py3.6-linux-x86_64.egg/tables/__init__.py\", line 93, in from .utilsextension import ( ImportError: libhdf5.so.200: cannot open shared object file: No such file or directory # 看报错信息 ld -ldhf5 --verbose # 是没找到 hdf5.so 导致的， 尝试加入，并检查了一下其他库都有在 ln -s /usr/local/hdf5/lib/libhdf5.so /usr/local/lib/libhdf5.so # 再尝试 ld -ldhf5 --verbose # 换了一个报错 # ld: warning: cannot find entry symbol _start; not setting start address # 查了一下是因为没有libc.so导致的， 但确实有加载到啊 # found libc.so.6 at //lib/x86_64-linux-gnu/libc.so.6 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/project/bash.html":{"url":"common/Linux/project/bash.html","title":"bash脚本","keywords":"","body":"bash脚本 bash脚本 参数 判断 逻辑表达式 条件跳转 参数 # 参数数量 $# # 所有参数，不分隔 $* # 所有参数，分隔 $@ # 当前脚本名 $0 # 获得当前进程 ID $$ # 获得之前(上一个)进程 ID $! # 获得之前(上一个)进程结束的状态码 (0 表示成功, 1 表示失败) $? # 参数1~9 $1-$9 # 向左移动参数n位 shift n $(basename $1) $(dirname $1) 判断 if [逻辑表达式]; then [代码段] else [代码段] fi 逻辑表达式 test \"$1\" = \"$arg\" [\"$1\" = \"$arg\"] = : == : != : -n : -z : -eq n : -ne n : -lt n : -lr n : -gt n : -ge n : -b file : 文件存在并且是块设备文件。 -c file : 文件存在并且是字符设备文件。 -d file : 文件存在并且是一个目录。 -e file : 文件存在。 -f file : 文件存在并且是一般文件。 -g file : 文件存在并且设置了 setgid 位。 -h file : 文件存在并且是一个链接文件。 -p file : 文件存在并且是一个命名管道(FIFO)。 -r file : 文件存在并且是可读的。 -s file : 文件存在并且有内容。 -u file : 文件存在并且设置了 setuid。 -w file : 文件存在并且是可写的。 -x file : 文件存在并且是可执行的。 -S file : 文件存在并且是一个 socket。 条件跳转 case [变量] in [条件1]) [代码段] ;; [条件2]) [代码段] ;; *) [代码段] ;; esac powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Linux/project/file-system.html":{"url":"common/Linux/project/file-system.html","title":"文件系统","keywords":"","body":"文件系统 文件系统 文件系统 RAW CDFS FAT16 FAT32 NTFS exFAT Ext2 Ext3 Ext4 HFS HFS+ Btrfs ZFS 挂载 挂载 取消挂载 文件系统 RAW 磁盘原始文件系统 格式化 ->CDFS 光盘 CD-R/CD-RWFAT16 win 9X ~4GB 小 簇 32KBFAT32 win 2000 512MB~32GB 小 簇 4KBNTFS win 安全性高， 日志式， 多次读写 MBR ~2TB GPT 无限制exFAT win 闪存( U盘 ) 超过4GBU盘默认 NTFSExt2 linuxExt3 linux Ext2 的扩展， 添加了日志 最大16TB的文件系统， 单文件最大2TBExt4 linux Ext3 的改进 最大1EB的文件系统， 单文件最大16TBHFS appleHFS+ appleBtrfs Oracle 提出， 为了取代Ext3ZFS 挂载 挂载 tmpfs mount -t tmpfs -o size=500m tmpfs /mnt/tmp/ vi /etc/fstab tmpfs /mnt/tmp tmpfs defaults,size=4000M 0 0 取消挂载 umount /mnt/tmp powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/windows/":{"url":"common/windows/","title":"windows","keywords":"","body":"windows powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/windows/tool/":{"url":"common/windows/tool/","title":"工具","keywords":"","body":"工具 工具 系统工具 系统工具 Funzioni Comandi 备注 Versione Windows winver 版本信息 Windows Memory Diagnostic Scheduler mdsched 内存诊断 Windows Features optionalfeatures windows功能 Windows Firewall firewall.cpl 防火墙设置 Windows Firewall with Advanced Security wf.msc 防火墙高级设置 Volume Mixer sndvol 音频合成器 Task Scheduler taskschd.msc 任务计划程序 Task Manager taskmgr 任务管理器 System Restore rstrui 系统还原 System Information msinfo32 系统信息 System Configuration msconfig 系统配置 Services services.msc 服务 Resource Monitor resmon 资源监视器 Registry Editor regedit 注册表编辑器 Registry Editor 32 regedt32 注册表编辑器 Programs and Features appwiz.cpl 程序和功能 Computer Management compmgmt.msc 计算机管理 Device Manager devmgmt.msc 设备管理器 Open Documents Folder documents 文档文件夹 Open Videos folder videos 视频文件夹 Open Downloads Folder downloads 下载文件夹 Open Favorites Folder favorites 收藏夹 Open Recent Folder recent 最近访问 Open Pictures Folder pictures 图片文件夹 Windows Sideshow control.exe /name Microsoft.WindowsSideshow win10无效 Windows CardSpace control.exe /name Microsoft.cardspace win10无效 Windows Anytime Upgrade WindowsAnytimeUpgradeui win10无效 Taskbar and Start Menu control.exe /name Microsoft.TaskbarandStartMenu win10无效 Troubleshooting control.exe /name Microsoft.Troubleshooting win10无效 User Accounts control.exe /name Microsoft.UserAccounts win10无效 Adding a new Device devicepairingwizard 添加设备 Add Hardware Wizard hdwwiz 添加硬件向导 Advanced User Accounts netplwiz 用户账号 Advanced User Accounts azman.msc 授权管理器 Backup and Restore sdclt 备份和还原 Bluetooth File Transfer fsquirt 蓝牙文件传送 Calculator calc 计算器 Certificates certmgr.msc 证书 Change Computer Performance Settings systempropertiesperformance 性能选项 Change Data Execution Prevention Settings systempropertiesdataexecutionprevention 性能选项 Change Data Execution Prevention Settings printui 打印机用户界面 Character Map charmap 字符映射表 ClearType Tuner cttune 文本调谐器 Color Management colorcpl 颜色管理 Command Prompt cmd CMD Component Services comexp.msc 组件服务 Component Services dcomcnfg 组件服务 Computer Management compmgmt.msc 计算机管理 Computer Management compmgmtlauncher 计算机管理 Connessione proiettore di rete netproj win10无效 Connect to a Projector displayswitch 投影设置 Control Panel control 控制面板 Create A Shared Folder Wizard shrpubw 创建共享文件夹向导 Create a System Repair Disc recdisc 系统修复光盘 Credential Backup and Restore Wizard credwiz 存储用户密码 Data Execution Prevention systempropertiesdataexecutionprevention 性能选项 Date and Time timedate.cpl 日期和时间 Default Location locationnotifications win10无效 Device Manager devmgmt.msc 设备管理器 Device Manager hdwwiz.cpl 设备管理器 Device Pairing Wizard devicepairingwizard 添加设备 Diagnostics Troubleshooting Wizard msdt 支持诊断工具 Digitizer Calibration Tool tabcal 数字化器校准工具 DirectX Diagnostic Tool dxdiag directx诊断工具 Disk Cleanup cleanmgr 磁盘清理 Disk Defragmenter dfrgui 优化驱动器 Disk Management diskmgmt.msc 磁盘管理 Display dpiscaling 显示 Display Color Calibration dccw 显示颜色较准 Display Switch displayswitch 投影设置 DPAPI Key Migration Wizard dpapimig 受保护的内容更新 Driver Verifier Manager verifier 驱动程序验证程序管理器 Ease of Access Center utilman 显示 EFS Wizard rekeywiz 加密文件系统 Event Viewer eventvwr.msc 事件查看器 Fax Cover Page Editor fxscover 传真 File Signature Verification sigverif 文件签名验证 Font Viewer fontview win10无效 Game Controllers joy.cpl 游戏控制器 Getting Started gettingstarted win10无效 IExpress Wizard iexpress IE 密码管理 Getting Started irprops.cpl 红外线管理 Install or Uninstall Display Languages lusrmgr win10无效 Internet Explorer iexplore IE Internet Options inetcpl.cpl internet选项 iSCSI Initiator Configuration Tool iscsicpl scsi管理 Language Pack Installer lpksetup 安装/卸载语言 Local Group Policy Editor gpedit.msc 本地组策略编辑器 Local Security Policy secpol.msc 本地安全策略 Local Users and Groups lusrmgr.msc 用户组管理 Location Activity locationnotifications win10无效 Magnifier magnify 放大镜 Malicious Software Removal Tool mrt 恶意软件删除工具 Manage Your File Encryption Certificates rekeywiz 加密文件系统 Math Input Panel mip 数学公式创建 Microsoft Management Console mmc 控制管理台 Microsoft Support Diagnostic Tool msdt window支持诊断工具 Mouse main.cpl 鼠标属性 NAP Client Configuration napclcfg.msc win10无效 Narrator narrator 讲述人设置 Network Connections ncpa.cpl 网络连接 New Scan Wizard wiaacmgr 扫描仪 Notepad notepad 笔记本 ODBC Data Source Administrator odbcad32 ODBC数据库管理 ODBC Driver Configuration odbcconf win10无效 On-Screen Keyboard osk 屏幕键盘 Paint mspaint 画图 Pen and Touch tabletpc.cpl 平板 People Near Me collab.cpl win10无效 Performance Monitor perfmon.msc 性能监控器 Performance Options systempropertiesperformance 性能选项 Phone and Modem telephon.cpl 位置信息 Phone Dialer dialer 电话拨号程序 Power Options powercfg.cpl 电源选项 Presentation Settings presentationsettings 电源选项 Print Management printmanagement.msc 打印管理 Printer Migration printbrmui 打印机迁移 Printer User Interface printui 打印机用户界面 Private Character Editor eudcedit 字符编辑器 Problem Steps Recorder psr 步骤记录器 Programs and Features appwiz.cpl 程序和功能 Protected Content Migration dpapimig 受保护的内容更新 Region and Language intl.cpl 区域 Registry Editor regedit 注册表编辑器 Registry Editor 32 regedt32 注册表编辑器 Remote Access Phonebook rasphone VPN连接 Remote Desktop Connection mstsc 远程连接桌面 Resource Monitor resmon 资源监视器 Resultant Set of Policy rsop.msc 策略的结果集 SAM Lock Tool syskey win10无效 Screen Resolution desk.cpl 显示 Securing the Windows Account Database syskey win10无效 Services services.msc 服务 Set Program Access and Computer Defaults computerdefaults 默认应用 Share Creation Wizard shrpubw 创建共享文件夹向导 Shared Folders fsmgmt.msc 共享文件夹 Snipping Tool snippingtool 截图工具 Sound mmsys.cpl 声音 Sound recorder soundrecorder win10无效 SQL Server Client Network Utility cliconfg SQL server Sticky Notes stikynot win10无效 Stored User Names and Passwords credwiz 备份存储用户信息、密码 Sync Center mobsync 同步中心 System Configuration msconfig 系统配置 System Configuration Editor sysedit win10无效 System Information msinfo32 系统信息 System Properties sysdm.cpl 系统属性 System Properties (Advanced Tab) systempropertiesadvanced 系统属性-advanced System Properties (Computer Name Tab) systempropertiescomputername 系统属性-computername System Properties (Hardware Tab) systempropertieshardware 系统属性-hardware System Properties (Remote Tab) systempropertiesremote 系统属性-remote System Properties (System Protection Tab) systempropertiesprotection 系统属性-protection System Restore rstrui 系统还原 Task Manager taskmgr 任务管理器 Task Scheduler taskschd.msc 任务计划程序 Trusted Platform Module (TPM) Management tpm.msc 平台模块管理 User Account Control Settings useraccountcontrolsettings 用户账号控制设置 Utility Manager utilman 显示设置 Version Reporter Applet winver window版本信息 Volume Mixer sndvol 音频合成器 Windows Action Center wscui.cpl 安全与维护 Windows Activation Client slui windows激活 Windows Anytime Upgrade Results windowsanytimeupgraderesults win10无效 Windows CardSpace infocardcpl.cpl win10无效 Windows Disc Image Burning Tool isoburn 光盘刻录 Windows DVD Maker dvdmaker win10无效 Windows Easy Transfer migwiz 过时 Windows Explorer explorer IE Windows Fax and Scan wfs 传真和扫描 Windows Features optionalfeatures windows功能 Windows Firewall firewall.cpl 防火墙设置 Windows Firewall with Advanced Security wf.msc 防火墙高级设置 Windows Journal journal win10无效 Windows Media Player wmplayer Windows media player Windows Memory Diagnostic Scheduler mdsched 内存诊断 Windows Mobility Center mblctr window移动中心（笔记本） Windows Picture Acquisition Wizard wiaacmgr 扫描仪 Windows PowerShell powershell powershell Windows PowerShell ISE powershell_ise powershell编辑器 Windows Remote Assistance msra 远程协助 Windows Repair Disc recdisc 修复光盘 Windows Script Host wscript 启动脚本（不懂） Windows Update wuapp win10无效 Windows Update Standalone Installer wusa window 跟新独立安装程序 Versione Windows winver 版本信息 WMI Management wmimgmt.msc WMI 数据管理 WordPad write 写字板 XPS Viewer xpsrchvw XPS查看器（office文件压缩） powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/windows/project/":{"url":"common/windows/project/","title":"专题","keywords":"","body":"专题 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/windows/project/check.html":{"url":"common/windows/project/check.html","title":"问题排查","keywords":"","body":"问题排查 问题排查 2022/09/13 win32clipboard 引用失败 2021/03/18 pip 失败 vcode无法运行powershell 2020/11/23 系统蓝屏重启 2020/11/09 本地服务无法远程访问 2020/08/10 照片应用无法使用 windows Terminal 设置管理员权限 2022/09/13 win32clipboard 引用失败 Traceback (most recent call last): File \".\\mdt_gui.py\", line 14, in import win32clipboard ImportError: DLL load failed while importing win32clipboard: 找不到指定的程序。 # 添加引用规则，在win32clipboard之前需要引用pywintypes import pywintypes import win32clipboard 2021/03/18 pip 失败 pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available 需要添加Miniconda相关的环境变量 vcode无法运行powershell PowerShell：因为在此系统上禁止运行脚本 查询策略: get-executionpolicy 设置策略: set-executionpolicy remotesigned 2020/11/23 系统蓝屏重启 win + x -> 事件查看器/windows 日志/系统 查看有如下几个错误: 由于在创建转储期间出错，创建转储文件失败。 来源: volmgr 事件ID: 161 并不是因为这个事件导致的蓝屏。出现这个事件是因为在win10蓝屏之后，系统会尝试创建一个转储文件，这个文件包含了蓝屏的详细信息。如果转储文件创建不成功就会出现这个错误。然而真正导致你蓝屏的应该是其他的原因 https://www.zhihu.com/question/389864585 https://answers.microsoft.com/zh-hans/windows/forum/all/%E7%94%B1%E4%BA%8E%E5%9C%A8%E5%88%9B%E5%BB%BA/b50a6826-a31d-42ad-9322-1f91fe265ef4 我的电脑>>属性 > 关于 > 高级系统设置 > 高级 >>> 小内存存储 %SystemRoot%\\Minidump 上一次系统的 16:02:44 在 ‎2020/‎11/‎23 上的关闭是意外的。 来源: EventLog 事件ID: 6008 创建 TLS 客户端 凭据时发生严重错误。内部错误状态为 10013。 来源: Schannel 事件ID: 36871 2020/11/09 本地服务无法远程访问 Windows Denfener 开放端口 入站规则 -> 新建规则 -> 端口 2020/08/10 照片应用无法使用 卸载应用 get-appxpackage *photo* remove-appxpackage {PackageFullName} 安装应用 Get-AppxPackage | % { Add-AppxPackage -DisableDevelopmentMode -Register \"$($_.InstallLocation)\\AppxManifest.xml\" -verbose } windows Terminal 设置管理员权限 PowerShell -Command \"Set-ExecutionPolicy RemoteSigned -scope Process; iwr -useb https://raw.githubusercontent.com/gerardog/gsudo/master/installgsudo.ps1 | iex\" 添加到settings { \"guid\": \"{41dd7a51-f0e1-4420-a2ec-1a7130b7e950}\", \"name\": \"Windows PowerShell Elevated\", \"commandline\": \"gsudo.exe powershell.exe\", \"hidden\": false, \"colorScheme\": \"Solarized Dark\", \"fontFace\": \"Fira Code\", \"icon\" : \"https://i.imgur.com/Giuj3FT.png\" }, 文档: https://blog.csdn.net/weixin_39858881/article/details/107026065 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/windows/project/cmd.html":{"url":"common/windows/project/cmd.html","title":"CMD","keywords":"","body":"CMD CMD 流程 if for 打印 参数及扩展 组合示例 迁移 软链接 任务 net 参考资料 流程 if if exist c:\\windows\\History\\*.* del c:\\windows\\History\\*.* for for %%i in (set) do echo %%i for /L %%i in (start, step, end) do echo %%i for /F %%i in (file) do echo %%i ''' a.txt 1.1 1.2 1.3 2.1 2.2 2.3 3.1 3.2 3.3 ''' for /F %%i in (a.txt) do echo %%i ''' 1.1 1.2 1.3 2.1 2.2 2.3 3.1 3.2 3.3 ''' for /f \"delims= \" %%i in (a.txt) do echo %%i ''' 1.1 2.1 3.1 ''' for /f \"delims=.\" %%i in (a.txt) do echo %%i ''' 1 2 3 ''' for /f \"tokens=2,3 delims= \" %%i in (a.txt) do echo %%i %%j for /f \"tokens=2-3 delims= \" %%i in (a.txt) do echo %%i %%j for /f \"tokens=2,* delims= \" %%i in (a.txt) do echo %%i %%j ''' 1.2 1.3 2.2 2.3 3.2 3.3 ''' for /f \"skip=2 tokens=*\" %%i in (a.txt) do echo %%i ''' 3.1 3.2 3.3 ''' for /f \"eol=1 tokens=*\" %%i in (a.txt) do echo %%i ''' 2.1 2.2 2.3 3.1 3.2 3.3 ''' 打印 echo @echo off 参数及扩展 命令 效果 %0 批处理文件本身 %1~9 第一~九个参数 %* 从第一个参数开始的所有参数 %~1 删除引号(\\\")，扩充 %1 %~f1 将 %1 扩充到一个完全合格的路径名 %~d1 仅将 %1 扩充到一个驱动器号 %~p1 仅将 %1 扩充到一个路径 %~n1 仅将 %1 扩充到一个文件名 %~x1 仅将 %1 扩充到一个文件扩展名 %~s1 扩充的路径指含有短名 %~a1 将 %1 扩充到文件属性 %~t1 将 %1 扩充到文件的日期/时间 %~z1 将 %1 扩充到文件的大小 %~$PATH 查找列在 PATH 环境变量的目录，并将 %1扩充到找到的第一个完全合格的名称。如果环境变量名未被定义，或者没有找到文件，此组合键会扩充到空字符串 组合示例 命令 效果 %~dp1 只将 %1 扩展到驱动器号和路径 %~nx1 只将 %1 扩展到文件名和扩展名 %~dp$PATH:1 在列在 PATH 环境变量中的目录里查找 %1，并扩展到找到的第一个文件的驱动器号和路径。 %~ftza1 将 %1 扩展到类似 DIR 的输出行。 迁移 pushd \"%~dp0\" REM 这里的root被修改到该文件的地址了 popd 软链接 需要是NTFS系统 mklink \\j link target mklink \\d rmlink 任务 tasklist | findstr \"2720\" taskkill /f /t /im python.exe net netstat -aon | findstr \"8050\" nbtstat -A 192.168.0.1 ping sz1.tencent.com 参考资料 教程: https://www.tutorialspoint.com/batch_script/index.htm powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/windows/project/wsl.html":{"url":"common/windows/project/wsl.html","title":"wsl","keywords":"","body":"wsl wsl 安装 Enable the Windows Subsystem for Linux Enable Virtual Machine feature Download the Linux kernel update package 更新 安装docker(弃用， 直接转wsl2) 安装 python3 修改复制模式 问题处理 问题: 参考的对象类型不支持尝试的操作。 安装 Enable the Windows Subsystem for Linux win+x powershell(管理员) Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux Enable Virtual Machine feature dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart Download the Linux kernel update package wsl --set-default-version 2 wsl --list --verbose wsl --set-version 教程: https://docs.microsoft.com/en-us/windows/wsl/install-win10 更新 sudo apt-get update 安装docker(弃用， 直接转wsl2) 教程: https://docs.docker.com/engine/install/ubuntu/#installation-methods WSL使用docker: https://blog.jayway.com/2017/04/19/running-docker-on-bash-on-windows/ win10 Home 安装docker: https://itnext.io/install-docker-on-windows-10-home-d8e621997c1d Update the apt package index and install packages to allow apt to use a repository over HTTPS: sudo apt-get update sudo apt-get install apt-transport-https ca-certificates curl gnupg-agent software-properties-common Add Docker’s official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Use the following command to set up the stable repository sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" install the latest version of Docker Engine and containerd sudo apt-get install docker-ce docker-ce-cli containerd.io WSL现在用不了docker，主要是用不了systemctl 尝试windows上启动服务，wsl启动client 已检测到虚拟机监控程序。将不显示 Hyper-V 所需的功能。 Home装不了docker..., 转装docker-toolbox # 关闭Hyper-V bcdedit /set {d89bf857-d769-11e9-a69e-3c18a0727e3f} hypervisorlaunchtype OFF # 关闭Hyper-V bcdedit /set hypervisorlaunchtype off # 开启Hyper-V bcdedit /set hypervisorlaunchtype auto 安装 python3 apt-get install python3-distutils python3-dev build-essential libssl-dev libffi-dev libxml2-dev libxslt1-dev zlib1g-dev python3 python3-pip 修改复制模式 Right-click on the title bar > Properties Options tab > Edit options > enable QuickEdit Mode 问题处理 问题: 参考的对象类型不支持尝试的操作。 netsh winsock reset powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/RegExp/":{"url":"common/RegExp/","title":"RegExp","keywords":"","body":"RegExp powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/RegExp/command.html":{"url":"common/RegExp/command.html","title":"命令","keywords":"","body":"命令 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/RegExp/example.html":{"url":"common/RegExp/example.html","title":"实例","keywords":"","body":"实例 实例 python validate配置 删除特征 去除不合法的空格 python validate配置 ALL = r'.*' USERNAME = r'[a-zA-Z0-9_\\-]{1,}' PASSWORD = r'[a-zA-Z0-9]{6,16}' EMAIL = r'([a-zA-Z0-9_\\.\\-])+\\@([a-zA-Z0-9\\-])+(\\.([a-zA-Z0-9]{2,6}))+' MOBILE = r'(1[3-9][0-9])[0-9]{8}' PHONE = r'(\\(((010)|(021)|(0\\d{3,4}))\\)( ?)([0-9]{7,8}))|((010|021|0\\d{3,4}))([- ]?)([0-9]{7,8})|([0-9]{7,8})' PHONE_COMMON = r'([0-9]{3}[_ -][0-9]{8}|[0-9]{4}-[0-9]{7}|[0-9]{8}|1[0-9]{10})' COLUMN_ID = r'[a-fA-F0-9]{24}' FORM_GET = r'application/x-www-form-urlencoded' FORM_FILE = r'multipart/form-data' HTML_TAG = r'' REQUEST_HEADER = r'httputil' CH = r'[^\\u0000-\\u00FF]*' NUMBER = r'([-]?[0-9]+(\\.[0-9]+){0,1})' MONTH = r'(0?[1-9]|1[0-2])' DAY = r'((0?[1-9])|((1|2)[0-9])|(3[01]))' TIME = r'(0?[1-9]|1[0-9]|2[0-4])((:|-|\\/|\\\\)(0?[0-9]|[1-5][0-9])){2}' DATE = r'[1-9][0-9]{0,3}(?:年|\\||\\\\|\\/|\\s|,|、|-)(0?[1-9]|1[0-2])(?:月|\\||\\\\|\\/|\\s|,|、|-)((0?[1-9])|((1|2)[0-9])|(3[01]))日?' BIRTHDAY = r'(19|20)[0-9]{2}(:|-|\\/|\\\\)(((0?[1-9]|1[0-2])(:|-|\\/|\\\\)(0?[1-9]|1[0-9]|2[0-9]))|((0?[13-9]|1[0-2])(:|-|\\/|\\\\)(30))|((0?[13578]|1[02])(:|-|\\/|\\\\)(31)))' CREDIT = '[1-9][0-9]{5}[1-9][0-9]{3}((0[0-9])|(1[0-2]))(([0|1|2][0-9])|3[0-1])[0-9]{3}([0-9]|x|X)' URL = r'((http|ftp|https)://)?(([a-zA-Z0-9\\._-]+\\.[a-zA-Z]{2,6})|([0-9]{1,3}(\\.[0-9]{1,3}){3}))(:[0-9]{1,4})*(/[a-zA-Z0-9\\&%_\\./-~-]*)?' IPV4AGENT = r'(192\\.168\\.|169\\.254\\.|10\\.|172\\.(1[6-9]|2[0-9]|3[01]))' IPV4 = r'[0-9]{1,3}(\\.[0-9]{1,3}){3}' IPV6 = r'[a-f0-9]{1,4}(:[a-f0-9]{1,4}){7})' 删除特征 (?^((?!tr).)*$ 去除不合法的空格 ([^'\"\\(\\),(LIMIT)]) ([^'\"\\(\\),(AS)(WHEN)(BY)]) powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/RegExp/language/":{"url":"common/RegExp/language/","title":"方言","keywords":"","body":"方言 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/RegExp/language/python.html":{"url":"common/RegExp/language/python.html","title":"python","keywords":"","body":"python 模式 描述 ^ 匹配字符串的开头 $ 匹配字符串的末尾。 . 匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符。 [...] 用来表示一组字符,单独列出：[amk] 匹配 'a'，'m'或'k' ... 不在[]中的字符：abc 匹配除了a,b,c之外的字符。 re* 匹配0个或多个的表达式。 re+ 匹配1个或多个的表达式。 re? 匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式 re{n} 精确匹配 n 个前面表达式。例如， o{2} 不能匹配 \"Bob\" 中的 \"o\"，但是能匹配 \"food\" 中的两个 o。 re{n,} 匹配 n 个前面表达式。例如， o{2,} 不能匹配\"Bob\"中的\"o\"，但能匹配 \"foooood\"中的所有 o。\"o{1,}\" 等价于 \"o+\"。\"o{0,}\" 则等价于 \"o*\"。 re{n,m} 匹配 n 到 m次由前面的正则表达式定义的片段，贪婪方式 a\\ b 匹配a或b (re) 对正则表达式分组并记住匹配的文本 (?imx) 正则表达式包含三种可选标志：i, m, 或 x。只影响括号中的区域。 (?-imx) 正则表达式关闭 i, m, 或 x可选标志。只影响括号中的区域。 (?:re) 类似 (...), 但是不表示一个组 (?imx:re) 在括号中使用i, m, 或 x可选标志 (?-imx:re) 在括号中不使用i, m, 或 x可选标志 (?#...) 注释. (?=re) 前向肯定界定符。如果所含正则表达式，以 ... 表示，在当前位置成功匹配时成功，否则失败。但一旦所含表达式已经尝试，匹配引擎根本没有提高；模式的剩余部分还要尝试界定符的右边。 (?!re) 前向否定界定符。与肯定界定符相反；当所含表达式不能在字符串当前位置匹配时成功 (?>re) 匹配的独立模式，省去回溯。 \\w 匹配字母数字及下划线 \\W 匹配非字母数字及下划线 \\s 匹配任意空白字符，等价于 [\\t\\n\\r\\f]. \\S 匹配任意非空字符 \\d 匹配任意数字，等价于 [0-9]. \\D 匹配任意非数字 \\A 匹配字符串开始 \\Z 匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串。 \\z 匹配字符串结束 \\G 匹配最后匹配完成的位置。 \\b 匹配一个单词边界，也就是指单词和空格间的位置。例如， 'er\\b' 可以匹配\"never\" 中的 'er'，但不能匹配 \"verb\" 中的 'er'。 \\B 匹配非单词边界。'er\\B' 能匹配 \"verb\" 中的 'er'，但不能匹配 \"never\" 中的 'er'。 \\n, \\t, 等. 匹配一个换行符。匹配一个制表符。等 \\1...\\9 匹配第n个分组的内容。 \\10 匹配第n个分组的内容，如果它经匹配。否则指的是八进制字符码的表达式。 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/HTML/":{"url":"common/HTML/","title":"HTML","keywords":"","body":"HTML powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/HTML/base.html":{"url":"common/HTML/base.html","title":"基础","keywords":"","body":"基础 基础 占位 loading 空 颜色 快速编写 Emmet 占位 loading 空 颜色 快速编写 Emmet powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Markdown/":{"url":"common/Markdown/","title":"Markdown","keywords":"","body":"Markdown powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Markdown/command.html":{"url":"common/Markdown/command.html","title":"命令","keywords":"","body":"命令 命令 # h1 ## h2 ### h3 #### h4 ##### h5 ###### h6 * li * li * li 1. ol 2. ol 3. ol > 引用 [baidu](www.baidu.com) ![1 png](http:) **blod** *i* | Tables | Are | Cool | | ------------- | :-----------: | ----: | | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | `code` tab *** --- ```sh ``` ```json { \"type\" : \"```json```\" } ``` ```js let type = js ``` powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Markdown/codeblock.html":{"url":"common/Markdown/codeblock.html","title":"代码块","keywords":"","body":"代码块 代码块 名称 关键字 调用的js 说明 AppleScript applescript shBrushAppleScript.js ActionScript3.0 actionscript3 , as3 shBrushAS3.js Shell bash , shell shBrushBash.js ColdFusion coldfusion , cf shBrushColdFusion.js C cpp , c shBrushCpp.js C# c# , c-sharp , csharp shBrushCSharp.js CSS css shBrushCss.js Delphi delphi , pascal , pas shBrushDelphi.js diff&patch diff patch shBrushDiff.js 用代码版本库时,遇到代码冲突,其语法就是这个. Erlang erl , erlang shBrushErlang.js Groovy groovy shBrushGroovy.js Java java shBrushJava.js JavaFX jfx , javafx shBrushJavaFX.js JavaScript js , jscript , javascript shBrushJScript.js Perl perl , pl , Perl shBrushPerl.js PHP php shBrushPhp.js text text , plain shBrushPlain.js 就是普通文本. Python py , python shBrushPython.js Ruby ruby , rails , ror , rb shBrushRuby.js SASS&SCSS sass , scss shBrushSass.js Scala scala shBrushScala.js SQL sql shBrushSql.js Visual Basic vb , vbnet shBrushVb.js XML xml , xhtml , xslt , html shBrushXml.js Objective C objc , obj-c shBrushObjectiveC.js F# f# f-sharp , fsharp shBrushFSharp.js xpp , dynamics-xpp shBrushDynamics.js R r , s , splus shBrushR.js matlab matlab shBrushMatlab.js swift swift shBrushSwift.js GO go , golang shBrushGo.js powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Python/":{"url":"common/Python/","title":"Python","keywords":"","body":"Python Python（英国发音：/ˈpaɪθən/ 美国发音：/ˈpaɪθɑːn/）是一种广泛使用的解释型、高级编程、通用型编程语言，由吉多·范罗苏姆创造，第一版发布于1991年。可以视之为一种改良（加入一些其他编程语言的优点，如面向对象）的LISP。[来源请求]Python的设计哲学强调代码的可读性和简洁的语法（尤其是使用空格缩进划分代码块，而非使用大括号或者关键词）。相比于C++或Java，Python让开发者能够用更少的代码表达想法。不管是小型还是大型程序，该语言都试图让程序的结构清晰明了。 与Scheme、Ruby、Perl、Tcl等动态类型编程语言一样，Python拥有动态类型系统和垃圾回收功能，能够自动管理内存使用，并且支持多种编程范式，包括面向对象、命令式、函数式和过程式编程。其本身拥有一个巨大而广泛的标准库。 Python 解释器本身几乎可以在所有的操作系统中运行。Python的其中一个解释器CPython是用C语言编写的、是一个由社群驱动的自由软件，当前由Python软件基金会管理。 Python powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Python/install.html":{"url":"common/Python/install.html","title":"安装","keywords":"","body":"安装 安装 包管理 pip 安装 更新 包加速 临时设置 全局设值 可以使用的国内镜像 包卸载 包解析 包缓存 指定wheel地址 特殊包 conda 安装: 换源: 添加多版本python 版本切换 pipenv 安装: 指定版本python生成虚拟环境 虚拟环境切换 包管理 pip 安装 安装pip curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py 安装文档: https://pip.pypa.io/en/stable/installing/ 更新 pip install -U pip 包加速 临时设置 -i/--index-url --trusted-host 全局设值 pip config set global.index-url [index-url] pip config set global.trusted-host [index-domain] 可以使用的国内镜像 清华(推荐): https://pypi.tuna.tsinghua.edu.cn/simple pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple/ pip config set global.trusted-host pypi.tuna.tsinghua.edu.cn 豆瓣(不推荐) http://pypi.douban.com/simple/ pip config set global.index-url http://pypi.douban.com/simple/ pip config set global.trusted-host pypi.douban.com 阿里(不推荐) http://mirrors.aliyun.com/pypi/simple/ pip config set global.index-url http://mirrors.aliyun.com/pypi/simple/ pip config set global.trusted-host mirrors.aliyun.com 包卸载 pip uninstall [package] 包解析 pip install pip-tools pip-compile requirements.in -o requirements.txt 包缓存 pip2tgz /var/lib/packages -r requirement.txt 指定wheel地址 pip wheel --wheel-dir=/root/tm_wheel 特殊包 等价代替 pip install pycrypto => pip install pycryptodome conda 安装: 版本列表: https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/ windows: https://docs.conda.io/en/latest/miniconda.html ssh 问题解决: https://blog.csdn.net/Sky_Tree_Delivery/article/details/109078288 将~miniconda3\\Library\\bin添加到环境变量 linux: wget https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh wget https://repo.continuum.io/miniconda/Miniconda3-py39_4.9.2-Linux-x86_64.sh 换源: conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --set show_channel_urls yes conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ 添加多版本python conda create -n python36 python=3.6 版本切换 conda activate python36 conda deactivate pipenv 安装: pip install pipenv 指定版本python生成虚拟环境 pipenv --python [path] 虚拟环境切换 pipenv shell powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Python/project/":{"url":"common/Python/project/","title":"专题","keywords":"","body":"专题 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Python/project/version.html":{"url":"common/Python/project/version.html","title":"version","keywords":"","body":"version version python2.0 python2.6 python3.5 python3.6 python3.7 python3.8 python3.9 python3.10 python2.0 PEP 0 -- Index of Python Enhancement Proposals (PEPs)python2.6 PEP 361 -- Python 2.6 and 3.0 Release Schedule PEP 352 -- Required Superclass for Exceptions 之前居然是个类就可以被raise... PEP 358 -- The \"bytes\" Object PEP 366 -- Main module explicit relative imports if __name__ == \"__main__\" and __package__ is None: __package__ = \"expected.package.name\" PEP 370 -- Per user site-packages directory PEP 3112 -- Bytes literals in Python 3000 PEP 3127 -- Integer Literal Support and Syntax PEP 371 -- Addition of the multiprocessing package to the standard librarypython3.5 PEP 478 -- Python 3.5 Release Schedule PEP 492 -- Coroutines with async and await syntax New syntax for defining coroutines: async def and new await keyword. New await method for Future-like objects, and new tp_as_async.am_await slot in PyTypeObject. New syntax for asynchronous context managers: async with. And associated protocol with aenter and aexit methods. New syntax for asynchronous iteration: async for. And associated protocol with aiter, aexit and new built-in exception StopAsyncIteration. New tp_as_async.am_aiter and tp_as_async.am_anext slots in PyTypeObject. New AST nodes: AsyncFunctionDef, AsyncFor, AsyncWith, Await. New functions: sys.set_coroutine_wrapper(callback), sys.get_coroutine_wrapper(), types.coroutine(gen), inspect.iscoroutinefunction(func), inspect.iscoroutine(obj),inspect.isawaitable(obj), inspect.getcoroutinestate(coro), and inspect.getcoroutinelocals(coro). New CO_COROUTINE and CO_ITERABLE_COROUTINE bit flags for code objects. New ABCs: collections.abc.Awaitable, collections.abc.Coroutine, collections.abc.AsyncIterable, and collections.abc.AsyncIterator. C API changes: new PyCoro_Type (exposed to Python as types.CoroutineType) and PyCoroObject.PyCoro_CheckExact(*o) to test if o is a native coroutine. PEP 465 -- A dedicated infix operator for matrix multiplication PEP 448 -- Additional Unpacking Generalizations PEP 484 -- Type Hints PEP 441 Improving Python ZIP Application Support. python -m zipapp directory [options] -o archive / --output archive -p interpreter / --python interpreter --show PEP 461 -- Adding % formatting to bytes and bytearraypython3.6 PEP 494 -- Python 3.6 Release Schedule PEP 498 -- Literal String Interpolation { } PEP 515 -- Underscores in Numeric Literals PEP 526 -- Syntax for Variable Annotations PEP 525 -- Asynchronous Generators def func(): # a function return pass def genfunc(): # a generator function yield async def coro(): # a coroutine function await smth() async def asyncgen(): # an asynchronous generator function yield 42 types.AsyncGeneratorType -- type of asynchronous generator object. sys.set_asyncgen_hooks() and sys.get_asyncgen_hooks() methods to set up asynchronous generators finalizers and iteration interceptors in event loops. inspect.isasyncgen() and inspect.isasyncgenfunction() introspection functions. New method for asyncio event loop: loop.shutdown_asyncgens(). New collections.abc.AsyncGenerator abstract base class. async def ticker(delay, to): for i in range(to): yield i await asyncio.sleep(delay) async def run(): async for i in ticker(1, 10): print(i) import asyncio loop = asyncio.get_event_loop() try: loop.run_until_complete(run()) finally: loop.close() PEP 530 -- Asynchronous Comprehensions {i async for i in agen()}; [i async for i in agen()]; {i: i ** 2 async for i in agen()}; (i ** 2 async for i in agen()). result = [await fun() for fun in funcs] result = {await fun() for fun in funcs} result = {fun: await fun() for fun in funcs} result = [await fun() for fun in funcs if await smth] result = {await fun() for fun in funcs if await smth} result = {fun: await fun() for fun in funcs if await smth} result = [await fun() async for fun in funcs] result = {await fun() async for fun in funcs} result = {fun: await fun() async for fun in funcs} result = [await fun() async for fun in funcs if await smth] result = {await fun() async for fun in funcs if await smth} result = {fun: await fun() async for fun in funcs if await smth} PEP 506 -- Adding A Secrets Module To The Standard Library PEP 528 -- Change Windows console encoding to UTF-8 PEP 529 -- Change Windows filesystem encoding to UTF-8python3.7 PEP 537 -- Python 3.7 Release Schedule PEP 563 -- Postponed Evaluation of Annotations 类型注释放在 annotation 里， 推迟判断 async and await are now reserved keywords. PEP 567 -- Context Variables ipython里好像每个交互都是独立的 PEP 557 -- Data Classes @dataclass def dataclass(*, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False) PEP 553 -- Built-in breakpoint() PEP 562 -- Module getattr and dir # lib.py from warnings import warn deprecated_names = [\"old_function\", ...] def _deprecated_old_function(arg, other): ... def __getattr__(name): if name in deprecated_names: warn(f\"{name} is deprecated\", DeprecationWarning) return globals()[f\"_deprecated_{name}\"] raise AttributeError(f\"module {__name__} has no attribute {name}\") # main.py from lib import old_function # Works, but emits the warning # lib/__init__.py import importlib __all__ = ['submod', ...] def __getattr__(name): if name in __all__: return importlib.import_module(\".\" + name, __name__) raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\") # lib/submod.py print(\"Submodule loaded\") class HeavyClass: ... # main.py import lib lib.submod.HeavyClass # prints \"Submodule loaded\" PEP 560 -- Core support for typing module and generic types ```py class MyList: def __getitem__(self, index): return index + 1 def __class_getitem__(cls, item): return f\"{cls.__name__}[{item.__name__}]\" class MyOtherList(MyList): pass assert MyList()[0] == 1 assert MyList[int] == \"MyList[int]\" assert MyOtherList()[0] == 1 assert MyOtherList[int] == \"MyOtherList[int]\" class GenericAlias: def __init__(self, origin, item): self.origin = origin self.item = item def __mro_entries__(self, bases): return (self.origin,) class NewList: def __class_getitem__(cls, item): return GenericAlias(cls, item) class Tokens(NewList[int]): ... assert Tokens.__bases__ == (NewList,) assert Tokens.__orig_bases__ == (NewList[int],) assert Tokens.__mro__ == (Tokens, NewList, object) ``` PEP 564 -- New Time Functions With Nanosecond Resolution PEP 538 -- Legacy C Locale Coercion PEP 540 -- Forced UTF-8 Runtime Mode PEP 552 -- Hash-based .pyc Files PEP 565 -- improved DeprecationWarning handling PEP 539 -- new C API for thread-local storagepython3.8 PEP 569 -- Python 3.8 Release Schedule PEP 572 -- Assignment Expressions # Handle a matched regex if (match := pattern.search(data)) is not None: # Do something with match # A loop that can't be trivially rewritten using 2-arg iter() while chunk := file.read(8192): process(chunk) # Reuse a value that's expensive to compute [y := f(x), y**2, y**3] # Share a subexpression between a comprehension filter clause and its output filtered_data = [y for x in data if (y := f(x)) is not None] PEP 570 -- Python Positional-Only Parameters position_only, /，position_and_keyword, *, keyword_only PEP 578 -- Python Runtime Audit Hooks PEP 587 -- Python Initialization Configurationpython3.9 PEP 596 -- Python 3.9 Release Schedule PEP 573 -- Module State Access from C Extension Methods PEP 584 -- Add Union Operators To dict PEP 585 -- Type Hinting Generics In Standard Collections PEP 593 -- Flexible function and variable annotations PEP 614 -- Relaxing Grammar Restrictions On Decorators PEP 617 -- New PEG parser for CPythonpython3.10 PEP 619 -- Python 3.10 Release Schedule powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Python/project/python2to3.html":{"url":"common/Python/project/python2to3.html","title":"2to3","keywords":"","body":"2to3 2to3 python2to3 编码 python2to3 2to3 --output-dir=[output_dirpath] -W -n [input_dirpath] 编码 字符编码 ascii utf8 1）对于单字节的符号，字节的第一位设为0，后面7位为这个符号的unicode码。因此对于英语字母，UTF-8编码和ASCII码是相同的。 2）对于n字节的符号（n>1），第一个字节的前n位都设为1，第n+1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的unicode码。 utf16 utf32 GB2312 GBK GB18030 编码规范 unicode 转换 decode 方法 str.decode unicode 用处 str->unicode encode 方法 unicode.encode 用处 unicode->str 其他 都派生于basestring sys.setdefaultencoding powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Python/project/GIL.html":{"url":"common/Python/project/GIL.html","title":"GIL","keywords":"","body":"GIL OLD GIL 系统决定切换 存在竞争导致的 NEW GIL 系统设置超时， 修改状态，出让，回报状态 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Python/project/pre-commit.html":{"url":"common/Python/project/pre-commit.html","title":"pre-commit","keywords":"","body":"pre-commit pre-commit pre-commit 插件列表 插件 black flake8 链接 pre-commit 安装git钩子 pip install pre-commit pre-commit install 初始化下项目 pre-commit run --all-files 自动合并配置 pre-commit migrate-config 自动更新 hooks pre-commit autoupdate 配置文件pre-commit-config.yaml示例 repos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v3.2.0 hooks: - id: trailing-whitespace - repo: https://github.com/ambv/black rev: 20.8b1 hooks: - id: black 插件列表 名称 效果 black 代码格式化 flake8 代码格式校验 插件 black 配置文件pyproject.toml示例 [tool.black] line-length = 108 py36 = true skip-string-normalization = true include = '\\.pyi?$' exclude = ''' /( \\.git | \\.hg | \\.mypy_cache | \\.tox | \\.venv | __pycache__ )/ ''' flake8 配置文件.flake8示例 # file: $PROJ/.flake8 # # This config expects that the flake8-bugbear extension to be installed. # bugbear looks at the line length and allows a slight variance as opposed # to a hard limit. When it detects excessive line lengths, it returns B950. # This config looks for B950 and ignores the default flake8 E501 line length error. [flake8] max-complexity = 50 max-line-length = 108 select = C,E,F,W,B ignore = F401,F841,W503,E501,E126 exclude = *migrations*, *.pyc, .git, .cover, __pycache__, */node_modules/*, # Local Variables: # mode: conf # End: 链接 pre-commit document: https://pre-commit.com/#intro black document: https://black.readthedocs.io/en/stable/index.html flake document: https://flake8.pycqa.org/en/latest/# flake8 rules: https://www.flake8rules.com/ powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Python/project/logging.html":{"url":"common/Python/project/logging.html","title":"logging","keywords":"","body":"logging logging 介绍 参数 机制 介绍 Logging是python自带的模块，这个模块支持输出不同级别的日志，可以输出到控制台和写入文件，支持TCP、HTTP、GET/POST、SMTP、Socket等协议，将日志信息发送到网络等等。 参数 logging.basicConfig()函数中的具体参数: 参数 效果 filename 指定的文件名创建FiledHandler，这样日志会被存储在指定的文件中； filemode 文件打开方式，在指定了filename时使用这个参数，默认值为“w”还可指定为“a”； format 指定handler使用的日志显示格式； datefmt 指定日期时间格式。， 格式参考strftime时间格式化（下文） level 设置rootlogger的日志级别 stream 用指定的stream创建StreamHandler。可以指定输出到sys.stderr,sys.stdout或者文件，默认为sys.stderr。若同时列出了filename和stream两个参数，则stream参数会被忽略。 format参数中可能用到的格式化信息： 参数 效果 %(name)s Logger的名字 %(levelno)s 数字形式的日志级别 %(levelname)s 文本形式的日志级别 %(pathname)s 调用日志输出函数的模块的完整路径名，可能没有 %(filename)s 调用日志输出函数的模块的文件名 %(module)s 调用日志输出函数的模块名 %(funcName)s 调用日志输出函数的函数名 %(lineno)d 调用日志输出函数的语句所在的代码行 %(created)f 当前时间，用UNIX标准的表示时间的浮 点数表示 %(relativeCreated)d 输出日志信息时的，自Logger创建以 来的毫秒数 %(asctime)s 字符串形式的当前时间。默认格式是 “2003-07-08 16:49:45,896”。逗号后面的是毫秒 %(thread)d 线程ID。可能没有 %(threadName)s 线程名。可能没有 %(process)d 进程ID。可能没有 %(message)s 用户输出的消息 python中时间日期格式化符号： 参数 效果 %y 两位数的年份表示（00-99） %Y 四位数的年份表示（000-9999） %m 月份（01-12） %d 月内中的一天（0-31） %H 24小时制小时数（0-23） %I 12小时制小时数（01-12） %M 分钟数（00=59） %S 秒（00-59） %a 本地简化星期名称 %A 本地完整星期名称 %b 本地简化的月份名称 %B 本地完整的月份名称 %c 本地相应的日期表示和时间表示 %j 年内的一天（001-366） %p 本地A.M.或P.M.的等价符 %U 一年中的星期数（00-53）星期天为星期的开始 %w 星期（0-6），星期天为星期的开始 %W 一年中的星期数（00-53）星期一为星期的开始 %x 本地相应的日期表示 %X 本地相应的时间表示 %Z 当前时区的名称 %% %号本身 机制 graph TD; 开始 --> 结束; powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/NodeJS/":{"url":"common/NodeJS/","title":"NodeJS","keywords":"","body":"NodeJS powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/NodeJS/install.html":{"url":"common/NodeJS/install.html","title":"安装","keywords":"","body":"安装 安装 安装 centos 安装 windows 安装 包管理 npm yarn cnpm 包加速 临时设置 全局设置 可用的国内源 查看\\设置 npm 安装路径 安装 centos 安装 rpm -ivh http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6 rpm -ivh http://rpms.famillecollet.com/enterprise/remi-release-6.rpm rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-remi curl --silent --location https://rpm.nodesource.com/setup_6.x | bash - yum install -y gcc-c++ make yum install -y nodejs npm install -g cnpm --registry=https://registry.npm.taobao.org windows 安装 下载 包管理 npm yarn npm install yarn -g --registry https://registry.npm.taobao.org cnpm npm install yarn -g --registry https://registry.npm.taobao.org 包加速 临时设置 --registry 全局设置 npm config set registry https://registry.npm.taobao.org 可用的国内源 https://registry.npm.taobao.org 查看\\设置 npm 安装路径 npm config get prefix npm config set prefix powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Java/":{"url":"common/Java/","title":"Java","keywords":"","body":"Java powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Java/install.html":{"url":"common/Java/install.html","title":"安装","keywords":"","body":"安装 安装 安装 安装 下载: https://www.java.com/zh_CN/download/win10.jsp powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Clojure/":{"url":"common/Clojure/","title":"Clojure","keywords":"","body":"Clojure powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Clojure/install.html":{"url":"common/Clojure/install.html","title":"安装","keywords":"","body":"安装 安装 安装 Clojure 安装 lein 安装 Clojure 安装Java: Java install 下载: https://github.com/clojure/tools.deps.alpha/wiki/clj-on-Windows 安装 lein 地址: https://leiningen.org/#install lein self-install powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/RabbitMQ/":{"url":"common/RabbitMQ/","title":"RabbitMQ","keywords":"","body":"RabbitMQ # 队列查询 rabbitmqctl list_vhosts rabbitmqctl list_queues -p clickhouse powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/RabbitMQ/install.html":{"url":"common/RabbitMQ/install.html","title":"安装","keywords":"","body":"安装 安装 docker 配置 设置 docker image: https://hub.docker.com/_/rabbitmq docker pull rabbitmq:3-management docker volume create --name rabbitmqconfig docker run -d --hostname rabbit --name rabbit -p 15672:15672 -p 5672:5672 -v rabbitmqconfig:/var/lib/rabbitmq rabbitmq:3-management # user/password: guest/guest 配置 文档: https://www.rabbitmq.com/configure.html#customise-environment 设置 rabbitmqctl add_user rabbitmqctl add_vhost rabbitmqctl set_permissions -p '.*' '.*' '.*' rabbitmqctl set_user_tags administrator e.g. rabbitmqctl add_user clickhouse clickhouse rabbitmqctl add_vhost clickhouse rabbitmqctl set_permissions -p clickhouse clickhouse '.*' '.*' '.*' rabbitmqctl set_user_tags clickhouse administrator powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/RabbitMQ/project/":{"url":"common/RabbitMQ/project/","title":"专题","keywords":"","body":"专题 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/RabbitMQ/project/check.html":{"url":"common/RabbitMQ/project/check.html","title":"问题排查","keywords":"","body":"问题排查 问题排查 2022/07/01 rabbitmq ConnectionResetError (104, 'Connection reset by peer') 2022/07/01 rabbitmq ConnectionResetError (104, 'Connection reset by peer') rabbitmqctl set_vm_memory_high_watermark 0.6 rabbitmqctl set_vm_memory_high_watermark absolute \"4G\" 文档 https://www.rabbitmq.com/memory.html powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Kafka/":{"url":"common/Kafka/","title":"Kafka","keywords":"","body":"Kafka powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/MongoDB/":{"url":"common/MongoDB/","title":"MongoDB","keywords":"","body":"MongoDB MongoDB powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/MongoDB/install.html":{"url":"common/MongoDB/install.html","title":"安装","keywords":"","body":"安装 安装 docker window 下载安装 配置启动任务 linux 下载安装 yum安装 3.6 4.0 配置 运行 docker https://hub.docker.com/_/mongo/ docker pull mongo docker volume create --name mongodb docker volume create --name mongoconfigdb docker run --name mongod -v mongodb:/data/db -v mongoconfigdb:/data/configdb -p 27017:27017 -d mongo window 下载安装 https://www.mongodb.com/try/download/community 配置启动任务 .\\mongod.exe --config .\\mongod.conf --directoryperdb --serviceName Mongodb --install #mongod.conf dbpath=F:\\mongodb\\db logpath=F:\\mongodb\\logs\\mongod.log logappend=true journal=true quiet=true bind_ip=127.0.0.1 port=27017 如果碰到错误100, 尝试删除db文件夹下的mongod.lock与storage.bson linux 下载安装 下载址定版本并解压 wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-3.2.17.tgz tar -zxvf mongodb-linux-x86_64-3.2.17.tgz -C /usr/local/ cd /usr/local/ mv mongodb-linux-x86_64-3.2.17 mongodb3.2.17 配置 cat >> /etc/profile yum安装 3.6 cd /etc/yum.repos.d cat >> mongodb-org-3.6.repo 4.0 cd /etc/yum.repos.d cat >> mongodb-org-4.0.repo 配置 vim /etc/mongod.conf # mongod.conf # for documentation of all options, see: # http://docs.mongodb.org/manual/reference/configuration-options/ # where to write logging data. systemLog: destination: file logAppend: true logRotate: reopen path: /var/log/mongodb/mongod.log # Where and how to store data. storage: dbPath: /var/lib/mongo journal: enabled: true engine: wiredTiger mmapv1: nsSize: 1024 # wiredTiger: directoryPerDB: true # how the process runs processManagement: fork: true # fork and run in background pidFilePath: /var/run/mongodb/mongod.pid # location of pidfile # network interfaces net: port: 27017 bindIp: 127.0.0.1,172.16.66.131 # Listen to local interface only, comment to listen on all interfaces. #security: #operationProfiling: #replication: #sharding: ## Enterprise-Only Options #auditLog: #snmp: 运行 环境:DB1:192.168.200.201 创建数据和日志目录 mkdir -p /data/database/mongodb/cemdb mkdir -p /data/database/mongodb/cemdb_slave mkdir -p /data/log/mongodb chmod -R 777 /data/database/mongodb/cemdb chmod -R 777 /data/database/mongodb/cemdb_slave chmod -R 777 /data/log/ 启动mongod服务 mongod --dbpath /data/database/mongodb/cemdb --logpath=/data/log/mongodb/cemdb.log --logappend --logRotate reopen Master: numactl --interleave=all /usr/local/mongodb3.2.17/bin/mongod --dbpath=/data/database/mongodb/cemdb --logpath=/data/log/mongodb/cemdb.log --logappend --logRotate reopen --fork --directoryperdb --nssize=1024 --port=27017 --bind_ip=10.183.188.184 --storageEngine=wiredTiger --master Slave: numactl --interleave=all /usr/local/mongodb3.2.17/bin/mongod --dbpath=/data/database/mongodb/cemdb_slave --logpath=/data/log/mongodb/cemdb.log --logappend --logRotate reopen --fork --directoryperdb --nssize=1024 --port=27017 --source=10.183.188.184 --bind_ip=10.183.188.179 --storageEngine=wiredTiger --slave 检查状态 netstat -nltp | grep mongod powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/MongoDB/command.html":{"url":"common/MongoDB/command.html","title":"命令","keywords":"","body":"命令 命令 创建 查询 查询条件 显示规则 更新 更新规则 是否复数修改 删除 其他 Aggregation MapReduce index 文档 创建 insert insertOne insertMany 查询 find [查询条件] [显示规则] count distinct查询条件 $or $not $exists $in $nin $mod $lt $gt $ne $all $each $size $where $search $regex: /reg/ 显示规则 $slice 更新 update [查询条件] [更新规则] [是否追加] [是否复数修改] updateOne updateMany replaceOne更新规则 $inc $set $unset $push $pull $pop $pop: {key: 1} 数组末尾 $pop: {key： -1} 数组开头 $addToSet是否复数修改 {$mutli: 1} 删除 deleteOne deleteMany remove drop dropDatabase 其他 sort limit skip Aggregation $match $redact $addFields $project $group MapReduce map reduce query output index ensureIndex createIndex {$unique: true} {$dropDup: true} 文档 官方文档: https://docs.mongodb.com/manual/introduction/ powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/MongoDB/project/":{"url":"common/MongoDB/project/","title":"专题","keywords":"","body":"专题 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/MongoDB/project/version.html":{"url":"common/MongoDB/project/version.html","title":"version","keywords":"","body":"version powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/MongoDB/project/aggregate.html":{"url":"common/MongoDB/project/aggregate.html","title":"聚合查询","keywords":"","body":"聚合查询 聚合查询 MapReduce 常用聚合写法 查询并修改 查询并修改2 关联查询 多层关联查询 多层关联查询2 分组后按数量排序 统计数量分段 MapReduce EX1 var mapFunction1 = function() { emit(this.cust_id, this.price); }; var reduceFunction1 = function(keyCustId, valuesPrices) { return Array.sum(valuesPrices); }; db.orders.mapReduce( mapFunction1, reduceFunction1, { out: \"map_reduce_example\" } ) EX2 var map = function() {for (var key in this) {emit(key, {count: 1})}} var reduce = function(key, emits) {total = 0; for (var i in emits) {total += emits[i].count;} return {\"count\": total}} 常用聚合写法 BI实现思路， 实现了多维度交叉， 度量的总计， 计数， 去重计数， 平均 db.getCollection('USER_GROUP').aggregate([ { '$match': { \"project_id\": \"5cdccfa1b047aa1f77f8703b\" } }, { '$addFields': { \"All\": \"All\" } }, { '$group': { \"_id\": { \"column_id_4\": \"$groupLevelID\", \"column_id_5\": \"$email\" }, \"column_id_1\": { \"$sum\": \"$seq\" }, \"column_id_2\": { \"$sum\": 1 }, \"column_id_3\": { \"$addToSet\": \"$groupType\" }, \"column_id_6\": { \"$avg\": \"$seq\" } } }, { '$project': { \"_id\": 0, \"column_id_1\": 1, \"column_id_2\": 1, \"column_id_3\": {\"$size\": \"$column_id_3\"}, \"column_id_4\": \"$_id.column_id_4\", \"column_id_5\": \"$_id.column_id_5\", \"column_id_6\": 1 } } ]) 查询并修改 db.DELIVER_DELIVER。find().forEach(function(item){db.getCollection('DELIVER_DELIVER').update({\"_id\": item._id},{$set:{\"name\": item.code}})}) 查询并修改2 db.BI_COLUMN.aggregate([ { \"$lookup\": { from: 'BI_WORKTABLE', localField: 'worktable_id', foreignField: \"_id\", as: \"worktable_doc\" } }, { \"$unwind\": { path: \"$worktable_doc\", preserveNullAndEmptyArrays: true } }] ).forEach( function(item){ if(item.worktable_doc) { db.BI_COLUMN.update( { \"_id\": item._id }, { $set: { \"orgID\": item.worktable_doc.orgID } }, false, true ) } } ) 关联查询 db.getCollection('BI_COLUMN').aggregate([ { $lookup: { from: \"BI_WORKTABLE\", let: { \"worktable_id\": \"$worktable_id\" }, \"pipeline\": [ { $match: { $expr: { $eq: [\"$_id\", \"$$worktable_id\"] } } }, ], as: \"question_list\" } } ]) 多层关联查询 db.getCollection('DELIVER_SMSTEMPLATE').aggregate([ { $lookup: { from: \"DELIVER_SMSDELIVER\", let: { \"template_id\": \"$_id\" }, pipeline: [ { $match: { $expr: { $eq: [\"$sms_template_id\", \"$$template_id\"] } } }, { $lookup: { from: \"DELIVER_SMSRECORD\", let: { \"deliver_id\": \"$_id\" }, pipeline: [ { $match: { $expr: { $eq: [\"$sms_deliver_id\", \"$$deliver_id\"] } } } ], as: \"record_list\" } } ], as: \"deliver_list\" } } ]) 多层关联查询2 db.getCollection('BI_COLUMN').aggregate([ { $lookup: { from: \"BI_WORKTABLE\", localField: \"worktable_id\", foreignField: \"_id\", as: \"worktable_docs\" } }, { $addFields: { datasource_id: \"$worktable_docs.datasource_id\" } }, { $lookup: { from: \"BI_DATASOURCE\", localField: \"worktable_docs.datasource_id\", foreignField: \"_id\", as: \"datasource_docs\" } } ]) 分组后按数量排序 db.getCollection('BI_COLUMN').aggregate([ { $match: { dtype: 5, ttype: 0 } }, { $group: { _id: { \"worktable\": \"$worktable_id\" }, \"number\": { \"$sum\": 1 } } }, { $project: { \"_id\": 1, \"number\": 1, } }, { $sort: { number: -1 } } ]) 统计数量分段 db.getCollection('BI_COLUMN').aggregate([ { $match: { ttype: 0, dtype: 1 } }, { $group: { _id: { \"worktable\": \"$worktable_id\" }, \"number\": { \"$sum\": 1 } } }, { $project: { \"_id\": 0, \"number\": 1, \"steps\": { \"$cond\": { \"if\": { \"$lt\": ['$number', 200] }, \"then\": \"1000\" } } } } } } } } } }, { $group: { _id: { \"steps\": \"$steps\" }, \"stepnumber\": { \"$sum\": 1 } } } ]) powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/MongoDB/project/export.html":{"url":"common/MongoDB/project/export.html","title":"导入导出","keywords":"","body":"导入导出 导入导出 同步整个库 特殊功能参数说明 同步指定表 同步整个库 mongodump -h IP --port 端口 -u 用户名 -p 密码 -d 数据库 -o 文件存在路径 mongorestore -h IP --port 端口 -u 用户名 -p 密码 -d 数据库 --drop 文件存在路径 mongorestore --gzip --archive=dump.gz 库名是 cem_server_vehicel 特殊功能参数说明 # 如果mongo版本不一致 # Mongodump: Unrecognized field 'snapshot' --forceTableScan 同步指定表 mongoexport -h IP --port 端口 -u 用户名 -p 密码 -d 数据库 -c 表名 -f 字段 -q 条件导出 --csv -o 文件名 mongoimport -h IP --port 端口 -u 用户名 -p 密码 -d 数据库 -c 表名 --upsert 插入或者更新现有数据 --drop 文件名 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/MongoDB/project/profile.html":{"url":"common/MongoDB/project/profile.html","title":"优化","keywords":"","body":"优化 优化 慢日志 通过命令行配置 通过ini文件配置 通过yaml文件配置 慢查询常用命令 MongoDB慢查询日志解析 慢日志 通过命令行配置 # 查询慢查询级别和其它信息 db.getProfilingStatus() # 仅返回慢查询级别 db.getProfilingLevel() # 禁用慢查询 db.setProfilingLevel(0) # 指定数据库，并指定阈值慢查询 ，超过20毫秒的查询被记录 use testdb.setProfilingLevel(1, { slowms: 20 }) # 随机采集慢查询的百分比值，sampleRate 值默认为1，表示都采集，0.42 表示采集42%的内容。 db.setProfilingLevel(1, { sampleRate: 0.42 }) # 为所有数据库开启慢查询记录 db.setProfilingLevel(2) 通过ini文件配置 profile = 1 slowms = 300 通过yaml文件配置 operationProfiling: mode:# 默认为 off，可选值 off、slowOp(对应上面的等级 1)、all(对应上面的等级 2) slowOpThresholdMs:# 阈值，默认值为100，单位毫秒 slowOpSampleRate:# 随机采集慢查询的百分比值，sampleRate 值默认为1，表示都采集，0.42 表示采集42%的内容 慢查询常用命令 # 查询最近的10个慢查询日志 db.system.profile.find().limit(10).sort( { ts : -1 } ).pretty() # 查询除命令类型为 ‘command’ 的日志 db.system.profile.find( { op: { $ne : 'command' } } ).pretty() # 查询数据库为 mydb 集合为 test 的 日志 db.system.profile.find( { ns : 'mydb.test' } ).pretty() # 查询 低于 5毫秒的日志 db.system.profile.find( { millis : { $gt : 5 } } ).pretty() # 查询时间从 2012-12-09 3点整到 2012-12-09 3点40分之间的日志 db.system.profile.find({ ts : { $gt: new ISODate(\"2012-12-09T03:00:00Z\"), $lt: new ISODate(\"2012-12-09T03:40:00Z\") } }).pretty() MongoDB慢查询日志解析 { \"op\" : \"query\", # 操作类型，值可为command、count、distinct、geoNear、getMore、group、insert、mapReduce、query、remove、update \"ns\" : \"test.report\", # 操作的数据库和集合 \"command\" : { # 命令 \"find\" : \"report\", # 操作的集合 \"filter\" : { \"a\" : { \"$lte\" : 500 } }, # 查询条件 \"lsid\" : { \"id\" : UUID(\"5ccd5b81-b023-41f3-8959-bf99ed696ce9\") #用户的会话id }, \"$db\" : \"test\" # 操作的数据库 }, \"cursorid\" : 33629063128, # query和getmore 的游标id \"keysExamined\" : 101, # MongoDB为执行操作而扫描的索引键的数量 \"docsExamined\" : 101, # MongoDB为了执行操作而扫描的集合中的文档数。 \"numYield\" : 2, # 让步次数，操作时让其他的操作完成的次数。 \"nreturned\" : 101, # 操作返回的文档数 \"queryHash\" : \"811451DD\", # 查询的hash值 \"planCacheKey\" : \"759981BA\", \"locks\" : { # 操作期间的锁和所的类型 \"Global\" : { #表示全局锁定 \"acquireCount\" : { #锁定的次数 \"r\" : NumberLong(3) # 表示共享锁 } }, \"Database\" : { # 数据库锁 \"acquireCount\" : { \"r\" : NumberLong(1) }, \"acquireWaitCount\" : { \"r\" : NumberLong(1) }, \"timeAcquiringMicros\" : { \"r\" : NumberLong(69130694) } }, \"Collection\" : { # 集合锁 \"acquireCount\" : { \"r\" : NumberLong(1) } } }, \"storage\" : { # 储存 \"data\" : { \"bytesRead\" : NumberLong(14736), #操作 从磁盘放到缓存的数据的字节数 \"timeReadingMicros\" : NumberLong(17) # 操作 花费在磁盘读取的时间，以微妙为单位 } }, \"responseLength\" : 1305014, # 操作返回结果的文档长度，单位为字节 \"protocol\" : \"op_msg\", # 消息的协议 \"millis\" : 69132, # 从 MongoDB 操作开始到结束耗费的时间 \"planSummary\" : \"IXSCAN { a: 1, _id: -1 }\", # 摘要 \"execStats\" : { # 操作执行过程中的详细信息 \"stage\" : \"FETCH\", # 操作形式 ，COLLSCAN 用于集合扫描，IXSCAN 用于扫描索引键，FETCH 用于检索文档 \"nReturned\" : 101, # 返回的文档数量 \"executionTimeMillisEstimate\" : 0, \"works\" : 101, \"advanced\" : 101, \"needTime\" : 0, \"needYield\" : 0, \"saveState\" : 3, \"restoreState\" : 2, \"isEOF\" : 0, \"invalidates\" : 0, \"docsExamined\" : 101, \"alreadyHasObj\" : 0, \"inputStage\" : { ... } }, \"ts\" : ISODate(\"2019-01-14T16:57:33.450Z\"), #操作的时间戳 \"client\" : \"127.0.0.1\", # 客户端的ip \"appName\" : \"MongoDB Shell\", #客户端应用标识符 \"allUsers\" : [ { \"user\" : \"someuser\", # 用户 \"db\" : \"admin\" # 验证的数据库 } ], \"user\" : \"someuser@admin\" # 经过验证的用户 } powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/MySQL/":{"url":"common/MySQL/","title":"MySQL","keywords":"","body":"MySQL powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/MySQL/install.html":{"url":"common/MySQL/install.html","title":"安装","keywords":"","body":"安装 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/MySQL/command.html":{"url":"common/MySQL/command.html","title":"命令","keywords":"","body":"命令 命令 库操作 表操作 字段操作 索引操作 配置操作 数据操作 库操作 建数据库 CREATE DATABASE 删数据库 DROP DATABASE 备份 USE master EXEC sp_addumpdevice 'disk', 'testBack', 'c:\\mssql7backup\\MyNwind_1.dat' BACKUP DATABASE TO testBack 表操作 建数据表 CREATE TABLE ( col1 type1 [not null] [primary key], col2 type2 [not null], .. ) 基于旧表建新表 CREATE TABLE like 基于旧表建新表, 只同步表结构 CREATE TABLE AS SELECT col1, col2, … FROM definition only 删数据表 DROP TABLE 字段操作 字段级别操作ALTER TABLE ADD column col type 添加主键 Alter TABLE tabname add primary key(col) 删除主键 Alter TABLE tabname drop primary key(col) 索引操作 创建索引 CREATE [unique] index idxname on tabname(col….) 删除索引 DROP index idxname 配置操作 数据库字段编码修改ALTER TABLE CONVERT TO CHARACTER SET utf8 COLLATE 'utf8_unicode_ci'; 数据操作 选择 select * from table1 where 插入 insert into table1(field1,field2) values(value1,value2) 删除 delete from table1 where 更新 update table1 set field1=value1 where 查找 select * from table1 where field1 like ’%value1%’ powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/MySQL/project/":{"url":"common/MySQL/project/","title":"专题","keywords":"","body":"专题 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/MySQL/project/best_practice.html":{"url":"common/MySQL/project/best_practice.html","title":"最佳实践","keywords":"","body":"最佳实践 最佳实践 在编写生产代码时，应该遵循如下指导方针。 SQL 优化 可能会导致慢查询的原因及建议 相关工具 在编写生产代码时，应该遵循如下指导方针。 获取单条记录时，要多用 first 方法，尽量不要用 fetchone 和 scalar 方法，因为对程序员来说，first 方法更加清晰。 尽量使用可迭代对象 ResultProxy，而不要用 fetchall 和 fetchone 方法，因为前者的内存效率更高，而且我们往往一次只对一条记录进行操作。 避免使用 fetchone 方法，因为如果不小心，它会一直让连接处于打开状态。 谨慎使用 scalar 方法，因为如果查询返回多行多列，就会引发错误，多行多行在测试过程中经常会丢失。 SQL 优化 Parsing-Binding-Optimization-Execution 可能会导致慢查询的原因及建议 join和where条件中存在函数 触发了隐式转换 查询条件中存在OR -> 最好可以分开查询 使用了%LIKE%查询 -> 根据条件尽量使用%LIKE, LIKE%以使用索引 大批量写入（会导致锁表） 缺失索引 多表合并 Query Hints 相关工具 执行计划 统计IO 执行耗时 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/MySQL/project/aggregate.html":{"url":"common/MySQL/project/aggregate.html","title":"聚合查询","keywords":"","body":"聚合查询 聚合查询 删除重复数据 整理字符集 删除重复数据 delete from windmachine where id not in ( select * from ( select max(id) as max_id from windmachine group by address ) b where windmachine.id = b.max_id ) 整理字符集 -- 修改数据库表校对规则SQL，执行时将表中列的校对规则一并修改。 delimiter // DROP procedure if exists `alter_table_character` // -- 若已存在则删除 CREATE procedure `alter_table_character`() BEGIN DECLARE f_name varchar(100); DECLARE b int default 0; /*是否达到记录的末尾控制变量*/ -- 注意修改下面的数据库名称 wsm_aliyun DECLARE table_name cursor for SELECT TABLE_NAME FROM information_schema.TABLES where TABLE_SCHEMA = 'wsm_aliyun' and TABLE_NAME like 'wsm_%' AND TABLE_COLLATION = 'utf8_unicode_ci'; DECLARE CONTINUE HANDLER FOR NOT FOUND SET b = 1; -- 打开游标 OPEN table_name; REPEAT FETCH table_name INTO f_name; /*获取第一条记录*/ SET @STMT := CONCAT( \"ALTER TABLE \", f_name, \" CONVERT TO CHARACTER SET utf8 COLLATE utf8_general_ci;\" ); PREPARE STMT FROM @STMT; EXECUTE STMT; -- INSERT into TestTable(name) VALUES (f_name); -- ALTER TABLE f_name CONVERT TO CHARACTER SET utf8 COLLATE utf8_general_ci; UNTIL b = 1 END REPEAT; CLOSE table_name; END; // /*切换回系统默认的命令结束标志*/ delimiter; -- 执行存储过程 call alter_table_character(); -- 修改数据库的校对规则 set names 'utf8' collate 'utf8_general_ci'; -- 查询修改的结果，其实还可以用下面的语句生成相应的SQL，执行这个SQL来完成修改，当然没有上面的存储过程效率高。 -- 查看数据库的校对规则，结果全都为：utf8_general_ci，表示已修改 show variables like 'collation_%'; -- 查看数据库的校对规则，没有数据表明已全部修改。 SELECT CONCAT( 'alter table ', TABLE_NAME, ' CONVERT TO CHARACTER SET utf8 COLLATE utf8_general_ci;' ) as new_sql FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'wsm_aliyun' AND TABLE_NAME LIKE 'wsm_%' -- 数据库名称 AND TABLE_COLLATION = 'utf8_unicode_ci'; -- 查询列结果，没有数据表明已全部修改。 SELECT CONCAT( 'ALTER TABLE `', table_name, '` MODIFY `', column_name, '` ', DATA_TYPE, '(', CHARACTER_MAXIMUM_LENGTH, ') CHARACTER SET UTF8 COLLATE utf8_general_ci;' ) as new_sql FROM information_schema.COLUMNS WHERE TABLE_SCHEMA = 'wsm_aliyun' -- 数据库名 AND TABLE_NAME LIKE 'wsm_%' AND DATA_TYPE = 'varchar' AND CHARACTER_SET_NAME = 'utf8' AND COLLATION_NAME = 'utf8_unicode_ci'; powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/MySQL/project/feature8.0.html":{"url":"common/MySQL/project/feature8.0.html","title":"8.0特性","keywords":"","body":"8.0特性 8.0特性 mysql版本 功能 X-Pulgin ID 索引 增删改查 指定collection 增 查 改 删 优缺点 优点 缺点 参考文献 mysql版本 5.5 (Oracle接受) 5.6 5.7 5.7.8 MySQL数据库从5.7.8版本开始，也提供了对JSON的支持。 8.0 新的系统字典表 整合了存储有关数据库对象信息的事务数据字典，所有的元数据都用InnoDB引擎进行存储 权限支持role MySQL 8.0更好支持文档型数据库和JSON 不可见索引，开始支持invisible index，在优化SQL的过程中可以设置索引为不可见，优化器不会利用不可见索引 支持降序索引，可以对索引定义 DESC，之前，索引可以被反序扫描，但影响性能，而降序索引就可以高效的完成 支持RANK(), LAG()、NTILE()等函数 正则表达式增强，提供了REGEXP_LIKE()，EGEXP_INSTR(), REGEXP_REPLACE(), REGEXP_SUBSTR()等函数 新增备份锁，允许在线备份期间的DML，同时防止可能导致快照不一致的操作。 备份锁由LOCK INSTANCE FOR BACKUP和UNLOCK INSTANCE语法支持 默认字符集由latin1变为utf8mb4 8.0.11 A MySQL server sets an _id value if the document does not contain the _id field 8.0.17 Indexing Array Fields 功能 X-Pulgin 8开始默认开启， 但是我看5.6开始就已经存在，但是支持的CURD方法什么的不完善 X Plugin is enabled by default in MySQL 8, therefore installing or upgrading to MySQL 8 makes the plugin available. 需要基于 X Protocol 的connector去链接 ID 会创建一个默认的 “_id” 字段(The document ID is a VARBINARY() with a maximum length of 32 characters.), 格式如下: unique_prefix start_timestamp serial 4 bytes 8 bytes 16 bytes 一个示例：0000 5a640138 000000000000002f In MySQL 8.0.11 and higher, document IDs are generated by the server, not the client, so MySQL Shell does not automatically set an _id value. A MySQL server at 8.0.11 or higher sets an _id value if the document does not contain the _id field. A MySQL server at an earlier 8.0 release or at 5.7 does not set an _id value in this situation, so you must specify it explicitly. If you do not, MySQL Shell returns error 5115 Document is missing a required field. 索引 JSON类型字段不支持索引，不过感觉我们也没有用json字段做索引的 因为没有schema，索引需要以这样的语法创建 myCollection.createIndex(\"count\", {fields:[{\"field\": \"$.count\", \"type\":\"INT\", required:true}]}); 多字段联合索引 myCollection.createIndex( 'myIndex', {fields: [ {field: '$.myField', type: 'TEXT'}, {field: '$.myField2', type: 'TEXT(10)'}, {field: '$.myField3', type: 'INT'} ]} ) 列表类型可以，但只能有一个索引里面只能有一个字段是列表类型 For MySQL 8.0.17 and later, X DevAPI supports creating indexes based on array fields by setting the Boolean array field in the IndexField description to true. For example, to create an index on the emails array field: collection.createIndex(\"emails_idx\", // {fields: [{\"field\": \"$.emails\", \"type\":\"CHAR(128)\", \"array\": true}]}); 增删改查 指定collection # Use the collection 'my_collection' my_coll = my_schema.get_collection('my_collection') 增 my_coll.add([ {'name': 'Nadya', 'age': 54}, {'name': 'Lukas', 'age': 32} ]).execute() 查 # Find a single document that has a field 'name' that starts with 'L' docs = my_coll.find('name like :param').limit(1).bind('param', 'L%').execute() 改 删 优缺点 优点 语法和mongo相似 缺点 目前应该没有现成的ORM可用 没找到合表查询的方法 参考文献 官方文档 id 索引部分 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Clickhouse/":{"url":"common/Clickhouse/","title":"Clickhouse","keywords":"","body":"Clickhouse powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Clickhouse/install.html":{"url":"common/Clickhouse/install.html","title":"安装","keywords":"","body":"安装 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Clickhouse/example.html":{"url":"common/Clickhouse/example.html","title":"实例","keywords":"","body":"实例 实例 查询分区数据大小 查询所有待优化分区，并拼接优化语句 查询分区数据大小 SELECT partition AS `分区`, sum(rows) AS `总行数`, formatReadableSize(sum(data_uncompressed_bytes)) AS `原始大小`, formatReadableSize(sum(data_compressed_bytes)) AS `压缩大小`, round((sum(data_compressed_bytes) / sum(data_uncompressed_bytes)) * 100, 0) AS `压缩率` FROM system.parts WHERE table = '' GROUP BY partition 查询所有待优化分区，并拼接优化语句 SELECT concat('OPTIMIZE TABLE ', database, '.', table, ' on cluster xm_replica PARTITION ', partition , ' FINAL;') as cmd FROM system.parts WHERE active and (engine like '%ReplacingMergeTree' or engine like '%CollapsingMergeTree') and database ='xm' and table ='survey_schema' GROUP BY database,table,partition HAVING count()>1; powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Clickhouse/project/":{"url":"common/Clickhouse/project/","title":"专题","keywords":"","body":"专题 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Clickhouse/project/version.html":{"url":"common/Clickhouse/project/version.html","title":"version","keywords":"","body":"version version ClickHouse Release 1.1 ClickHouse Release 1.1.54245, 2017-07-04 ClickHouse Release 1.1.54276, 2017-08-16 ClickHouse Release 1.1.54289, 2017-09-13 ClickHouse Release 1.1.54292, 2017-09-20 ClickHouse Release 1.1.54304, 2017-10-19 ClickHouse Release 1.1.54310, 2017-11-01 ClickHouse Release 1.1.54337, 2018-01-18 ClickHouse Release 1.1.54362, 2018-03-11 ClickHouse Release 1.1.54370, 2018-03-16 ClickHouse Release 1.1.54378, 2018-04-16 ClickHouse Release 1.1.54380, 2018-04-21 ClickHouse Release 1.1.54388, 2018-06-28¶ ClickHouse Release 1.1.54390, 2018-07-06¶ ClickHouse Release 18.* ClickHouse Release 18.1.0, 2018-07-23 ClickHouse Release 18.4.0, 2018-07-28 ClickHouse Release 18.5.1, 2018-07-31 ClickHouse Release 18.6.0, 2018-08-02 ClickHouse Release 18.10.3, 2018-08-13 ClickHouse Release 18.12.13, 2018-09-10 ClickHouse Release 18.12.14, 2018-09-13 ClickHouse Release 18.12.17, 2018-09-16 ClickHouse Release 18.14.9, 2018-10-16 ClickHouse Release 18.16.0, 2018-12-14¶ ClickHouse Release 19.* ClickHouse Release 19.1.6, 2019-01-24¶ ClickHouse Release 19.3.3, 2019-02-13 ClickHouse Release 19.4.0.49, 2019-03-09 ClickHouse Release 19.5.2.6, 2019-04-15 ClickHouse Release 19.6.2.11, 2019-05-13 ClickHouse Release 19.7.3.9, 2019-05-30 ClickHouse Release 19.7.5.27, 2019-06-09¶ ClickHouse Release 19.8.3.8, 2019-06-11 ClickHouse Release 19.9.2.4, 2019-06-24¶ ClickHouse Release 19.10.1.5, 2019-07-12 ClickHouse Release 19.11.3.11, 2019-07-18 ClickHouse Release 19.13.2.19, 2019-08-14 ClickHouse Release 19.14.3.3, 2019-09-10 ClickHouse Release 19.15.2.2, 2019-10-01 ClickHouse Release 19.16.2.2, 2019-10-30 ClickHouse Release 19.17.4.11, 2019-11-22¶ ClickHouse Release 20.* ClickHouse release v20.1.2.4, 2020-01-22¶ ClickHouse release v20.1.6.30, 2020-03-05 ClickHouse release v20.3.2.1, 2020-03-12 ClickHouse release v20.3.6.40, 2020-04-16 ClickHouse release v20.3.11.97-lts 2020-06-10 ClickHouse release v20.4.2.9, 2020-05-12 ClickHouse release v20.5.2.7-stable 2020-07-02 ClickHouse release v20.6.3.28-stable ClickHouse release v20.7.2.30-stable, 2020-08-31 ClickHouse release v20.8.2.3-stable, 2020-09-08 ClickHouse release v20.9.2.20, 2020-09-22 ClickHouse release v20.10.3.30, 2020-10-28 ClickHouse release v20.11.2.1, 2020-11-11 ClickHouse release v20.12.3.3-stable, 2020-12-13 ClickHouse release 21.* ClickHouse release v21.1.2.15-stable 2021-01-18 ClickHouse release v21.2.2.8-stable, 2021-02-07 ClickHouse release v21.3-lts, 2021-03-12 ClickHouse release 21.4.1 2021-04-12 ClickHouse release 21.5, 2021-05-20 ClickHouse release 21.6, 2021-06-05 ClickHouse release v21.7, 2021-07-09 ClickHouse release v21.8, 2021-08-12 ClickHouse release v21.9, 2021-09-09 ClickHouse release v21.10, 2021-10-16 ClickHouse release v21.11, 2021-11-09 ClickHouse release v21.12, 2021-12-15 ClickHouse Release 22.* ClickHouse release v22.1, 2022-01-18 ClickHouse release v22.2, 2022-02-17 ClickHouse release v22.3-lts, 2022-03-17 ClickHouse release 22.4, 2022-04-19 ClickHouse release 22.5, 2022-05-19 ClickHouse release 22.6, 2022-06-16 ClickHouse release 22.7, 2022-07-21 ClickHouse release 22.8, 2022-08-18 ClickHouse release 22.9, 2022-09-22 ClickHouse release 22.10, 2022-10-25 ClickHouse release 22.11, 2022-11-17 ClickHouse Release 1.1 ClickHouse Release 1.1.54245, 2017-07-04 Distributed DDL (for example, CREATE TABLE ON CLUSTER) The replicated query ALTER TABLE CLEAR COLUMN IN PARTITION. The engine for Dictionary tables (access to dictionary data in the form of a table). Dictionary database engine (this type of database automatically has Dictionary tables available for all the connected external dictionaries). You can check for updates to the dictionary by sending a request to the source. Qualified column names Quoting identifiers using double quotation marks. Sessions in the HTTP interface. The OPTIMIZE query for a Replicated table can can run not only on the leader. Removed SET GLOBAL. ClickHouse Release 1.1.54276, 2017-08-16 Added an optional WITH section for a SELECT query. Example query: `WITH 1+1 AS a SELECT a, aa`* INSERT can be performed synchronously in a Distributed table: OK is returned only after all the data is saved on all the shards. This is activated by the setting insert_distributed_sync=1. Added the UUID data type for working with 16-byte identifiers. Added aliases of CHAR, FLOAT and other types for compatibility with the Tableau. Added the functions toYYYYMM, toYYYYMMDD, and toYYYYMMDDhhmmss for converting time into numbers. You can use IP addresses (together with the hostname) to identify servers for clustered DDL queries. Added support for non-constant arguments and negative offsets in the function substring(str, pos, len). Added the max_size parameter for the groupArray(max_size)(column) aggregate function, and optimized its performance. Changed the binary format of aggregate states of groupArray(array_column) functions for arrays. ClickHouse Release 1.1.54289, 2017-09-13 SYSTEM queries for server administration: SYSTEM RELOAD DICTIONARY, SYSTEM RELOAD DICTIONARIES, SYSTEM DROP DNS CACHE, SYSTEM SHUTDOWN, SYSTEM KILL. Added functions for working with arrays: concat, arraySlice, arrayPushBack, arrayPushFront, arrayPopBack, arrayPopFront. Added root and identity parameters for the ZooKeeper configuration. This allows you to isolate individual users on the same ZooKeeper cluster. Added aggregate functions groupBitAnd, groupBitOr, and groupBitXor (for compatibility, they are also available under the names BIT_AND, BIT_OR, and BIT_XOR). External dictionaries can be loaded from MySQL by specifying a socket in the filesystem. External dictionaries can be loaded from MySQL over SSL (ssl_cert, ssl_key, ssl_ca parameters). Added the max_network_bandwidth_for_user setting to restrict the overall bandwidth use for queries per user. Support for DROP TABLE for temporary tables. Support for reading DateTime values in Unix timestamp format from the CSV and JSONEachRow formats. Lagging replicas in distributed queries are now excluded by default (the default threshold is 5 minutes). FIFO locking is used during ALTER: an ALTER query isn’t blocked indefinitely for continuously running queries. Option to set umask in the config file. Improved performance for queries with DISTINCT . ClickHouse Release 1.1.54292, 2017-09-20 Added the pointInPolygon function for working with coordinates on a coordinate plane. Added the sumMap aggregate function for calculating the sum of arrays, similar to SummingMergeTree. Added the trunc function. Improved performance of the rounding functions (round, floor, ceil, roundToExp2) and corrected the logic of how they work. Changed the logic of the roundToExp2 function for fractions and negative numbers. The ClickHouse executable file is now less dependent on the libc version. The same ClickHouse executable file can run on a wide variety of Linux systems. There is still a dependency when using compiled queries (with the setting compile = 1 , which is not used by default). Reduced the time needed for dynamic compilation of queries. ClickHouse Release 1.1.54304, 2017-10-19 TLS support in the native protocol (to enable, set tcp_ssl_port in config.xml ). ClickHouse Release 1.1.54310, 2017-11-01 Custom partitioning key for the MergeTree family of table engines. Kafka table engine. Added support for loading CatBoost models and applying them to data stored in ClickHouse. Added support for time zones with non-integer offsets from UTC. Added support for arithmetic operations with time intervals. The range of values for the Date and DateTime types is extended to the year 2105. Added the CREATE MATERIALIZED VIEW x TO y query (specifies an existing table for storing the data of a materialized view). Added the ATTACH TABLE query without arguments. The processing logic for Nested columns with names ending in -Map in a SummingMergeTree table was extracted to the sumMap aggregate function. You can now specify such columns explicitly. Max size of the IP trie dictionary is increased to 128M entries. Added the getSizeOfEnumType function. Added the sumWithOverflow aggregate function. Added support for the Cap’n Proto input format. You can now customize compression level when using the zstd algorithm. Creation of temporary tables with an engine other than Memory is not allowed. Explicit creation of tables with the View or MaterializedView engine is not allowed. During table creation, a new check verifies that the sampling key expression is included in the primary key. ClickHouse Release 1.1.54337, 2018-01-18 Added support for storage of multi-dimensional arrays and tuples (Tuple data type) in tables. Support for table functions for DESCRIBE and INSERT queries. Added support for subqueries in DESCRIBE. Examples: DESC TABLE remote('host', default.hits); DESC TABLE (SELECT 1); INSERT INTO TABLE FUNCTION remote('host', default.hits). Support for INSERT INTO TABLE in addition to INSERT INTO. Improved support for time zones. The DateTime data type can be annotated with the timezone that is used for parsing and formatting in text formats. Example: DateTime('Europe/Moscow'). When timezones are specified in functions for DateTime arguments, the return type will track the timezone, and the value will be displayed as expected. Added the functions toTimeZone, timeDiff, toQuarter, toRelativeQuarterNum. The toRelativeHour/Minute/Second functions can take a value of type Date as an argument. The now function name is case-sensitive. Added the toStartOfFifteenMinutes function (Kirill Shvakov). Added the clickhouse format tool for formatting queries. Added the format_schema_path configuration parameter (Marek Vavruşa). It is used for specifying a schema in Cap'n Proto format. Schema files can be located only in the specified directory. Added support for config substitutions (incl and conf.d) for configuration of external dictionaries and models (Pavel Yakunin). Added a column with documentation for the system.settings table (Kirill Shvakov). Added the system.parts_columns table with information about column sizes in each data part of MergeTree tables. Added the system.models table with information about loaded CatBoost machine learning models. Added the mysql and odbc table function and corresponding MySQL and ODBC table engines for accessing remote databases. This functionality is in the beta stage. Added the possibility to pass an argument of type AggregateFunction for the groupArray aggregate function (so you can create an array of states of some aggregate function). Removed restrictions on various combinations of aggregate function combinators. For example, you can use avgForEachIf as well as avgIfForEach aggregate functions, which have different behaviors. The -ForEach aggregate function combinator is extended for the case of aggregate functions of multiple arguments. Added support for aggregate functions of Nullable arguments even for cases when the function returns a non-Nullable result (added with the contribution of Silviu Caragea). Example: groupArray, groupUniqArray, topK. Added the max_client_network_bandwidth for clickhouse-client (Kirill Shvakov). Users with the readonly = 2 setting are allowed to work with TEMPORARY tables (CREATE, DROP, INSERT…) (Kirill Shvakov). Added support for using multiple consumers with the Kafka engine. Extended configuration options for Kafka (Marek Vavruša). Added the intExp3 and intExp4 functions. Added the sumKahan aggregate function. Added the to * Number* OrNull functions, where * Number* is a numeric type. Added support for WITH clauses for an INSERT SELECT query (author: zhang2014). Added settings: http_connection_timeout, http_send_timeout, http_receive_timeout. In particular, these settings are used for downloading data parts for replication. Changing these settings allows for faster failover if the network is overloaded. Added support for ALTER for tables of type Null (Anastasiya Tsarkova). The reinterpretAsString function is extended for all data types that are stored contiguously in memory. Added the --silent option for the clickhouse-local tool. It suppresses printing query execution info in stderr. Added support for reading values of type Date from text in a format where the month and/or day of the month is specified using a single digit instead of two digits (Amos Bird). The format for marks in Log type tables that contain Nullable columns was changed in a backward incompatible way. If you have these tables, you should convert them to the TinyLog type before starting up the new server version. To do this, replace ENGINE = Log with ENGINE = TinyLog in the corresponding .sql file in the metadata directory. If your table does not have Nullable columns or if the type of your table is not Log, then you do not need to do anything. Removed the experimental_allow_extended_storage_definition_syntax setting. Now this feature is enabled by default. The runningIncome function was renamed to runningDifferenceStartingWithFirstvalue to avoid confusion. Removed the FROM ARRAY JOIN arr syntax when ARRAY JOIN is specified directly after FROM with no table (Amos Bird). Removed the BlockTabSeparated format that was used solely for demonstration purposes. Changed the state format for aggregate functions varSamp, varPop, stddevSamp, stddevPop, covarSamp, covarPop, corr. If you have stored states of these aggregate functions in tables (using the AggregateFunction data type or materialized views with corresponding states), please write to feedback@clickhouse.com. In previous server versions there was an undocumented feature: if an aggregate function depends on parameters, you can still specify it without parameters in the AggregateFunction data type. Example: AggregateFunction(quantiles, UInt64) instead of AggregateFunction(quantiles(0.5, 0.9), UInt64). This feature was lost. Although it was undocumented, we plan to support it again in future releases. Enum data types cannot be used in min/max aggregate functions. This ability will be returned in the next release. ClickHouse Release 1.1.54362, 2018-03-11 Aggregation without GROUP BY for an empty set (such as SELECT count(*) FROM table WHERE 0) now returns a result with one row with null values for aggregate functions, in compliance with the SQL standard. To restore the old behavior (return an empty result), set empty_result_for_aggregation_by_empty_set to 1. Added type conversion for UNION ALL. Different alias names are allowed in SELECT positions in UNION ALL, in compliance with the SQL standard. Arbitrary expressions are supported in LIMIT BY clauses. Previously, it was only possible to use columns resulting from SELECT. An index of MergeTree tables is used when IN is applied to a tuple of expressions from the columns of the primary key. Example: WHERE (UserID, EventDate) IN ((123, '2000-01-01'), ...) (Anastasiya Tsarkova). Added the clickhouse-copier tool for copying between clusters and resharding data (beta). Added consistent hashing functions: yandexConsistentHash, jumpConsistentHash, sumburConsistentHash. They can be used as a sharding key in order to reduce the amount of network traffic during subsequent reshardings. Added functions: arrayAny, arrayAll, hasAny, hasAll, arrayIntersect, arrayResize. Added the arrayCumSum function (Javi Santana). Added the parseDateTimeBestEffort, parseDateTimeBestEffortOrZero, and parseDateTimeBestEffortOrNull functions to read the DateTime from a string containing text in a wide variety of possible formats. Data can be partially reloaded from external dictionaries during updating (load just the records in which the value of the specified field greater than in the previous download) (Arsen Hakobyan). Added the cluster table function. Example: cluster(cluster_name, db, table). The remote table function can accept the cluster name as the first argument, if it is specified as an identifier. The remote and cluster table functions can be used in INSERT queries. Added the create_table_query and engine_full virtual columns to the system.tablestable . The metadata_modification_time column is virtual. Added the data_path and metadata_path columns to system.tablesandsystem.databases tables, and added the path column to the system.parts and system.parts_columns tables. Added additional information about merges in the system.part_log table. An arbitrary partitioning key can be used for the system.query_log table (Kirill Shvakov). The SHOW TABLES query now also shows temporary tables. Added temporary tables and the is_temporary column to system.tables (zhang2014). Added DROP TEMPORARY TABLE and EXISTS TEMPORARY TABLE queries (zhang2014). Support for SHOW CREATE TABLE for temporary tables (zhang2014). Added the system_profile configuration parameter for the settings used by internal processes. Support for loading object_id as an attribute in MongoDB dictionaries (Pavel Litvinenko). Reading null as the default value when loading data for an external dictionary with the MongoDB source (Pavel Litvinenko). Reading DateTime values in the Values format from a Unix timestamp without single quotes. Failover is supported in remote table functions for cases when some of the replicas are missing the requested table. Configuration settings can be overridden in the command line when you run clickhouse-server. Example: clickhouse-server -- --logger.level=information. Implemented the empty function from a FixedString argument: the function returns 1 if the string consists entirely of null bytes (zhang2014). Added the listen_tryconfiguration parameter for listening to at least one of the listen addresses without quitting, if some of the addresses can’t be listened to (useful for systems with disabled support for IPv4 or IPv6). Added the VersionedCollapsingMergeTree table engine. Support for rows and arbitrary numeric types for the library dictionary source. MergeTree tables can be used without a primary key (you need to specify ORDER BY tuple()). A Nullable type can be CAST to a non-Nullable type if the argument is not NULL. RENAME TABLE can be performed for VIEW. Added the throwIf function. Added the odbc_default_field_size option, which allows you to extend the maximum size of the value loaded from an ODBC source (by default, it is 1024). The system.processes table and SHOW PROCESSLIST now have the is_cancelled and peak_memory_usage columns. ClickHouse Release 1.1.54370, 2018-03-16 Added the system.macros table and auto updating of macros when the config file is changed. Added the SYSTEM RELOAD CONFIG query. Added the maxIntersections(left_col, right_col) aggregate function, which returns the maximum number of simultaneously intersecting intervals [left; right]. The maxIntersectionsPosition(left, right) function returns the beginning of the “maximum” interval. (Michael Furmur). ClickHouse Release 1.1.54378, 2018-04-16 Logging level can be changed without restarting the server. Added the SHOW CREATE DATABASE query. The query_id can be passed to clickhouse-client (elBroom). New setting: max_network_bandwidth_for_all_users. Added support for ALTER TABLE ... PARTITION ... for MATERIALIZED VIEW. Added information about the size of data parts in uncompressed form in the system table. Server-to-server encryption support for distributed tables (1 in the replica config in ). Configuration of the table level for the ReplicatedMergeTree family in order to minimize the amount of data stored in Zookeeper: : use_minimalistic_checksums_in_zookeeper = 1 Configuration of the clickhouse-client prompt. By default, server names are now output to the prompt. The server’s display name can be changed. It’s also sent in the X-ClickHouse-Display-Name HTTP header (Kirill Shvakov). Multiple comma-separated topics can be specified for the Kafka engine (Tobias Adamson) When a query is stopped by KILL QUERY or replace_running_query, the client receives the Query was canceled exception instead of an incomplete result. ClickHouse Release 1.1.54380, 2018-04-21 Added the table function file(path, format, structure). An example reading bytes from /dev/urandom: ln -s /dev/urandom /var/lib/clickhouse/user_files/randomclickhouse-client -q \"SELECT * FROM file('random', 'RowBinary', 'd UInt8') LIMIT 10\". ClickHouse Release 1.1.54388, 2018-06-28¶ Support for the ALTER TABLE t DELETE WHERE query for replicated tables. Added the system.mutations table to track progress of this type of queries. Support for the ALTER TABLE t [REPLACE|ATTACH] PARTITION query for *MergeTree tables. Support for the TRUNCATE TABLE query (Winter Zhang) Several new SYSTEM queries for replicated tables (RESTART REPLICAS, SYNC REPLICA, [STOP|START] [MERGES|FETCHES|SENDS REPLICATED|REPLICATION QUEUES]). Added the ability to write to a table with the MySQL engine and the corresponding table function (sundy-li). Added the url() table function and the URL table engine (Alexander Sapin). Added the windowFunnel aggregate function (sundy-li). New startsWith and endsWith functions for strings (Vadim Plakhtinsky). The numbers() table function now allows you to specify the offset (Winter Zhang). The password to clickhouse-client can be entered interactively. Server logs can now be sent to syslog (Alexander Krasheninnikov). Support for logging in dictionaries with a shared library source (Alexander Sapin). Support for custom CSV delimiters (Ivan Zhukov) Added the date_time_input_format setting. If you switch this setting to 'best_effort', DateTime values will be read in a wide range of formats. Added the clickhouse-obfuscator utility for data obfuscation. Usage example: publishing data used in performance tests. ClickHouse Release 1.1.54390, 2018-07-06¶ Queries can be sent in multipart/form-data format (in the query field), which is useful if external data is also sent for query processing (Olga Hvostikova). Added the ability to enable or disable processing single or double quotes when reading data in CSV format. You can configure this in the format_csv_allow_single_quotes and format_csv_allow_double_quotes settings (Amos Bird). Now OPTIMIZE TABLE ... FINAL can be used without specifying the partition for non-replicated variants of MergeTree (Amos Bird). ClickHouse Release 18.* ClickHouse Release 18.1.0, 2018-07-23 Support for the ALTER TABLE t DELETE WHERE query for non-replicated MergeTree tables (#2634). Support for arbitrary types for the uniq* family of aggregate functions (#2010). Support for arbitrary types in comparison operators (#2026). The users.xml file allows setting a subnet mask in the format 10.0.0.1/255.255.255.0. This is necessary for using masks for IPv6 networks with zeros in the middle (#2637). Added the arrayDistinct function (#2670). The SummingMergeTree engine can now work with AggregateFunction type columns (Constantin S. Pan). ClickHouse Release 18.4.0, 2018-07-28 Added system tables: formats, data_type_families, aggregate_function_combinators, table_functions, table_engines, collations #2721. Added the ability to use a table function instead of a table as an argument of a remote or cluster table function #2708. Support for HTTP Basic authentication in the replication protocol #2727. The has function now allows searching for a numeric value in an array of Enum values Maxim Khrisanfov. Support for adding arbitrary message separators when reading from Kafka Amos Bird. ClickHouse Release 18.5.1, 2018-07-31 Added the hash function murmurHash2_32 #2756. ClickHouse Release 18.6.0, 2018-08-02 Added support for ON expressions for the JOIN ON syntax: JOIN ON Expr([table.]column ...) = Expr([table.]column, ...) [AND Expr([table.]column, ...) = Expr([table.]column, ...) ...] The expression must be a chain of equalities joined by the AND operator. Each side of the equality can be an arbitrary expression over the columns of one of the tables. The use of fully qualified column names is supported (table.name, database.table.name, table_alias.name, subquery_alias.name) for the right table. #2742 HTTPS can be enabled for replication. #2760 ClickHouse Release 18.10.3, 2018-08-13 HTTPS can be used for replication. #2760 Added the functions murmurHash2_64, murmurHash3_32, murmurHash3_64, and murmurHash3_128 in addition to the existing murmurHash2_32. #2791 Support for Nullable types in the ClickHouse ODBC driver (ODBCDriver2 output format). #2834 Support for UUID in the key columns. ClickHouse Release 18.12.13, 2018-09-10 Added the DECIMAL(digits, scale) data type (Decimal32(scale), Decimal64(scale), Decimal128(scale)). To enable it, use the setting allow_experimental_decimal_type. #2846 #2970 #3008 #3047 New WITH ROLLUP modifier for GROUP BY (alternative syntax: GROUP BY ROLLUP(...)). #2948 In queries with JOIN, the star character expands to a list of columns in all tables, in compliance with the SQL standard. You can restore the old behavior by setting asterisk_left_columns_only to 1 on the user configuration level. Winter Zhang Added support for JOIN with table functions. Winter Zhang Autocomplete by pressing Tab in clickhouse-client. Sergey Shcherbin Ctrl+C in clickhouse-client clears a query that was entered. #2877 Added the join_default_strictness setting (values: \", 'any', 'all'). This allows you to not specify ANY or ALL for JOIN. #2982 Each line of the server log related to query processing shows the query ID. #2482 Now you can get query execution logs in clickhouse-client (use the send_logs_level setting). With distributed query processing, logs are cascaded from all the servers. #2482 The system.query_log and system.processes (SHOW PROCESSLIST) tables now have information about all changed settings when you run a query (the nested structure of the Settings data). Added the log_query_settings setting. #2482 The system.query_log and system.processes tables now show information about the number of threads that are participating in query execution (see the thread_numbers column). #2482 Added ProfileEvents counters that measure the time spent on reading and writing over the network and reading and writing to disk, the number of network errors, and the time spent waiting when network bandwidth is limited. #2482 Added ProfileEventscounters that contain the system metrics from rusage (you can use them to get information about CPU usage in userspace and the kernel, page faults, and context switches), as well as taskstats metrics (use these to obtain information about I/O wait time, CPU wait time, and the amount of data read and recorded, both with and without page cache). #2482 The ProfileEvents counters are applied globally and for each query, as well as for each query execution thread, which allows you to profile resource consumption by query in detail. #2482 Added the system.query_thread_log table, which contains information about each query execution thread. Added the log_query_threads setting. #2482 The system.metrics and system.events tables now have built-in documentation. #3016 Added the arrayEnumerateDense function. Amos Bird Added the arrayCumSumNonNegative and arrayDifference functions. Aleksey Studnev Added the retention aggregate function. Sundy Li Now you can add (merge) states of aggregate functions by using the plus operator, and multiply the states of aggregate functions by a nonnegative constant. #3062 #3034 Tables in the MergeTree family now have the virtual column _partition_id. #3089 ClickHouse Release 18.12.14, 2018-09-13 Added support for ALTER UPDATE queries. #3035 Added the allow_ddl option, which restricts the user’s access to DDL queries. #3104 Added the min_merge_bytes_to_use_direct_io option for MergeTree engines, which allows you to set a threshold for the total size of the merge (when above the threshold, data part files will be handled using O_DIRECT). #3117 The system.merges system table now contains the partition_id column. #3099 ClickHouse Release 18.12.17, 2018-09-16 invalidate_query (the ability to specify a query to check whether an external dictionary needs to be updated) is implemented for the clickhouse source. #3126 Added the ability to use UInt*, Int*, and DateTime data types (along with the Date type) as a range_hashed external dictionary key that defines the boundaries of ranges. Now NULL can be used to designate an open range. Vasily Nemkov The Decimal type now supports var* and stddev* aggregate functions. #3129 The Decimal type now supports mathematical functions (exp, sin and so on.) #3129 The system.part_log table now has the partition_id column. #3089 ClickHouse Release 18.14.9, 2018-10-16 The WITH CUBE modifier for GROUP BY (the alternative syntax GROUP BY CUBE(...) is also available). #3172 Added the formatDateTime function. Alexandr Krasheninnikov Added the JDBC table engine and jdbc table function (requires installing clickhouse-jdbc-bridge). Alexandr Krasheninnikov Added functions for working with the ISO week number: toISOWeek, toISOYear, toStartOfISOYear, and toDayOfYear. #3146 Now you can use Nullable columns for MySQL and ODBC tables. #3362 Nested data structures can be read as nested objects in JSONEachRow format. Added the input_format_import_nested_json setting. Veloman Yunkan Parallel processing is available for many MATERIALIZED VIEWs when inserting data. See the parallel_view_processing setting. Marek Vavruša Added the SYSTEM FLUSH LOGS query (forced log flushes to system tables such as query_log) #3321 Now you can use pre-defined database and table macros when declaring Replicated tables. #3251 Added the ability to read Decimal type values in engineering notation (indicating powers of ten). #3153 ClickHouse Release 18.16.0, 2018-12-14¶ DEFAULT expressions are evaluated for missing fields when loading data in semi-structured input formats (JSONEachRow, TSKV). The feature is enabled with the insert_sample_with_metadata setting. #3555 The ALTER TABLE query now has the MODIFY ORDER BY action for changing the sorting key when adding or removing a table column. This is useful for tables in the MergeTree family that perform additional tasks when merging based on this sorting key, such as SummingMergeTree, AggregatingMergeTree, and so on. #3581 #3755 For tables in the MergeTree family, now you can specify a different sorting key (ORDER BY) and index (PRIMARY KEY). The sorting key can be longer than the index. #3581 Added the hdfs table function and the HDFS table engine for importing and exporting data to HDFS. chenxing-xc Added functions for working with base64: base64Encode, base64Decode, tryBase64Decode. Alexander Krasheninnikov Now you can use a parameter to configure the precision of the uniqCombined aggregate function (select the number of HyperLogLog cells). #3406 Added the system.contributors table that contains the names of everyone who made commits in ClickHouse. #3452 Added the ability to omit the partition for the ALTER TABLE ... FREEZE query in order to back up all partitions at once. #3514 Added dictGet and dictGetOrDefault functions that do not require specifying the type of return value. The type is determined automatically from the dictionary description. Amos Bird Now you can specify comments for a column in the table description and change it using ALTER. #3377 Reading is supported for Join type tables with simple keys. Amos Bird Now you can specify the options join_use_nulls, max_rows_in_join, max_bytes_in_join, and join_overflow_mode when creating a Join type table. Amos Bird Added the joinGet function that allows you to use a Join type table like a dictionary. Amos Bird Added the partition_key, sorting_key, primary_key, and sampling_key columns to the system.tables table in order to provide information about table keys. #3609 Added the is_in_partition_key, is_in_sorting_key, is_in_primary_key, and is_in_sampling_key columns to the system.columns table. #3609 Added the min_time and max_time columns to the system.parts table. These columns are populated when the partitioning key is an expression consisting of DateTime columns. Emmanuel Donin de Rosière ClickHouse Release 19.* ClickHouse Release 19.1.6, 2019-01-24¶ Custom per column compression codecs for tables. #3899 #4111 (alesapin, Winter Zhang, Anatoly) Added compression codec Delta. #4052 (alesapin) Allow to ALTER compression codecs. #4054 (alesapin) Added functions left, right, trim, ltrim, rtrim, timestampadd, timestampsub for SQL standard compatibility. #3826 (Ivan Blinkov) Support for write in HDFS tables and hdfs table function. #4084 (alesapin) Added functions to search for multiple constant strings from big haystack: multiPosition, multiSearch ,firstMatch also with -UTF8, -CaseInsensitive, and -CaseInsensitiveUTF8 variants. #4053 (Danila Kutenin) Pruning of unused shards if SELECT query filters by sharding key (setting optimize_skip_unused_shards). #3851 (Gleb Kanterov, Ivan) Allow Kafka engine to ignore some number of parsing errors per block. #4094 (Ivan) Added support for CatBoost multiclass models evaluation. Function modelEvaluate returns tuple with per-class raw predictions for multiclass models. libcatboostmodel.so should be built with #607. #3959 (KochetovNicolai) Added functions filesystemAvailable, filesystemFree, filesystemCapacity. #4097 (Boris Granveaud) Added hashing functions xxHash64 and xxHash32. #3905 (filimonov) Added gccMurmurHash hashing function (GCC flavoured Murmur hash) which uses the same hash seed as gcc #4000 (sundyli) Added hashing functions javaHash, hiveHash. #3811 (shangshujie365) Added table function remoteSecure. Function works as remote, but uses secure connection. #4088 (proller) ClickHouse Release 19.3.3, 2019-02-13 Added the KILL MUTATION statement that allows removing mutations that are for some reasons stuck. Added latest_failed_part, latest_fail_time, latest_fail_reason fields to the system.mutations table for easier troubleshooting. #4287 (Alex Zatelepin) Added aggregate function entropy which computes Shannon entropy. #4238 (Quid37) Added ability to send queries INSERT INTO tbl VALUES (.... to server without splitting on query and data parts. #4301 (alesapin) Generic implementation of arrayWithConstant function was added. #4322 (alexey-milovidov) Implemented NOT BETWEEN comparison operator. #4228 (Dmitry Naumov) Implement sumMapFiltered in order to be able to limit the number of keys for which values will be summed by sumMap. #4129 (Léo Ercolanelli) Added support of Nullable types in mysql table function. #4198 (Emmanuel Donin de Rosière) Support for arbitrary constant expressions in LIMIT clause. #4246 (k3box) Added topKWeighted aggregate function that takes additional argument with (unsigned integer) weight. #4245 (Andrew Golman) StorageJoin now supports join_any_take_last_row setting that allows overwriting existing values of the same key. #3973 (Amos Bird Added function toStartOfInterval. #4304 (Vitaly Baranov) Added RowBinaryWithNamesAndTypes format. #4200 (Oleg V. Kozlyuk) Added IPv4 and IPv6 data types. More effective implementations of IPv* functions. #3669 (Vasily Nemkov) Added function toStartOfTenMinutes(). #4298 (Vitaly Baranov) Added Protobuf output format. #4005 #4158 (Vitaly Baranov) Added brotli support for HTTP interface for data import (INSERTs). #4235 (Mikhail) Added hints while user make typo in function name or type in command line client. #4239 (Danila Kutenin) Added Query-Id to Server’s HTTP Response header. #4231 (Mikhail) ClickHouse Release 19.4.0.49, 2019-03-09 Added full support for Protobuf format (input and output, nested data structures). #4174 #4493 (Vitaly Baranov) Added bitmap functions with Roaring Bitmaps. #4207 (Andy Yang) #4568 (Vitaly Baranov) Parquet format support. #4448 (proller) N-gram distance was added for fuzzy string comparison. It is similar to q-gram metrics in R language. #4466 (Danila Kutenin) Combine rules for graphite rollup from dedicated aggregation and retention patterns. #4426 (Mikhail f. Shiryaev) Added max_execution_speed and max_execution_speed_bytes to limit resource usage. Added min_execution_speed_bytes setting to complement the min_execution_speed. #4430 (Winter Zhang) Implemented function flatten. #4555 #4409 (alexey-milovidov, kzon) Added functions arrayEnumerateDenseRanked and arrayEnumerateUniqRanked (it’s like arrayEnumerateUniq but allows to fine tune array depth to look inside multidimensional arrays). #4475 (proller) #4601 (alexey-milovidov) Multiple JOINS with some restrictions: no asterisks, no complex aliases in ON/WHERE/GROUP BY/… #4462 (Artem Zuikov) ClickHouse Release 19.5.2.6, 2019-04-15 Hyperscan multiple regular expression matching was added (functions multiMatchAny, multiMatchAnyIndex, multiFuzzyMatchAny, multiFuzzyMatchAnyIndex). #4780, #4841 (Danila Kutenin) multiSearchFirstPosition function was added. #4780 (Danila Kutenin) Implement the predefined expression filter per row for tables. #4792 (Ivan) A new type of data skipping indices based on bloom filters (can be used for equal, in and like functions). #4499 (Nikita Vasilev) Added ASOF JOIN which allows to run queries that join to the most recent value known. #4774 #4867 #4863 #4875 (Martijn Bakker, Artem Zuikov) Rewrite multiple COMMA JOIN to CROSS JOIN. Then rewrite them to INNER JOIN if possible. #4661 (Artem Zuikov) ClickHouse Release 19.6.2.11, 2019-05-13 TTL expressions for columns and tables. #4212 (Anton Popov) Added support for brotli compression for HTTP responses (Accept-Encoding: br) #4388 (Mikhail) Added new function isValidUTF8 for checking whether a set of bytes is correctly utf-8 encoded. #4934 (Danila Kutenin) Add new load balancing policy first_or_random which sends queries to the first specified host and if it’s inaccessible send queries to random hosts of shard. Useful for cross-replication topology setups. #5012 (nvartolomei) ClickHouse Release 19.7.3.9, 2019-05-30 Allow to limit the range of a setting that can be specified by user.These constraints can be set up in user settings profile.#4931 (VitalyBaranov) Add a second version of the function groupUniqArray with an optional max_size parameter that limits the size of the resulting array. This behavior is similar to groupArray(max_size)(x) function.#5026 (GuillaumeTassery) For TSVWithNames/CSVWithNames input file formats, column order can now be determined from file header. This is controlled by input_format_with_names_use_header parameter.#5081(Alexander) ClickHouse Release 19.7.5.27, 2019-06-09¶ Added bitmap related functions bitmapHasAny and bitmapHasAll analogous to hasAny and hasAll functions for arrays. #5279 (Sergi Vladykin) ClickHouse Release 19.8.3.8, 2019-06-11 Added functions to work with JSON #4686 (hcz) #5124. (Vitaly Baranov) Add a function basename, with a similar behaviour to a basename function, which exists in a lot of languages (os.path.basename in python, basename in PHP, etc…). Work with both an UNIX-like path or a Windows path. #5136 (Guillaume Tassery) Added LIMIT n, m BY or LIMIT m OFFSET n BY syntax to set offset of n for LIMIT BY clause. #5138 (Anton Popov) Added new data type SimpleAggregateFunction, which allows to have columns with light aggregation in an AggregatingMergeTree. This can only be used with simple functions like any, anyLast, sum, min, max. #4629 (Boris Granveaud) Added support for non-constant arguments in function ngramDistance #5198 (Danila Kutenin) Added functions skewPop, skewSamp, kurtPop and kurtSamp to compute for sequence skewness, sample skewness, kurtosis and sample kurtosis respectively. #5200 (hcz) Support rename operation for MaterializeView storage. #5209 (Guillaume Tassery) Added server which allows connecting to ClickHouse using MySQL client. #4715 (Yuriy Baranov) Add toDecimalOrZero and toDecimalOrNull functions. #5291 (Artem Zuikov) Support Decimal types in functions: quantile, quantiles, median, quantileExactWeighted, quantilesExactWeighted, medianExactWeighted. #5304 (Artem Zuikov) Added toValidUTF8 function, which replaces all invalid UTF-8 characters by replacement character � (U+FFFD). #5322 (Danila Kutenin) Added format function. Formatting constant pattern (simplified Python format pattern) with the strings listed in the arguments. #5330 (Danila Kutenin) Added system.detached_parts table containing information about detached parts of MergeTree tables. #5353 (akuzm) Added ngramSearch function to calculate the non-symmetric difference between needle and haystack. #5418#5422 (Danila Kutenin) Implementation of basic machine learning methods (stochastic linear regression and logistic regression) using aggregate functions interface. Has different strategies for updating model weights (simple gradient descent, momentum method, Nesterov method). Also supports mini-batches of custom size. #4943 (Quid37) Implementation of geohashEncode and geohashDecode functions. #5003 (Vasily Nemkov) Added aggregate function timeSeriesGroupSum, which can aggregate different time series that sample timestamp not alignment. It will use linear interpolation between two sample timestamp and then sum time-series together. Added aggregate function timeSeriesGroupRateSum, which calculates the rate of time-series and then sum rates together. #4542 (Yangkuan Liu) Added functions IPv4CIDRtoIPv4Range and IPv6CIDRtoIPv6Range to calculate the lower and higher bounds for an IP in the subnet using a CIDR. #5095 (Guillaume Tassery) Add a X-ClickHouse-Summary header when we send a query using HTTP with enabled setting send_progress_in_http_headers. Return the usual information of X-ClickHouse-Progress, with additional information like how many rows and bytes were inserted in the query. #5116 (Guillaume Tassery) ClickHouse Release 19.9.2.4, 2019-06-24¶ Print information about frozen parts in system.parts table. #5471 (proller) Ask client password on clickhouse-client start on tty if not set in arguments #5092 (proller) Implement dictGet and dictGetOrDefault functions for Decimal types. #5394 (Artem Zuikov) ClickHouse Release 19.10.1.5, 2019-07-12 Add new column codec: T64. Made for (U)IntX/EnumX/Data(Time)/DecimalX columns. It should be good for columns with constant or small range values. Codec itself allows enlarge or shrink data type without re-compression. #5557 (Artem Zuikov) Add database engine MySQL that allow to view all the tables in remote MySQL server #5599 (Winter Zhang) bitmapContains implementation. It’s 2x faster than bitmapHasAny if the second bitmap contains one element. #5535 (Zhichang Yu) Support for crc32 function (with behaviour exactly as in MySQL or PHP). Do not use it if you need a hash function. #5661 (Remen Ivan) Implemented SYSTEM START/STOP DISTRIBUTED SENDS queries to control asynchronous inserts into Distributed tables. #4935 (Winter Zhang) ClickHouse Release 19.11.3.11, 2019-07-18 Added support for prepared statements. #5331 (Alexander) #5630 (alexey-milovidov) DoubleDelta and Gorilla column codecs #5600 (Vasily Nemkov) Added os_thread_priority setting that allows to control the “nice” value of query processing threads that is used by OS to adjust dynamic scheduling priority. It requires CAP_SYS_NICE capabilities to work. This implements #5858 #5909 (alexey-milovidov) Implement _topic, _offset, _key columns for Kafka engine #5382 (Ivan) Note that Kafka is broken in this version. Add aggregate function combinator -Resample #5590 (hcz) Aggregate functions groupArrayMovingSum(win_size)(x) and groupArrayMovingAvg(win_size)(x), which calculate moving sum/avg with or without window-size limitation. #5595 (inv2004) Add synonim arrayFlatten \\ flatten #5764 (hcz) Intergate H3 function geoToH3 from Uber. #4724 (Remen Ivan) #5805 (alexey-milovidov) ClickHouse Release 19.13.2.19, 2019-08-14 Sampling profiler on query level. Example. #4247 (laplab) #6124 (alexey-milovidov) #6250 #6283 #6386 Allow to specify a list of columns with COLUMNS('regexp') expression that works like a more sophisticated variant of * asterisk. #5951 (mfridental), (alexey-milovidov) CREATE TABLE AS table_function() is now possible #6057 (dimarub2000) Adam optimizer for stochastic gradient descent is used by default in stochasticLinearRegression() and stochasticLogisticRegression() aggregate functions, because it shows good quality without almost any tuning. #6000 (Quid37) Added functions for working with the сustom week number #5212 (Andy Yang) RENAME queries now work with all storages. #5953 (Ivan) Now client receive logs from server with any desired level by setting send_logs_level regardless to the log level specified in server settings. #5964 (Nikita Mikhaylov) ClickHouse Release 19.14.3.3, 2019-09-10 WITH FILL modifier for ORDER BY. (continuation of #5069) #6610 (Anton Popov) WITH TIES modifier for LIMIT. (continuation of #5069) #6610 (Anton Popov) Parse unquoted NULL literal as NULL (if setting format_csv_unquoted_null_literal_as_null=1). Initialize null fields with default values if data type of this field is not nullable (if setting input_format_null_as_default=1). #5990 #6055 (tavplubix) Support for wildcards in paths of table functions file and hdfs. If the path contains wildcards, the table will be readonly. Example of usage: select from hdfs('hdfs://hdfs1:9000/some_dir/another_dir//file{0..9}{0..9}') and select * from file('some_dir/{some_file,another_file,yet_another}.tsv', 'TSV', 'value UInt32'). #6092 (Olga Khvostikova) New system.metric_log table which stores values of system.events and system.metrics with specified time interval. #6363 #6467 (Nikita Mikhaylov) #6530 (alexey-milovidov) Allow to write ClickHouse text logs to system.text_log table. #6037 #6103 (Nikita Mikhaylov) #6164 (alexey-milovidov) Show private symbols in stack traces (this is done via parsing symbol tables of ELF files). Added information about file and line number in stack traces if debug info is present. Speedup symbol name lookup with indexing symbols present in program. Added new SQL functions for introspection: demangle and addressToLine. Renamed function symbolizeAddress to addressToSymbol for consistency. Function addressToSymbol will return mangled name for performance reasons and you have to apply demangle. Added setting allow_introspection_functions which is turned off by default. #6201 (alexey-milovidov) Table function values (the name is case-insensitive). It allows to read from VALUES list proposed in #5984. Example: SELECT * FROM VALUES('a UInt64, s String', (1, 'one'), (2, 'two'), (3, 'three')). #6217. #6209 (dimarub2000) Added an ability to alter storage settings. Syntax: ALTER TABLE MODIFY SETTING = . #6366 #6669 #6685 (alesapin) Support for removing of detached parts. Syntax: ALTER TABLE DROP DETACHED PART ''. #6158 (tavplubix) Table constraints. Allows to add constraint to table definition which will be checked at insert. #5273 (Gleb Novikov) #6652 (alexey-milovidov) Suppport for cascaded materialized views. #6324 (Amos Bird) Turn on query profiler by default to sample every query execution thread once a second. #6283 (alexey-milovidov) Input format ORC. #6454 #6703 (akonyaev90) Added two new functions: sigmoid and tanh (that are useful for machine learning applications). #6254 (alexey-milovidov) Function hasToken(haystack, token), hasTokenCaseInsensitive(haystack, token) to check if given token is in haystack. Token is a maximal length substring between two non alphanumeric ASCII characters (or boundaries of haystack). Token must be a constant string. Supported by tokenbf_v1 index specialization. #6596, #6662 (Vasily Nemkov) New function neighbor(value, offset[, default_value]). Allows to reach prev/next value within column in a block of data. #5925 (Alex Krash) 6685365ab8c5b74f9650492c88a012596eb1b0c6 341e2e4587a18065c2da1ca888c73389f48ce36c Alexey Milovidov Created a function currentUser(), returning login of authorized user. Added alias user() for compatibility with MySQL. #6470 (Alex Krash) New aggregate functions quantilesExactInclusive and quantilesExactExclusive which were proposed in #5885. #6477 (dimarub2000) Function bitmapRange(bitmap, range_begin, range_end) which returns new set with specified range (not include the range_end). #6314 (Zhichang Yu) Function geohashesInBox(longitude_min, latitude_min, longitude_max, latitude_max, precision) which creates array of precision-long strings of geohash-boxes covering provided area. #6127 (Vasily Nemkov) Implement support for INSERT query with Kafka tables. #6012 (Ivan) Added support for _partition and _timestamp virtual columns to Kafka engine. #6400 (Ivan) Possibility to remove sensitive data from query_log, server logs, process list with regexp-based rules. #5710 (filimonov) Experimental Feature Input and output data format Template. It allows to specify custom format string for input and output. #4354 #6727 (tavplubix) Implementation of LIVE VIEW tables that were originally proposed in #2898, prepared in #3925, and then updated in #5541. See #5541 for detailed description. #5541 (vzakaznikov) #6425 (Nikolai Kochetov) #6656 (vzakaznikov) Note that LIVE VIEW feature may be removed in next versions. ClickHouse Release 19.15.2.2, 2019-10-01 Tiered storage: support to use multiple storage volumes for tables with MergeTree engine. It’s possible to store fresh data on SSD and automatically move old data to HDD. (example). #4918 (Igr) #6489 (alesapin) Add table function input for reading incoming data in INSERT SELECT query. #5450 (palasonic1) #6832 (Anton Popov) Add a sparse_hashed dictionary layout, that is functionally equivalent to the hashed layout, but is more memory efficient. It uses about twice as less memory at the cost of slower value retrieval. #6894 (Azat Khuzhin) Implement ability to define list of users for access to dictionaries. Only current connected database using. #6907 (Guillaume Tassery) Add LIMIT option to SHOW query. #6944 (Philipp Malkovsky) Add bitmapSubsetLimit(bitmap, range_start, limit) function, that returns subset of the smallest limit values in set that is no smaller than range_start. #6957 (Zhichang Yu) Add bitmapMin and bitmapMax functions. #6970 (Zhichang Yu) Add function repeat related to issue-6648 #6999 (flynn) Experimental Feature Implement (in memory) Merge Join variant that does not change current pipeline. Result is partially sorted by merge key. Set partial_merge_join = 1 to use this feature. The Merge Join is still in development. #6940 (Artem Zuikov) Add S3 engine and table function. It is still in development (no authentication support yet). #5596 (Vladimir Chebotarev) ClickHouse Release 19.16.2.2, 2019-10-30 Add missing arity validation for count/counIf.#7095,#7298 (Vdimir) Remove legacy asterisk_left_columns_only setting (it was disabled by default).#7335 (Artem Zuikov) Format strings for Template data format are now specified in files.#7118 (tavplubix) Introduce uniqCombined64() to calculate cardinality greater than UINT_MAX.#7213,#7222 (Azat Khuzhin) Support Bloom filter indexes on Array columns.#6984 (achimbab) Add a function getMacro(name) that returns String with the value of corresponding from server configuration. #7240 (alexey-milovidov) Set two configuration options for a dictionary based on an HTTP source: credentials and http-headers. #7092 (Guillaume Tassery) Add a new ProfileEvent Merge that counts the number of launched background merges.#7093 (Mikhail Korotov) Add fullHostName function that returns a fully qualified domain name.#7263, #7291 (sundyli) Add function arraySplit and arrayReverseSplit which split an array by “cut off” conditions. They are useful in time sequence handling.#7294 (hcz) Add new functions that return the Array of all matched indices in multiMatch family of functions.#7299 (Danila Kutenin) Add a new database engine Lazy that is optimized for storing a large number of small -Log tables. #7171 (Nikita Vasilev) Add aggregate functions groupBitmapAnd, -Or, -Xor for bitmap columns. #7109 (Zhichang Yu) Add aggregate function combinators -OrNull and -OrDefault, which return null or default values when there is nothing to aggregate.#7331 (hcz) Introduce CustomSeparated data format that supports custom escaping and delimiter rules. #7118 (tavplubix) Support Redis as source of external dictionary. #4361 #6962 (comunodi, Anton Popov) ClickHouse Release 19.17.4.11, 2019-11-22¶ Using column instead of AST to store scalar subquery results for better performance. Setting enable_scalar_subquery_optimization was added in 19.17 and it was enabled by default. It leads to errors like this during upgrade to 19.17.2 or 19.17.3 from previous versions. This setting was disabled by default in 19.17.4, to make possible upgrading from 19.16 and older versions without errors. #7392 (Amos Bird) Add the ability to create dictionaries with DDL queries. #7360 (alesapin) Make bloom_filter type of index supporting LowCardinality and Nullable #7363 #7561 (Nikolai Kochetov) Add function isValidJSON to check that passed string is a valid json. #5910 #7293 (Vdimir) Implement arrayCompact function #7328 (Memo) Created function hex for Decimal numbers. It works like hex(reinterpretAsString()), but does not delete last zero bytes. #7355 (Mikhail Korotov) Add arrayFill and arrayReverseFill functions, which replace elements by other elements in front/back of them in the array. #7380 (hcz) Add CRC32IEEE()/CRC64() support #7480 (Azat Khuzhin) Implement char function similar to one in mysql #7486 (sundyli) Add bitmapTransform function. It transforms an array of values in a bitmap to another array of values, the result is a new bitmap #7598 (Zhichang Yu) Implemented javaHashUTF16LE() function #7651 (achimbab) Add _shard_num virtual column for the Distributed engine #7624 (Azat Khuzhin) Support for processors (new query execution pipeline) in MergeTree. #7181 (Nikolai Kochetov) ClickHouse Release 20.* ClickHouse release v20.1.2.4, 2020-01-22¶ Make the setting merge_tree_uniform_read_distribution obsolete. The server still recognizes this setting but it has no effect. #8308 (alexey-milovidov) Changed return type of the function greatCircleDistance to Float32 because now the result of calculation is Float32. #7993 (alexey-milovidov) Now it's expected that query parameters are represented in \"escaped\" format. For example, to pass string ab you have to write a\\tb or a\\b and respectively, a%5Ctb or a%5C%09b in URL. This is needed to add the possibility to pass NULL as \\N. This fixes #7488. #8517 (alexey-milovidov) Enable use_minimalistic_part_header_in_zookeeper setting for ReplicatedMergeTree by default. This will significantly reduce amount of data stored in ZooKeeper. This setting is supported since version 19.1 and we already use it in production in multiple services without any issues for more than half a year. Disable this setting if you have a chance to downgrade to versions older than 19.1. #6850 (alexey-milovidov) Data skipping indices are production ready and enabled by default. The settings allow_experimental_data_skipping_indices, allow_experimental_cross_to_join_conversion and allow_experimental_multiple_joins_emulation are now obsolete and do nothing. #7974 (alexey-milovidov) Add new ANY JOIN logic for StorageJoin consistent with JOIN operation. To upgrade without changes in behaviour you need add SETTINGS any_join_distinct_right_table_keys = 1 to Engine Join tables metadata or recreate these tables after upgrade. #8400 (Artem Zuikov) Require server to be restarted to apply the changes in logging configuration. This is a temporary workaround to avoid the bug where the server logs to a deleted log file (see #8696). #8707 (Alexander Kuzmenkov) Added information about part paths to system.merges. #8043 (Vladimir Chebotarev) Add ability to execute SYSTEM RELOAD DICTIONARY query in ON CLUSTER mode. #8288 (Guillaume Tassery) Add ability to execute CREATE DICTIONARY queries in ON CLUSTER mode. #8163 (alesapin) Now user's profile in users.xml can inherit multiple profiles. #8343 (Mikhail f. Shiryaev) Added system.stack_trace table that allows to look at stack traces of all server threads. This is useful for developers to introspect server state. This fixes #7576. #8344 (alexey-milovidov) Add DateTime64 datatype with configurable sub-second precision. #7170 (Vasily Nemkov) Add table function clusterAllReplicas which allows to query all the nodes in the cluster. #8493 (kiran sunkari) Add aggregate function categoricalInformationValue which calculates the information value of a discrete feature. #8117 (hcz) Speed up parsing of data files in CSV, TSV and JSONEachRow format by doing it in parallel. #7780 (Alexander Kuzmenkov) Add function bankerRound which performs banker's rounding. #8112 (hcz) Support more languages in embedded dictionary for region names: 'ru', 'en', 'ua', 'uk', 'by', 'kz', 'tr', 'de', 'uz', 'lv', 'lt', 'et', 'pt', 'he', 'vi'. #8189 (alexey-milovidov) Improvements in consistency of ANY JOIN logic. Now t1 ANY LEFT JOIN t2 equals t2 ANY RIGHT JOIN t1. #7665 (Artem Zuikov) Add setting any_join_distinct_right_table_keys which enables old behaviour for ANY INNER JOIN. #7665 (Artem Zuikov) Add new SEMI and ANTI JOIN. Old ANY INNER JOIN behaviour now available as SEMI LEFT JOIN. #7665 (Artem Zuikov) Added Distributed format for File engine and file table function which allows to read from .bin files generated by asynchronous inserts into Distributed table. #8535 (Nikolai Kochetov) Add optional reset column argument for runningAccumulate which allows to reset aggregation results for each new key value. #8326 (Sergey Kononenko) Add ability to use ClickHouse as Prometheus endpoint. #7900 (vdimir) Add section in config.xml which restricts allowed hosts for remote table engines and table functions URL, S3, HDFS. #7154 (Mikhail Korotov) Added function greatCircleAngle which calculates the distance on a sphere in degrees. #8105 (alexey-milovidov) Changed Earth radius to be consistent with H3 library. #8105 (alexey-milovidov) Added JSONCompactEachRow and JSONCompactEachRowWithNamesAndTypes formats for input and output. #7841 (Mikhail Korotov) Added feature for file-related table engines and table functions (File, S3, URL, HDFS) which allows to read and write gzip files based on additional engine parameter or file extension. #7840 (Andrey Bodrov) Added the randomASCII(length) function, generating a string with a random set of ASCII printable characters. #8401 (BayoNet) Added function JSONExtractArrayRaw which returns an array on unparsed json array elements from JSON string. #8081 (Oleg Matrokhin) Add arrayZip function which allows to combine multiple arrays of equal lengths into one array of tuples. #8149 (Winter Zhang) Add ability to move data between disks according to configured TTL-expressions for *MergeTree table engines family. #8140 (Vladimir Chebotarev) Added new aggregate function avgWeighted which allows to calculate weighted average. #7898 (Andrey Bodrov) Now parallel parsing is enabled by default for TSV, TSKV, CSV and JSONEachRow formats. #7894 (Nikita Mikhaylov) Add several geo functions from H3 library: h3GetResolution, h3EdgeAngle, h3EdgeLength, h3IsValid and h3kRing. #8034 (Konstantin Malanchev) Added support for brotli (br) compression in file-related storages and table functions. This fixes #8156. #8526 (alexey-milovidov) Add groupBit* functions for the SimpleAggregationFunction type. #8485 (Guillaume Tassery) ClickHouse release v20.1.6.30, 2020-03-05 Add deduplicate_blocks_in_dependent_materialized_views option to control the behaviour of idempotent inserts into tables with materialized views. This new feature was added to the bugfix release by a special request from Altinity.#9070 (urykhy) ClickHouse release v20.3.2.1, 2020-03-12 Fixed the issue file name too long when sending data for Distributed tables for a large number of replicas. Fixed the issue that replica credentials were exposed in the server log. The format of directory name on disk was changed to [shard{shard_index}[_replica{replica_index}]]. #8911 (Mikhail Korotov) After you upgrade to the new version, you will not be able to downgrade without manual intervention, because old server version does not recognize the new directory format. If you want to downgrade, you have to manually rename the corresponding directories to the old format. This change is relevant only if you have used asynchronous INSERTs to Distributed tables. In the version 20.3.3 we will introduce a setting that will allow you to enable the new format gradually. Changed the format of replication log entries for mutation commands. You have to wait for old mutations to process before installing the new version. Implement simple memory profiler that dumps stacktraces to system.trace_log every N bytes over soft allocation limit #8765 (Ivan) #9472 (alexey-milovidov) The column of system.trace_log was renamed from timer_type to trace_type. This will require changes in third-party performance analysis and flamegraph processing tools. Use OS thread id everywhere instead of internal thread number. This fixes #7477 Old clickhouse-client cannot receive logs that are send from the server when the setting send_logs_level is enabled, because the names and types of the structured log messages were changed. On the other hand, different server versions can send logs with different types to each other. When you don't use the send_logs_level setting, you should not care. #8954 (alexey-milovidov) Remove indexHint function #9542 (alexey-milovidov) Remove findClusterIndex, findClusterValue functions. This fixes #8641. If you were using these functions, send an email to clickhouse-feedback@yandex-team.com #9543 (alexey-milovidov) Now it's not allowed to create columns or add columns with SELECT subquery as default expression. #9481 (alesapin) Require aliases for subqueries in JOIN. #9274 (Artem Zuikov) Improved ALTER MODIFY/ADD queries logic. Now you cannot ADD column without type, MODIFY default expression does not change type of column and MODIFY type does not loose default expression value. Fixes #8669. #9227 (alesapin) Require server to be restarted to apply the changes in logging configuration. This is a temporary workaround to avoid the bug where the server logs to a deleted log file (see #8696). #8707 (Alexander Kuzmenkov) The setting experimental_use_processors is enabled by default. This setting enables usage of the new query pipeline. This is internal refactoring and we expect no visible changes. If you will see any issues, set it to back zero. #8768 (alexey-milovidov) Add Avro and AvroConfluent input/output formats #8571 (Andrew Onyshchuk) #8957 (Andrew Onyshchuk) #8717 (alexey-milovidov) Multi-threaded and non-blocking updates of expired keys in cache dictionaries (with optional permission to read old ones). #8303 (Nikita Mikhaylov) Add query ALTER ... MATERIALIZE TTL. It runs mutation that forces to remove expired data by TTL and recalculates meta-information about TTL in all parts. #8775 (Anton Popov) Switch from HashJoin to MergeJoin (on disk) if needed #9082 (Artem Zuikov) Added MOVE PARTITION command for ALTER TABLE #4729 #6168 (Guillaume Tassery) Reloading storage configuration from configuration file on the fly. #8594 (Vladimir Chebotarev) Allowed to change storage_policy to not less rich one. #8107 (Vladimir Chebotarev) Added support for globs/wildcards for S3 storage and table function. #8851 (Vladimir Chebotarev) Implement bitAnd, bitOr, bitXor, bitNot for FixedString(N) datatype. #9091 (Guillaume Tassery) Added function bitCount. This fixes #8702. #8708 (alexey-milovidov) #8749 (ikopylov) Add generateRandom table function to generate random rows with given schema. Allows to populate arbitrary test table with data. #8994 (Ilya Yatsishin) JSONEachRowFormat: support special case when objects enclosed in top-level array. #8860 (Kruglov Pavel) Now it's possible to create a column with DEFAULT expression which depends on a column with default ALIAS expression. #9489 (alesapin) Allow to specify --limit more than the source data size in clickhouse-obfuscator. The data will repeat itself with different random seed. #9155 (alexey-milovidov) Added groupArraySample function (similar to groupArray) with reservior sampling algorithm. #8286 (Amos Bird) Now you can monitor the size of update queue in cache/complex_key_cache dictionaries via system metrics. #9413 (Nikita Mikhaylov) Allow to use CRLF as a line separator in CSV output format with setting output_format_csv_crlf_end_of_line is set to 1 #8934 #8935 #8963 (Mikhail Korotov) Implement more functions of the H3 API: h3GetBaseCell, h3HexAreaM2, h3IndexesAreNeighbors, h3ToChildren, h3ToString and stringToH3 #8938 (Nico Mandery) New setting introduced: max_parser_depth to control maximum stack size and allow large complex queries. This fixes #6681 and #7668. #8647 (Maxim Smirnov) Add a setting force_optimize_skip_unused_shards setting to throw if skipping of unused shards is not possible #8805 (Azat Khuzhin) Allow to configure multiple disks/volumes for storing data for send in Distributed engine #8756 (Azat Khuzhin) Support storage policy () for storing temporary data. #8750 (Azat Khuzhin) Added X-ClickHouse-Exception-Code HTTP header that is set if exception was thrown before sending data. This implements #4971. #8786 (Mikhail Korotov) Added function ifNotFinite. It is just a syntactic sugar: ifNotFinite(x, y) = isFinite(x) ? x : y. #8710 (alexey-milovidov) Added last_successful_update_time column in system.dictionaries table #9394 (Nikita Mikhaylov) Add blockSerializedSize function (size on disk without compression) #8952 (Azat Khuzhin) Add function moduloOrZero #9358 (hcz) Added system tables system.zeros and system.zeros_mt as well as tale functions zeros() and zeros_mt(). Tables (and table functions) contain single column with name zero and type UInt8. This column contains zeros. It is needed for test purposes as the fastest method to generate many rows. This fixes #6604 #9593 (Nikolai Kochetov) Add new compact format of parts in MergeTree-family tables in which all columns are stored in one file. It helps to increase performance of small and frequent inserts. The old format (one file per column) is now called wide. Data storing format is controlled by settings min_bytes_for_wide_part and min_rows_for_wide_part. #8290 (Anton Popov) Support for S3 storage for Log, TinyLog and StripeLog tables. #8862 (Pavel Kovalenko) ClickHouse release v20.3.6.40, 2020-04-16 Added function isConstant. This function checks whether its argument is constant expression and returns 1 or 0. It is intended for development, debugging and demonstration purposes. #10198 (alexey-milovidov). ClickHouse release v20.3.11.97-lts 2020-06-10 Now ClickHouse controls timeouts of dictionary sources on its side. Two new settings added to cache dictionary configuration: strict_max_lifetime_seconds, which is max_lifetime by default and query_wait_timeout_milliseconds, which is one minute by default. The first settings is also useful with allow_read_expired_keys settings (to forbid reading very expired keys). #10337 (Nikita Mikhaylov). ClickHouse release v20.4.2.9, 2020-05-12 System tables (e.g. system.query_log, system.trace_log, system.metric_log) are using compact data part format for parts smaller than 10 MiB in size. Compact data part format is supported since version 20.3. If you are going to downgrade to version less than 20.3, you should manually delete table data for system logs in /var/lib/clickhouse/data/system/. When string comparison involves FixedString and compared arguments are of different sizes, do comparison as if smaller string is padded to the length of the larger. This is intented for SQL compatibility if we imagine that FixedString data type corresponds to SQL CHAR. This closes #9272. #10363 (alexey-milovidov) Make SHOW CREATE TABLE multiline. Now it is more readable and more like MySQL. #10049 (Azat Khuzhin) Added a setting validate_polygons that is used in pointInPolygon function and enabled by default. #9857 (alexey-milovidov) Add support for secured connection from ClickHouse to Zookeeper #10184 (Konstantin Lebedev) Support custom HTTP handlers. See #5436 for description. #7572 (Winter Zhang) Add MessagePack Input/Output format. #9889 (Kruglov Pavel) Add Regexp input format. #9196 (Kruglov Pavel) Added output format Markdown for embedding tables in markdown documents. #10317 (Kruglov Pavel) Added support for custom settings section in dictionaries. Also fixes issue #2829. #10137 (Artem Streltsov) Added custom settings support in DDL-queries for CREATE DICTIONARY #10465 (Artem Streltsov) Add simple server-wide memory profiler that will collect allocation contexts when server memory usage becomes higher than the next allocation threshold. #10444 (alexey-milovidov) Add setting always_fetch_merged_part which restrict replica to merge parts by itself and always prefer dowloading from other replicas. #10379 (alesapin) Add function JSONExtractKeysAndValuesRaw which extracts raw data from JSON objects #10378 (hcz) Add memory usage from OS to system.asynchronous_metrics. #10361 (alexey-milovidov) Added generic variants for functions least and greatest. Now they work with arbitrary number of arguments of arbitrary types. This fixes #4767 #10318 (alexey-milovidov) Now ClickHouse controls timeouts of dictionary sources on its side. Two new settings added to cache dictionary configuration: strict_max_lifetime_seconds, which is max_lifetime by default, and query_wait_timeout_milliseconds, which is one minute by default. The first settings is also useful with allow_read_expired_keys settings (to forbid reading very expired keys). #10337 (Nikita Mikhaylov) Add log_queries_min_type to filter which entries will be written to query_log #10053 (Azat Khuzhin) Added function isConstant. This function checks whether its argument is constant expression and returns 1 or 0. It is intended for development, debugging and demonstration purposes. #10198 (alexey-milovidov) add joinGetOrNull to return NULL when key is missing instead of returning the default value. #10094 (Amos Bird) Consider NULL to be equal to NULL in IN operator, if the option transform_null_in is set. #10085 (achimbab) Add ALTER TABLE ... RENAME COLUMN for MergeTree table engines family. #9948 (alesapin) Support parallel distributed INSERT SELECT. #9759 (vxider) Add ability to query Distributed over Distributed (w/o distributed_group_by_no_merge) ... #9923 (Azat Khuzhin) Add function arrayReduceInRanges which aggregates array elements in given ranges. #9598 (hcz) Add Dictionary Status on prometheus exporter. #9622 (Guillaume Tassery) Add function arrayAUC #8698 (taiyang-li) Support DROP VIEW statement for better TPC-H compatibility. #9831 (Amos Bird) Add 'strict_order' option to windowFunnel() #9773 (achimbab) Support DATE and TIMESTAMP SQL operators, e.g. SELECT date '2001-01-01' #9691 (Artem Zuikov) ClickHouse release v20.5.2.7-stable 2020-07-02 Return non-Nullable result from COUNT(DISTINCT), and uniq aggregate functions family. If all passed values are NULL, return zero instead. This improves SQL compatibility. #11661 (alexey-milovidov). Added a check for the case when user-level setting is specified in a wrong place. User-level settings should be specified in users.xml inside section for specific user profile (or in for default settings). The server won't start with exception message in log. This fixes #9051. If you want to skip the check, you can either move settings to the appropriate place or add 1 to config.xml. #11449 (alexey-milovidov). The setting input_format_with_names_use_header is enabled by default. It will affect parsing of input formats -WithNames and -WithNamesAndTypes. #10937 (alexey-milovidov). Remove experimental_use_processors setting. It is enabled by default. #10924 (Nikolai Kochetov). Update zstd to 1.4.4. It has some minor improvements in performance and compression ratio. If you run replicas with different versions of ClickHouse you may see reasonable error messages Data after merge is not byte-identical to data on another replicas. with explanation. These messages are Ok and you should not worry. This change is backward compatible but we list it here in changelog in case you will wonder about these messages. #10663 (alexey-milovidov). Added a check for meaningless codecs and a setting allow_suspicious_codecs to control this check. This closes #4966. #10645 (alexey-milovidov). Several Kafka setting changes their defaults. See #11388. When upgrading from versions older than 20.5, if rolling update is performed and cluster contains both versions 20.5 or greater and less than 20.5, if ClickHouse nodes with old versions are restarted and old version has been started up in presence of newer versions, it may lead to Part ... intersects previous part errors. To prevent this error, first install newer clickhouse-server packages on all cluster nodes and then do restarts (so, when clickhouse-server is restarted, it will start up with the new version). TTL DELETE WHERE and TTL GROUP BY for automatic data coarsening and rollup in tables. #10537 (expl0si0nn). Implementation of PostgreSQL wire protocol. #10242 (Movses). Added system tables for users, roles, grants, settings profiles, quotas, row policies; added commands SHOW USER, SHOW [CURRENT|ENABLED] ROLES, SHOW SETTINGS PROFILES. #10387 (Vitaly Baranov). Support writes in ODBC Table function #10554 (ageraab). #10901 (tavplubix). Add query performance metrics based on Linux perf_events (these metrics are calculated with hardware CPU counters and OS counters). It is optional and requires CAP_SYS_ADMIN to be set on clickhouse binary. #9545 Andrey Skobtsov. #11226 (Alexander Kuzmenkov). Now support NULL and NOT NULL modifiers for data types in CREATE query. #11057 (Павел Потемкин). Add ArrowStream input and output format. #11088 (hcz). Support Cassandra as external dictionary source. #4978 (favstovol). Added a new layout direct which loads all the data directly from the source for each query, without storing or caching data. #10622 (Artem Streltsov). Added new complex_key_direct layout to dictionaries, that does not store anything locally during query execution. #10850 (Artem Streltsov). Added support for MySQL style global variables syntax (stub). This is needed for compatibility of MySQL protocol. #11832 (alexey-milovidov). Added syntax highligting to clickhouse-client using replxx. #11422 (Tagir Kuskarov). minMap and maxMap functions were added. #11603 (Ildus Kurbangaliev). Add the system.asynchronous_metric_log table that logs historical metrics from system.asynchronous_metrics. #11588 (Alexander Kuzmenkov). Add functions extractAllGroupsHorizontal(haystack, re) and extractAllGroupsVertical(haystack, re). #11554 (Vasily Nemkov). Add SHOW CLUSTER(S) queries. #11467 (hexiaoting). Add netloc function for extracting network location, similar to urlparse(url), netloc in python. #11356 (Guillaume Tassery). Add 2 more virtual columns for engine=Kafka to access message headers. #11283 (filimonov). Add _timestamp_ms virtual column for Kafka engine (type is Nullable(DateTime64(3))). #11260 (filimonov). Add function randomFixedString. #10866 (Andrei Nekrashevich). Add function fuzzBits that randomly flips bits in a string with given probability. #11237 (Andrei Nekrashevich). Allow comparison of numbers with constant string in comparison operators, IN and VALUES sections. #11647 (alexey-milovidov). Add round_robin load_balancing mode. #11645 (Azat Khuzhin). Add cast_keep_nullable setting. If set CAST(something_nullable AS Type) return Nullable(Type). #11733 (Artem Zuikov). Added column position to system.columns table and column_position to system.parts_columns table. It contains ordinal position of a column in a table starting with 1. This closes #7744. #11655 (alexey-milovidov). ON CLUSTER support for SYSTEM {FLUSH DISTRIBUTED,STOP/START DISTRIBUTED SEND}. #11415 (Azat Khuzhin). Add system.distribution_queue table. #11394 (Azat Khuzhin). Support for all format settings in Kafka, expose some setting on table level, adjust the defaults for better performance. #11388 (filimonov). Add port function (to extract port from URL). #11120 (Azat Khuzhin). Now dictGet* functions accept table names. #11050 (Vitaly Baranov). The clickhouse-format tool is now able to format multiple queries when the -n argument is used. #10852 (Darío). Possibility to configure proxy-resolver for DiskS3. #10744 (Pavel Kovalenko). Make pointInPolygon work with non-constant polygon. PointInPolygon now can take Array(Array(Tuple(..., ...))) as second argument, array of polygon and holes. #10623 (Alexey Ilyukhov) #11421 (Alexey Ilyukhov). Added move_ttl_info to system.parts in order to provide introspection of move TTL functionality. #10591 (Vladimir Chebotarev). Possibility to work with S3 through proxies. #10576 (Pavel Kovalenko). Add NCHAR and NVARCHAR synonims for data types. #11025 (alexey-milovidov). Resolved #7224: added FailedQuery, FailedSelectQuery and FailedInsertQuery metrics to system.events table. #11151 (Nikita Orlov). Add more jemalloc statistics to system.asynchronous_metrics, and ensure that we see up-to-date values for them. #11748 (Alexander Kuzmenkov). Allow to specify default S3 credentials and custom auth headers. #11134 (Grigory Pervakov). Added new functions to import/export DateTime64 as Int64 with various precision: to-/fromUnixTimestamp64Milli/-Micro/-Nano. #10923 (Vasily Nemkov). Allow specifying mongodb:// URI for MongoDB dictionaries. #10915 (Alexander Kuzmenkov). OFFSET keyword can now be used without an affiliated LIMIT clause. #10802 (Guillaume Tassery). Added system.licenses table. This table contains licenses of third-party libraries that are located in contrib directory. This closes #2890. #10795 (alexey-milovidov). New function function toStartOfSecond(DateTime64) -> DateTime64 that nullifies sub-second part of DateTime64 value. #10722 (Vasily Nemkov). Add new input format JSONAsString that accepts a sequence of JSON objects separated by newlines, spaces and/or commas. #10607 (Kruglov Pavel). Allowed to profile memory with finer granularity steps than 4 MiB. Added sampling memory profiler to capture random allocations/deallocations. #10598 (alexey-milovidov). SimpleAggregateFunction now also supports sumMap. #10000 (Ildus Kurbangaliev). Support ALTER RENAME COLUMN for the distributed table engine. Continuation of #10727. Fixes #10747. #10887 (alesapin). ClickHouse release v20.6.3.28-stable When upgrading from versions older than 20.5, if rolling update is performed and cluster contains both versions 20.5 or greater and less than 20.5, if ClickHouse nodes with old versions are restarted and old version has been started up in presence of newer versions, it may lead to Part ... intersects previous part errors. To prevent this error, first install newer clickhouse-server packages on all cluster nodes and then do restarts (so, when clickhouse-server is restarted, it will start up with the new version). Added an initial implementation of EXPLAIN query. Syntax: EXPLAIN SELECT .... This fixes #1118. #11873 (Nikolai Kochetov). Added storage RabbitMQ. #11069 (Kseniia Sumarokova). Implemented PostgreSQL-like ILIKE operator for #11710. #12125 (Mike). Supported RIGHT and FULL JOIN with SET join_algorithm = 'partial_merge'. Only ALL strictness is allowed (ANY, SEMI, ANTI, ASOF are not). #12118 (Artem Zuikov). Added a function initializeAggregation to initialize an aggregation based on a single value. #12109 (Guillaume Tassery). Supported ALTER TABLE ... [ADD|MODIFY] COLUMN ... FIRST #4006. #12073 (Winter Zhang). Added function parseDateTimeBestEffortUS. #12028 (flynn). Support format ORC for output (was supported only for input). #11662 (Kruglov Pavel). ClickHouse release v20.7.2.30-stable, 2020-08-31 Function modulo (operator %) with at least one floating point number as argument will calculate remainder of division directly on floating point numbers without converting both arguments to integers. It makes behaviour compatible with most of DBMS. This also applicable for Date and DateTime data types. Added alias mod. This closes #7323. #12585 (alexey-milovidov). Deprecate special printing of zero Date/DateTime values as 0000-00-00 and 0000-00-00 00:00:00. #12442 (alexey-milovidov). The function groupArrayMoving* was not working for distributed queries. It's result was calculated within incorrect data type (without promotion to the largest type). The function groupArrayMovingAvg was returning integer number that was inconsistent with the avg function. This fixes #12568. #12622 (alexey-milovidov). Add sanity check for MergeTree settings. If the settings are incorrect, the server will refuse to start or to create a table, printing detailed explanation to the user. #13153 (alexey-milovidov). Protect from the cases when user may set background_pool_size to value lower than number_of_free_entries_in_pool_to_execute_mutation or number_of_free_entries_in_pool_to_lower_max_size_of_merge. In these cases ALTERs won't work or the maximum size of merge will be too limited. It will throw exception explaining what to do. This closes #10897. #12728 (alexey-milovidov). When upgrading from versions older than 20.5, if rolling update is performed and cluster contains both versions 20.5 or greater and less than 20.5, if ClickHouse nodes with old versions are restarted and old version has been started up in presence of newer versions, it may lead to Part ... intersects previous part errors. To prevent this error, first install newer clickhouse-server packages on all cluster nodes and then do restarts (so, when clickhouse-server is restarted, it will start up with the new version). Polygon dictionary type that provides efficient \"reverse geocoding\" lookups - to find the region by coordinates in a dictionary of many polygons (world map). It is using carefully optimized algorithm with recursive grids to maintain low CPU and memory usage. #9278 (achulkov2). Added support of LDAP authentication for preconfigured users (\"Simple Bind\" method). #11234 (Denis Glazachev). Introduce setting alter_partition_verbose_result which outputs information about touched parts for some types of ALTER TABLE ... PARTITION ... queries (currently ATTACH and FREEZE). Closes #8076. #13017 (alesapin). Add bayesAB function for bayesian-ab-testing. #12327 (achimbab). Added system.crash_log table into which stack traces for fatal errors are collected. This table should be empty. #12316 (alexey-milovidov). Added http headers X-ClickHouse-Database and X-ClickHouse-Format which may be used to set default database and output format. #12981 (hcz). Add minMap and maxMap functions support to SimpleAggregateFunction. #12662 (Ildus Kurbangaliev). Add setting allow_non_metadata_alters which restricts to execute ALTER queries which modify data on disk. Disabled be default. Closes #11547. #12635 (alesapin). A function formatRow is added to support turning arbitrary expressions into a string via given format. It's useful for manipulating SQL outputs and is quite versatile combined with the columns function. #12574 (Amos Bird). Add FROM_UNIXTIME function for compatibility with MySQL, related to 12149. #12484 (flynn). Allow Nullable types as keys in MergeTree tables if allow_nullable_key table setting is enabled. Closes #5319. #12433 (Amos Bird). Integration with COS. #12386 (fastio). Add mapAdd and mapSubtract functions for adding/subtracting key-mapped values. #11735 (Ildus Kurbangaliev). ClickHouse release v20.8.2.3-stable, 2020-09-08 Now OPTIMIZE FINAL query does not recalculate TTL for parts that were added before TTL was created. Use ALTER TABLE ... MATERIALIZE TTL once to calculate them, after that OPTIMIZE FINAL will evaluate TTL's properly. This behavior never worked for replicated tables. #14220 (alesapin). Extend parallel_distributed_insert_select setting, adding an option to run INSERT into local table. The setting changes type from Bool to UInt64, so the values false and true are no longer supported. If you have these values in server configuration, the server will not start. Please replace them with 0 and 1, respectively. #14060 (Azat Khuzhin). Remove support for the ODBCDriver input/output format. This was a deprecated format once used for communication with the ClickHouse ODBC driver, now long superseded by the ODBCDriver2 format. Resolves #13629. #13847 (hexiaoting). When upgrading from versions older than 20.5, if rolling update is performed and cluster contains both versions 20.5 or greater and less than 20.5, if ClickHouse nodes with old versions are restarted and old version has been started up in presence of newer versions, it may lead to Part ... intersects previous part errors. To prevent this error, first install newer clickhouse-server packages on all cluster nodes and then do restarts (so, when clickhouse-server is restarted, it will start up with the new version). Add the ability to specify Default compression codec for columns that correspond to settings specified in config.xml. Implements: #9074. #14049 (alesapin). Support Kerberos authentication in Kafka, using krb5 and cyrus-sasl libraries. #12771 (Ilya Golshtein). Add function normalizeQuery that replaces literals, sequences of literals and complex aliases with placeholders. Add function normalizedQueryHash that returns identical 64bit hash values for similar queries. It helps to analyze query log. This closes #11271. #13816 (alexey-milovidov). Add time_zones table. #13880 (Bharat Nallan). Add function defaultValueOfTypeName that returns the default value for a given type. #13877 (hcz). Add countDigits(x) function that count number of decimal digits in integer or decimal column. Add isDecimalOverflow(d, [p]) function that checks if the value in Decimal column is out of its (or specified) precision. #14151 (Artem Zuikov). Add quantileExactLow and quantileExactHigh implementations with respective aliases for medianExactLow and medianExactHigh. #13818 (Bharat Nallan). Added date_trunc function that truncates a date/time value to a specified date/time part. #13888 (Vladimir Golovchenko). Add new optional section to the main config. #13425 (Vitaly Baranov). Add ALTER SAMPLE BY statement that allows to change table sample clause. #13280 (Amos Bird). Function position now supports optional start_pos argument. #13237 (vdimir). ClickHouse release v20.9.2.20, 2020-09-22 When upgrading from versions older than 20.5, if rolling update is performed and cluster contains both versions 20.5 or greater and less than 20.5, if ClickHouse nodes with old versions are restarted and old version has been started up in presence of newer versions, it may lead to Part ... intersects previous part errors. To prevent this error, first install newer clickhouse-server packages on all cluster nodes and then do restarts (so, when clickhouse-server is restarted, it will start up with the new version). Added column transformers EXCEPT, REPLACE, APPLY, which can be applied to the list of selected columns (after or COLUMNS(...)). For example, you can write SELECT EXCEPT(URL) REPLACE(number + 1 AS number). Another example: select * apply(length) apply(max) from wide_string_table to find out the maxium length of all string columns. #14233 (Amos Bird). Added an aggregate function rankCorr which computes a rank correlation coefficient. #11769 (antikvist) #14411 (Nikita Mikhaylov). Added table function view which turns a subquery into a table object. This helps passing queries around. For instance, it can be used in remote/cluster table functions. #12567 (Amos Bird). ClickHouse release v20.10.3.30, 2020-10-28 Make multiple_joins_rewriter_version obsolete. Remove first version of joins rewriter. #15472 (Artem Zuikov). Change default value of format_regexp_escaping_rule setting (it's related to Regexp format) to Raw (it means - read whole subpattern as a value) to make the behaviour more like to what users expect. #15426 (alexey-milovidov). Add support for nested multiline comments / comment / comment / / in SQL. This conforms to the SQL standard. #14655 (alexey-milovidov). Added MergeTree settings (max_replicated_merges_with_ttl_in_queue and max_number_of_merges_with_ttl_in_pool) to control the number of merges with TTL in the background pool and replicated queue. This change breaks compatibility with older versions only if you use delete TTL. Otherwise, replication will stay compatible. You can avoid incompatibility issues if you update all shard replicas at once or execute SYSTEM STOP TTL MERGES until you finish the update of all replicas. If you'll get an incompatible entry in the replication queue, first of all, execute SYSTEM STOP TTL MERGES and after ALTER TABLE ... DETACH PARTITION ... the partition where incompatible TTL merge was assigned. Attach it back on a single replica. #14490 (alesapin). When upgrading from versions older than 20.5, if rolling update is performed and cluster contains both versions 20.5 or greater and less than 20.5, if ClickHouse nodes with old versions are restarted and old version has been started up in presence of newer versions, it may lead to Part ... intersects previous part errors. To prevent this error, first install newer clickhouse-server packages on all cluster nodes and then do restarts (so, when clickhouse-server is restarted, it will start up with the new version). Background data recompression. Add the ability to specify TTL ... RECOMPRESS codec_name for MergeTree table engines family. #14494 (alesapin). Add parallel quorum inserts. This closes #15601. #15601 (Latysheva Alexandra). Settings for additional enforcement of data durability. Useful for non-replicated setups. #11948 (Anton Popov). When duplicate block is written to replica where it does not exist locally (has not been fetched from replicas), don't ignore it and write locally to achieve the same effect as if it was successfully replicated. #11684 (alexey-milovidov). Now we support WITH AS (subquery) ... to introduce named subqueries in the query context. This closes #2416. This closes #4967. #14771 (Amos Bird). Introduce enable_global_with_statement setting which propagates the first select's WITH statements to other select queries at the same level, and makes aliases in WITH statements visible to subqueries. #15451 (Amos Bird). Secure inter-cluster query execution (with initial_user as current query user). #13156 (Azat Khuzhin). #15551 (Azat Khuzhin). Add the ability to remove column properties and table TTLs. Introduced queries ALTER TABLE MODIFY COLUMN col_name REMOVE what_to_remove and ALTER TABLE REMOVE TTL. Both operations are lightweight and executed at the metadata level. #14742 (alesapin). Added format RawBLOB. It is intended for input or output a single value without any escaping and delimiters. This closes #15349. #15364 (alexey-milovidov). Add the reinterpretAsUUID function that allows to convert a big-endian byte string to UUID. #15480 (Alexander Kuzmenkov). Implement force_data_skipping_indices setting. #15642 (Azat Khuzhin). Add a setting output_format_pretty_row_numbers to numerate the result in Pretty formats. This closes #15350. #15443 (flynn). Added query obfuscation tool. It allows to share more queries for better testing. This closes #15268. #15321 (alexey-milovidov). Add table function null('structure'). #14797 (vxider). Added formatReadableQuantity function. It is useful for reading big numbers by human. #14725 (Artem Hnilov). Add format LineAsString that accepts a sequence of lines separated by newlines, every line is parsed as a whole as a single String field. #14703 (Nikita Mikhaylov), #13846 (hexiaoting). Add JSONStrings format which output data in arrays of strings. #14333 (hcz). Add support for \"Raw\" column format for Regexp format. It allows to simply extract subpatterns as a whole without any escaping rules. #15363 (alexey-milovidov). Allow configurable NULL representation for TSV output format. It is controlled by the setting output_format_tsv_null_representation which is \\N by default. This closes #9375. Note that the setting only controls output format and \\N is the only supported NULL representation for TSV input format. #14586 (Kruglov Pavel). Support Decimal data type for MaterializeMySQL. MaterializeMySQL is an experimental feature. #14535 (Winter Zhang). Add new feature: SHOW DATABASES LIKE 'xxx'. #14521 (hexiaoting). Added a script to import (arbitrary) git repository to ClickHouse as a sample dataset. #14471 (alexey-milovidov). Now insert statements can have asterisk (or variants) with column transformers in the column list. #14453 (Amos Bird). New query complexity limit settings max_rows_to_read_leaf, max_bytes_to_read_leaf for distributed queries to limit max rows/bytes read on the leaf nodes. Limit is applied for local reads only, excluding the final merge stage on the root node. #14221 (Roman Khavronenko). Allow user to specify settings for ReplicatedMergeTree storage in section of config file. It works similarly to section. For ReplicatedMergeTree storages settings from and are applied together, but settings from has higher priority. Added system.replicated_merge_tree_settings table. #13573 (Amos Bird). Add mapPopulateSeries function. #13166 (Ildus Kurbangaliev). Supporting MySQL types: decimal (as ClickHouse Decimal) and datetime with sub-second precision (as DateTime64). #11512 (Vasily Nemkov). Introduce event_time_microseconds field to system.text_log, system.trace_log, system.query_log and system.query_thread_log tables. #14760 (Bharat Nallan). Add event_time_microseconds to system.asynchronous_metric_log & system.metric_log tables. #14514 (Bharat Nallan). Add query_start_time_microseconds field to system.query_log & system.query_thread_log tables. #14252 (Bharat Nallan). ClickHouse release v20.11.2.1, 2020-11-11 If some profile was specified in distributed_ddl config section, then this profile could overwrite settings of default profile on server startup. It's fixed, now settings of distributed DDL queries should not affect global server settings. #16635 (tavplubix). Restrict to use of non-comparable data types (like AggregateFunction) in keys (Sorting key, Primary key, Partition key, and so on). #16601 (alesapin). Remove ANALYZE and AST queries, and make the setting enable_debug_queries obsolete since now it is the part of full featured EXPLAIN query. #16536 (Ivan). Aggregate functions boundingRatio, rankCorr, retention, timeSeriesGroupSum, timeSeriesGroupRateSum, windowFunnel were erroneously made case-insensitive. Now their names are made case sensitive as designed. Only functions that are specified in SQL standard or made for compatibility with other DBMS or functions similar to those should be case-insensitive. #16407 (alexey-milovidov). Make rankCorr function return nan on insufficient data #16124. #16135 (hexiaoting). When upgrading from versions older than 20.5, if rolling update is performed and cluster contains both versions 20.5 or greater and less than 20.5, if ClickHouse nodes with old versions are restarted and old version has been started up in presence of newer versions, it may lead to Part ... intersects previous part errors. To prevent this error, first install newer clickhouse-server packages on all cluster nodes and then do restarts (so, when clickhouse-server is restarted, it will start up with the new version). Added support of LDAP as a user directory for locally non-existent users. #12736 (Denis Glazachev). Add system.replicated_fetches table which shows currently running background fetches. #16428 (alesapin). Added setting date_time_output_format. #15845 (Maksim Kita). Added minimal web UI to ClickHouse. #16158 (alexey-milovidov). Allows to read/write Single protobuf message at once (w/o length-delimiters). #15199 (filimonov). Added initial OpenTelemetry support. ClickHouse now accepts OpenTelemetry traceparent headers over Native and HTTP protocols, and passes them downstream in some cases. The trace spans for executed queries are saved into the system.opentelemetry_span_log table. #14195 (Alexander Kuzmenkov). Allow specify primary key in column list of CREATE TABLE query. This is needed for compatibility with other SQL dialects. #15823 (Maksim Kita). Implement OFFSET offset_row_count {ROW | ROWS} FETCH {FIRST | NEXT} fetch_row_count {ROW | ROWS} {ONLY | WITH TIES} in SELECT query with ORDER BY. This is the SQL-standard way to specify LIMIT. #15855 (hexiaoting). errorCodeToName function - return variable name of the error (useful for analyzing query_log and similar). system.errors table - shows how many times errors has been happened (respects system_events_show_zero_values). #16438 (Azat Khuzhin). Added function untuple which is a special function which can introduce new columns to the SELECT list by expanding a named tuple. #16242 (Nikolai Kochetov, Amos Bird). Now we can provide identifiers via query parameters. And these parameters can be used as table objects or columns. #16594 (Amos Bird). Added big integers (UInt256, Int128, Int256) and UUID data types support for MergeTree BloomFilter index. Big integers is an experimental feature. #16642 (Maksim Kita). Add farmFingerprint64 function (non-cryptographic string hashing). #16570 (Jacob Hayes). Add log_queries_min_query_duration_ms, only queries slower than the value of this setting will go to query_log/query_thread_log (i.e. something like slow_query_log in mysql). #16529 (Azat Khuzhin). Ability to create a docker image on the top of Alpine. Uses precompiled binary and glibc components from ubuntu 20.04. #16479 (filimonov). Added toUUIDOrNull, toUUIDOrZero cast functions. #16337 (Maksim Kita). Add max_concurrent_queries_for_all_users setting, see #6636 for use cases. #16154 (nvartolomei). Add a new option print_query_id to clickhouse-client. It helps generate arbitrary strings with the current query id generated by the client. Also print query id in clickhouse-client by default. #15809 (Amos Bird). Add tid and logTrace functions. This closes #9434. #15803 (flynn). Add function formatReadableTimeDelta that format time delta to human readable string ... #15497 (Filipe Caixeta). Added disable_merges option for volumes in multi-disk configuration. #13956 (Vladimir Chebotarev). ClickHouse release v20.12.3.3-stable, 2020-12-13 Enable use_compact_format_in_distributed_parts_names by default (see the documentation for the reference). #16728 (Azat Khuzhin). Accept user settings related to file formats (e.g. format_csv_delimiter) in the SETTINGS clause when creating a table that uses File engine, and use these settings in all INSERTs and SELECTs. The file format settings changed in the current user session, or in the SETTINGS clause of a DML query itself, no longer affect the query. #16591 (Alexander Kuzmenkov). add .xz compression/decompression support.It enables using .xz in file() function. This closes #8828. #16578 (Abi Palagashvili). Introduce the query ALTER TABLE ... DROP|DETACH PART 'part_name'. #15511 (nvartolomei). Added new ALTER UPDATE/DELETE IN PARTITION syntax. #13403 (Vladimir Chebotarev). Allow formatting named tuples as JSON objects when using JSON input/output formats, controlled by the output_format_json_named_tuples_as_objects setting, disabled by default. #17175 (Alexander Kuzmenkov). Add a possibility to input enum value as it's id in TSV and CSV formats by default. #16834 (Kruglov Pavel). Add COLLATE support for Nullable, LowCardinality, Array and Tuple, where nested type is String. Also refactor the code associated with collations in ColumnString.cpp. #16273 (Kruglov Pavel). New tcpPort function returns TCP port listened by this server. #17134 (Ivan). Add new math functions: acosh, asinh, atan2, atanh, cosh, hypot, log1p, sinh. #16636 (Konstantin Malanchev). Possibility to distribute the merges between different replicas. Introduces the execute_merges_on_single_replica_time_threshold mergetree setting. #16424 (filimonov). Add setting aggregate_functions_null_for_empty for SQL standard compatibility. This option will rewrite all aggregate functions in a query, adding -OrNull suffix to them. Implements 10273. #16123 (flynn). Updated DateTime, DateTime64 parsing to accept string Date literal format. #16040 (Maksim Kita). Make it possible to change the path to history file in clickhouse-client using the --history_file parameter. #15960 (Maksim Kita). ClickHouse release 21.* ClickHouse release v21.1.2.15-stable 2021-01-18 The setting input_format_null_as_default is enabled by default. #17525 (alexey-milovidov). Check settings constraints for profile settings from config. Server will fail to start if users.xml contain settings that do not meet corresponding constraints. #18486 (tavplubix). Restrict ALTER MODIFY SETTING from changing storage settings that affects data parts (write_final_mark and enable_mixed_granularity_parts). #18306 (Amos Bird). Set insert_quorum_parallel to 1 by default. It is significantly more convenient to use than \"sequential\" quorum inserts. But if you rely to sequential consistency, you should set the setting back to zero. #17567 (alexey-milovidov). Remove sumburConsistentHash function. This closes #18120. #18656 (alexey-milovidov). Removed aggregate functions timeSeriesGroupSum, timeSeriesGroupRateSum because a friend of mine said they never worked. This fixes #16869. If you have luck using these functions, write a email to clickhouse-feedback@yandex-team.com. #17423 (alexey-milovidov). Prohibit toUnixTimestamp(Date()) (before it just returns UInt16 representation of Date). #17376 (Azat Khuzhin). Allow using extended integer types (Int128, Int256, UInt256) in avg and avgWeighted functions. Also allow using different types (integer, decimal, floating point) for value and for weight in avgWeighted function. This is a backward-incompatible change: now the avg and avgWeighted functions always return Float64 (as documented). Before this change the return type for Decimal arguments was also Decimal. #15419 (Mike). Expression toUUID(N) no longer works. Replace with toUUID('00000000-0000-0000-0000-000000000000'). This change is motivated by non-obvious results of toUUID(N) where N is non zero. SSL Certificates with incorrect \"key usage\" are rejected. In previous versions they are used to work. See #19262. incl references to substitutions file (/etc/metrika.xml) were removed from the default config (, , , , ). If you were using substitutions file and were relying on those implicit references, you should put them back manually and explicitly by adding corresponding sections with incl=\"...\" attributes before the update. See #18740 (alexey-milovidov). Implement gRPC protocol in ClickHouse. #15111 (Vitaly Baranov). Allow to use multiple zookeeper clusters. #17070 (fastio). Implemented REPLACE TABLE and CREATE OR REPLACE TABLE queries. #18521 (tavplubix). Implement UNION DISTINCT and treat the plain UNION clause as UNION DISTINCT by default. Add a setting union_default_mode that allows to treat it as UNION ALL or require explicit mode specification. #16338 (flynn). Added function accurateCastOrNull. This closes #10290. Add type conversions in x IN (subquery) expressions. This closes #10266. #16724 (Maksim Kita). IP Dictionary supports IPv4 / IPv6 types directly. #17571 (vdimir). IP Dictionary supports key fetching. Resolves #18241. #18480 (vdimir). Add .zst compression/decompression support for data import and export. It enables using .zst in file() function and Content-encoding: zstd in HTTP client. This closes #16791 . #17144 (Abi Palagashvili). Added mannWitneyUTest, studentTTest and welchTTest aggregate functions. Refactored rankCorr a bit. #16883 (Nikita Mikhaylov). Add functions countMatches/countMatchesCaseInsensitive. #17459 (Azat Khuzhin). Implement countSubstrings()/countSubstringsCaseInsensitive()/countSubstringsCaseInsensitiveUTF8() (Count the number of substring occurrences). #17347 (Azat Khuzhin). Add information about used databases, tables and columns in system.query_log. Add query_kind and normalized_query_hash fields. #17726 (Amos Bird). Add a setting optimize_on_insert. When enabled, do the same transformation for INSERTed block of data as if merge was done on this block (e.g. Replacing, Collapsing, Aggregating...). This setting is enabled by default. This can influence Materialized View and MaterializeMySQL behaviour (see detailed description). This closes #10683. #16954 (Kruglov Pavel). Kerberos Authenticaiton for HDFS. #16621 (Ilya Golshtein). Support SHOW SETTINGS statement to show parameters in system.settings. SHOW CHANGED SETTINGS and LIKE/ILIKE clause are also supported. #18056 (Jianmei Zhang). Function position now supports POSITION(needle IN haystack) synax for SQL compatibility. This closes #18701. ... #18779 (Jianmei Zhang). Now we have a new storage setting max_partitions_to_read for tables in the MergeTree family. It limits the max number of partitions that can be accessed in one query. A user setting force_max_partition_limit is also added to enforce this constraint. #18712 (Amos Bird). Add query_id column to system.part_log for inserted parts. Closes #10097. #18644 (flynn). Allow create table as select with columns specification. Example CREATE TABLE t1 (x String) ENGINE = Memory AS SELECT 1;. #18060 (Maksim Kita). Added arrayMin, arrayMax, arrayAvg aggregation functions. #18032 (Maksim Kita). Implemented ATTACH TABLE name FROM 'path/to/data/' (col1 Type1, ... query. It creates new table with provided structure and attaches table data from provided directory in user_files. #17903 (tavplubix). Add mutation support for StorageMemory. This closes #9117. #15127 (flynn). Support syntax EXISTS DATABASE name. #18458 (Du Chuan). Support builtin function isIPv4String && isIPv6String like MySQL. #18349 (Du Chuan). Add a new setting insert_distributed_one_random_shard = 1 to allow insertion into multi-sharded distributed table without any distributed key. #18294 (Amos Bird). Add settings min_compress_block_size and max_compress_block_size to MergeTreeSettings, which have higher priority than the global settings and take effect when they are set. close 13890. #17867 (flynn). Add support for 64bit roaring bitmaps. #17858 (Andy Yang). Extended OPTIMIZE ... DEDUPLICATE syntax to allow explicit (or implicit with asterisk/column transformers) list of columns to check for duplicates on. ... #17846 (Vasily Nemkov). Added functions toModifiedJulianDay, fromModifiedJulianDay, toModifiedJulianDayOrNull, and fromModifiedJulianDayOrNull. These functions convert between Proleptic Gregorian calendar date and Modified Julian Day number. #17750 (PHO). Add ability to use custom TLD list: added functions firstSignificantSubdomainCustom, cutToFirstSignificantSubdomainCustom. #17748 (Azat Khuzhin). Add support for PROXYv1 protocol to wrap native TCP interface. Allow quotas to be keyed by proxy-forwarded IP address (applied for PROXYv1 address and for X-Forwarded-For from HTTP interface). This is useful when you provide access to ClickHouse only via trusted proxy (e.g. CloudFlare) but want to account user resources by their original IP addresses. This fixes #17268. #17707 (alexey-milovidov). Now clickhouse-client supports opening EDITOR to edit commands. Alt-Shift-E. #17665 (Amos Bird). Add function encodeXMLComponent to escape characters to place string into XML text node or attribute. #17659 (nauta). Introduce DETACH TABLE/VIEW ... PERMANENTLY syntax, so that after restarting the table does not reappear back automatically on restart (only by explicit request). The table can still be attached back using the short syntax ATTACH TABLE. Implements #5555. Fixes #13850. #17642 (filimonov). Add asynchronous metrics on total amount of rows, bytes and parts in MergeTree tables. This fix #11714. #17639 (flynn). Add settings limit and offset for out-of-SQL pagination: #16176 They are useful for building APIs. These two settings will affect SELECT query as if it is added like select * from (your_original_select_query) t limit xxx offset xxx;. #17633 (hexiaoting). Provide a new aggregator combinator : -SimpleState to build SimpleAggregateFunction types via query. It's useful for defining MaterializedView of AggregatingMergeTree engine, and will benefit projections too. #16853 (Amos Bird). Added queries-file parameter for clickhouse-client and clickhouse-local. #15930 (Maksim Kita). Added query parameter for clickhouse-benchmark. #17832 (Maksim Kita). EXPLAIN AST now support queries other then SELECT. #18136 (taiyang-li). Experimental Feature Added functions for calculation of minHash and simHash of text n-grams and shingles. They are intended for semi-duplicate search. Also functions bitHammingDistance and tupleHammingDistance are added. #7649 (flynn). Add new data type Map. See #1841. First version for Map only supports String type of key and value. #15806 (hexiaoting). Implement alternative SQL parser based on ANTLR4 runtime and generated from EBNF grammar. #11298 (Ivan). ClickHouse release v21.2.2.8-stable, 2021-02-07 Bitwise functions (bitAnd, bitOr, etc) are forbidden for floating point arguments. Now you have to do explicit cast to integer. #19853 (Azat Khuzhin). Forbid lcm/gcd for floats. #19532 (Azat Khuzhin). Fix memory tracking for OPTIMIZE TABLE/merges; account query memory limits and sampling for OPTIMIZE TABLE/merges. #18772 (Azat Khuzhin). Disallow floating point column as partition key, see #18421. #18464 (hexiaoting). Excessive parenthesis in type definitions no longer supported, example: Array((UInt8)). Added PostgreSQL table engine (both select/insert, with support for multidimensional arrays), also as table function. Added PostgreSQL dictionary source. Added PostgreSQL database engine. #18554 (Kseniia Sumarokova). Data type Nested now supports arbitrary levels of nesting. Introduced subcolumns of complex types, such as size0 in Array, null in Nullable, names of Tuple elements, which can be read without reading of whole column. #17310 (Anton Popov). Added Nullable support for FlatDictionary, HashedDictionary, ComplexKeyHashedDictionary, DirectDictionary, ComplexKeyDirectDictionary, RangeHashedDictionary. #18236 (Maksim Kita). Adds a new table called system.distributed_ddl_queue that displays the queries in the DDL worker queue. #17656 (Bharat Nallan). Added support of mapping LDAP group names, and attribute values in general, to local roles for users from ldap user directories. #17211 (Denis Glazachev). Support insert into table function cluster, and for both table functions remote and cluster, support distributing data across nodes by specify sharding key. Close #16752. #18264 (flynn). Add function decodeXMLComponent to decode characters for XML. Example: SELECT decodeXMLComponent('Hello,\"world\"!') #17659. #18542 (nauta). Added functions parseDateTimeBestEffortUSOrZero, parseDateTimeBestEffortUSOrNull. #19712 (Maksim Kita). Add sign math function. #19527 (flynn). Add information about used features (functions, table engines, etc) into system.query_log. #18495. #19371 (Kseniia Sumarokova). Function formatDateTime support the %Q modification to format date to quarter. #19224 (Jianmei Zhang). Support MetaKey+Enter hotkey binding in play UI. #19012 (sundyli). Add three functions for map data type: 1. mapContains(map, key) to check weather map.keys include the second parameter key. 2. mapKeys(map) return all the keys in Array format 3. mapValues(map) return all the values in Array format. #18788 (hexiaoting). Add log_comment setting related to #18494. #18549 (Zijie Lu). Add support of tuple argument to argMin and argMax functions. #17359 (Ildus Kurbangaliev). Support EXISTS VIEW syntax. #18552 (Du Chuan). Add SELECT ALL syntax. closes #18706. #18723 (flynn). ClickHouse release v21.3-lts, 2021-03-12 Now it's not allowed to create MergeTree tables in old syntax with table TTL because it's just ignored. Attach of old tables is still possible. #20282 (alesapin). Now all case-insensitive function names will be rewritten to their canonical representations. This is needed for projection query routing (the upcoming feature). #20174 (Amos Bird). Fix creation of TTL in cases, when its expression is a function and it is the same as ORDER BY key. Now it's allowed to set custom aggregation to primary key columns in TTL with GROUP BY. Backward incompatible: For primary key columns, which are not in GROUP BY and aren't set explicitly now is applied function any instead of max, when TTL is expired. Also if you use TTL with WHERE or GROUP BY you can see exceptions at merges, while making rolling update. #15450 (Anton Popov). Add file engine settings: engine_file_empty_if_not_exists and engine_file_truncate_on_insert. #20620 (M0r64n). Add aggregate function deltaSum for summing the differences between consecutive rows. #20057 (Russ Frank). New event_time_microseconds column in system.part_log table. #20027 (Bharat Nallan). Added timezoneOffset(datetime) function which will give the offset from UTC in seconds. This close #issue:19850. #19962 (keenwolf). Add setting insert_shard_id to support insert data into specific shard from distributed table. #19961 (flynn). Function reinterpretAs updated to support big integers. Fixes #19691. #19858 (Maksim Kita). Added Server Side Encryption Customer Keys (the x-amz-server-side-encryption-customer-(key/md5) header) support in S3 client. See the link. Closes #19428. #19748 (Vladimir Chebotarev). Added implicit_key option for executable dictionary source. It allows to avoid printing key for every record if records comes in the same order as the input keys. Implements #14527. #19677 (Maksim Kita). Add quota type query_selects and query_inserts. #19603 (JackyWoo). Add function extractTextFromHTML #19600 (zlx19950903), (alexey-milovidov). Tables with MergeTree* engine now have two new table-level settings for query concurrency control. Setting max_concurrent_queries limits the number of concurrently executed queries which are related to this table. Setting min_marks_to_honor_max_concurrent_queries tells to apply previous setting only if query reads at least this number of marks. #19544 (Amos Bird). Added file function to read file from user_files directory as a String. This is different from the file table function. This implements #issue:18851. #19204 (keenwolf). Add experimental Replicated database engine. It replicates DDL queries across multiple hosts. #16193 (tavplubix). Introduce experimental support for window functions, enabled with allow_experimental_window_functions = 1. This is a preliminary, alpha-quality implementation that is not suitable for production use and will change in backward-incompatible ways in future releases. Please see the documentation for the list of supported features. #20337 (Alexander Kuzmenkov). Add the ability to backup/restore metadata files for DiskS3. #18377 (Pavel Kovalenko). ClickHouse release 21.4.1 2021-04-12 The toStartOfIntervalFunction will align hour intervals to the midnight (in previous versions they were aligned to the start of unix epoch). For example, toStartOfInterval(x, INTERVAL 11 HOUR) will split every day into three intervals: 00:00:00..10:59:59, 11:00:00..21:59:59 and 22:00:00..23:59:59. This behaviour is more suited for practical needs. This closes #9510. #22060 (alexey-milovidov). Age and Precision in graphite rollup configs should increase from retention to retention. Now it's checked and the wrong config raises an exception. #21496 (Mikhail f. Shiryaev). Fix cutToFirstSignificantSubdomainCustom()/firstSignificantSubdomainCustom() returning wrong result for 3+ level domains present in custom top-level domain list. For input domains matching these custom top-level domains, the third-level domain was considered to be the first significant one. This is now fixed. This change may introduce incompatibility if the function is used in e.g. the sharding key. #21946 (Azat Khuzhin). Column keys in table system.dictionaries was replaced to columns key.names and key.types. Columns key.names, key.types, attribute.names, attribute.types from system.dictionaries table does not require dictionary to be loaded. #21884 (Maksim Kita). Now replicas that are processing the ALTER TABLE ATTACH PART[ITION] command search in their detached/ folders before fetching the data from other replicas. As an implementation detail, a new command ATTACH_PART is introduced in the replicated log. Parts are searched and compared by their checksums. #18978 (Mike Kot). Note: ATTACH PART[ITION] queries may not work during cluster upgrade. It's not possible to rollback to older ClickHouse version after executing ALTER ... ATTACH query in new version as the old servers would fail to pass the ATTACH_PART entry in the replicated log. In this version, empty will block all access to remote hosts while in previous versions it did nothing. If you want to keep old behaviour and you have empty remote_url_allow_hosts element in configuration file, remove it. #20058 (Vladimir Chebotarev). Extended range of DateTime64 to support dates from year 1925 to 2283. Improved support of DateTime around zero date (1970-01-01). #9404 (alexey-milovidov, Vasily Nemkov). Not every time and date functions are working for extended range of dates. Added support of Kerberos authentication for preconfigured users and HTTP requests (GSS-SPNEGO). #14995 (Denis Glazachev). Add prefer_column_name_to_alias setting to use original column names instead of aliases. it is needed to be more compatible with common databases' aliasing rules. This is for #9715 and #9887. #22044 (Amos Bird). Added functions dictGetChildren(dictionary, key), dictGetDescendants(dictionary, key, level). Function dictGetChildren return all children as an array if indexes. It is a inverse transformation for dictGetHierarchy. Function dictGetDescendants return all descendants as if dictGetChildren was applied level times recursively. Zero level value is equivalent to infinity. Closes #14656. #22096 (Maksim Kita). Added executable_pool dictionary source. Close #14528. #21321 (Maksim Kita). Added table function dictionary. It works the same way as Dictionary engine. Closes #21560. #21910 (Maksim Kita). Support Nullable type for PolygonDictionary attribute. #21890 (Maksim Kita). Functions dictGet, dictHas use current database name if it is not specified for dictionaries created with DDL. Closes #21632. #21859 (Maksim Kita). Added function dictGetOrNull. It works like dictGet, but return Null in case key was not found in dictionary. Closes #22375. #22413 (Maksim Kita). Added async update in ComplexKeyCache, SSDCache, SSDComplexKeyCache dictionaries. Added support for Nullable type in Cache, ComplexKeyCache, SSDCache, SSDComplexKeyCache dictionaries. Added support for multiple attributes fetch with dictGet, dictGetOrDefault functions. Fixes #21517. #20595 (Maksim Kita). Support dictHas function for RangeHashedDictionary. Fixes #6680. #19816 (Maksim Kita). Add function timezoneOf that returns the timezone name of DateTime or DateTime64 data types. This does not close #9959. Fix inconsistencies in function names: add aliases timezone and timeZone as well as toTimezone and toTimeZone and timezoneOf and timeZoneOf. #22001 (alexey-milovidov). Add new optional clause GRANTEES for CREATE/ALTER USER commands. It specifies users or roles which are allowed to receive grants from this user on condition this user has also all required access granted with grant option. By default GRANTEES ANY is used which means a user with grant option can grant to anyone. Syntax: CREATE USER ... GRANTEES {user | role | ANY | NONE} [,...] [EXCEPT {user | role} [,...]]. #21641 (Vitaly Baranov). Add new column slowdowns_count to system.clusters. When using hedged requests, it shows how many times we switched to another replica because this replica was responding slowly. Also show actual value of errors_count in system.clusters. #21480 (Kruglov Pavel). Add _partition_id virtual column for MergeTree* engines. Allow to prune partitions by _partition_id. Add partitionID() function to calculate partition id string. #21401 (Amos Bird). Add function isIPAddressInRange to test if an IPv4 or IPv6 address is contained in a given CIDR network prefix. #21329 (PHO). Added new SQL command ALTER TABLE 'table_name' UNFREEZE [PARTITION 'part_expr'] WITH NAME 'backup_name'. This command is needed to properly remove 'freezed' partitions from all disks. #21142 (Pavel Kovalenko). Supports implicit key type conversion for JOIN. #19885 (Vladimir). Support RANGE OFFSET frame (for window functions) for floating point types. Implement lagInFrame/leadInFrame window functions, which are analogous to lag/lead, but respect the window frame. They are identical when the frame is between unbounded preceding and unbounded following. This closes #5485. #21895 (Alexander Kuzmenkov). Zero-copy replication for ReplicatedMergeTree over S3 storage. #16240 (ianton-ru). Added possibility to migrate existing S3 disk to the schema with backup-restore capabilities. #22070 (Pavel Kovalenko). ClickHouse release 21.5, 2021-05-20 Change comparison of integers and floating point numbers when integer is not exactly representable in the floating point data type. In new version comparison will return false as the rounding error will occur. Example: 9223372036854775808.0 != 9223372036854775808, because the number 9223372036854775808 is not representable as floating point number exactly (and 9223372036854775808.0 is rounded to 9223372036854776000.0). But in previous version the comparison will return as the numbers are equal, because if the floating point number 9223372036854776000.0 get converted back to UInt64, it will yield 9223372036854775808. For the reference, the Python programming language also treats these numbers as equal. But this behaviour was dependend on CPU model (different results on AMD64 and AArch64 for some out-of-range numbers), so we make the comparison more precise. It will treat int and float numbers equal only if int is represented in floating point type exactly. #22595 (alexey-milovidov). Remove support for argMin and argMax for single Tuple argument. The code was not memory-safe. The feature was added by mistake and it is confusing for people. These functions can be reintroduced under different names later. This fixes #22384 and reverts #17359. #23393 (alexey-milovidov). Added functions dictGetChildren(dictionary, key), dictGetDescendants(dictionary, key, level). Function dictGetChildren return all children as an array if indexes. It is a inverse transformation for dictGetHierarchy. Function dictGetDescendants return all descendants as if dictGetChildren was applied level times recursively. Zero level value is equivalent to infinity. Improved performance of dictGetHierarchy, dictIsIn functions. Closes #14656. #22096 (Maksim Kita). Added function dictGetOrNull. It works like dictGet, but return Null in case key was not found in dictionary. Closes #22375. #22413 (Maksim Kita). Added a table function s3Cluster, which allows to process files from s3 in parallel on every node of a specified cluster. #22012 (Nikita Mikhaylov). Added support for replicas and shards in MySQL/PostgreSQL table engine / table function. You can write SELECT * FROM mysql('host{1,2}-{1|2}', ...). Closes #20969. #22217 (Kseniia Sumarokova). Added ALTER TABLE ... FETCH PART ... query. It's similar to FETCH PARTITION, but fetches only one part. #22706 (turbo jason). Added a setting max_distributed_depth that limits the depth of recursive queries to Distributed tables. Closes #20229. #21942 (flynn). ClickHouse release 21.6, 2021-06-05 Add Postgres-like cast operator (::). E.g.: [1, 2]::Array(UInt8), 0.1::Decimal(4, 4), number::UInt16. #23871 (Anton Popov). Make big integers production ready. Add support for UInt128 data type. Fix known issues with the Decimal256 data type. Support big integers in dictionaries. Support gcd/lcm functions for big integers. Support big integers in array search and conditional functions. Support LowCardinality(UUID). Support big integers in generateRandom table function and clickhouse-obfuscator. Fix error with returning UUID from scalar subqueries. This fixes #7834. This fixes #23936. This fixes #4176. This fixes #24018. Backward incompatible change: values of UUID type cannot be compared with integer. For example, instead of writing uuid != 0 type uuid != '00000000-0000-0000-0000-000000000000'. #23631 (alexey-milovidov). Support Array data type for inserting and selecting data in Arrow, Parquet and ORC formats. #21770 (taylor12805). Implement table comments. Closes #23225. #23548 (flynn). Support creating dictionaries with DDL queries in clickhouse-local. Closes #22354. Added support for DETACH DICTIONARY PERMANENTLY. Added support for EXCHANGE DICTIONARIES for Atomic database engine. Added support for moving dictionaries between databases using RENAME DICTIONARY. #23436 (Maksim Kita). Add aggregate function uniqTheta to support Theta Sketch in ClickHouse. #23894. #22609 (Ping Yu). Add function splitByRegexp. #24077 (abel-cheng). Add function arrayProduct which accept an array as the parameter, and return the product of all the elements in array. Closes #21613. #23782 (Maksim Kita). Add thread_name column in system.stack_trace. This closes #23256. #24124 (abel-cheng). If insert_null_as_default = 1, insert default values instead of NULL in INSERT ... SELECT and INSERT ... SELECT ... UNION ALL ... queries. Closes #22832. #23524 (Kseniia Sumarokova). Add support for progress indication in clickhouse-local with --progress option. #23196 (Egor Savin). Add support for HTTP compression (determined by Content-Encoding HTTP header) in http dictionary source. This fixes #8912. #23946 (FArthur-cmd). Added SYSTEM QUERY RELOAD MODEL, SYSTEM QUERY RELOAD MODELS. Closes #18722. #23182 (Maksim Kita). Add setting json (boolean, 0 by default) for EXPLAIN PLAN query. When enabled, query output will be a single JSON row. It is recommended to use TSVRaw format to avoid unnecessary escaping. #23082 (Nikolai Kochetov). Add setting indexes (boolean, disabled by default) to EXPLAIN PIPELINE query. When enabled, shows used indexes, number of filtered parts and granules for every index applied. Supported for MergeTree* tables. #22352 (Nikolai Kochetov). LDAP: implemented user DN detection functionality to use when mapping Active Directory groups to ClickHouse roles. #22228 (Denis Glazachev). New aggregate function deltaSumTimestamp for summing the difference between consecutive rows while maintaining ordering during merge by storing timestamps. #21888 (Russ Frank). Added less secure IMDS credentials provider for S3 which works under docker correctly. #21852 (Vladimir Chebotarev). Add back indexHint function. This is for #21238. This reverts #9542. This fixes #9540. #21304 (Amos Bird). Add PROJECTION support for MergeTree* tables. #20202 (Amos Bird). ClickHouse release v21.7, 2021-07-09 Improved performance of queries with explicitly defined large sets. Added compatibility setting legacy_column_name_of_tuple_literal. It makes sense to set it to true, while doing rolling update of cluster from version lower than 21.7 to any higher version. Otherwise distributed queries with explicitly defined sets at IN clause may fail during update. #25371 (Anton Popov). Forward/backward incompatible change of maximum buffer size in clickhouse-keeper (an experimental alternative to ZooKeeper). Better to do it now (before production), than later. #25421 (alesapin). Support configuration in YAML format as alternative to XML. This closes #3607. #21858 (BoloniniD). Provides a way to restore replicated table when the data is (possibly) present, but the ZooKeeper metadata is lost. Resolves #13458. #13652 (Mike Kot). Support structs and maps in Arrow/Parquet/ORC and dictionaries in Arrow input/output formats. Present new setting output_format_arrow_low_cardinality_as_dictionary. #24341 (Kruglov Pavel). Added support for Array type in dictionaries. #25119 (Maksim Kita). Added function bitPositionsToArray. Closes #23792. Author [Kevin Wan] (@MaxWk). #25394 (Maksim Kita). Added function dateName to return names like 'Friday' or 'April'. Author [Daniil Kondratyev] (@dankondr). #25372 (Maksim Kita). Add toJSONString function to serialize columns to their JSON representations. #25164 (Amos Bird). Now query_log has two new columns: initial_query_start_time, initial_query_start_time_microsecond that record the starting time of a distributed query if any. #25022 (Amos Bird). Add aggregate function segmentLengthSum. #24250 (flynn). Add a new boolean setting prefer_global_in_and_join which defaults all IN/JOIN as GLOBAL IN/JOIN. #23434 (Amos Bird). Support ALTER DELETE queries for Join table engine. #23260 (foolchi). Add quantileBFloat16 aggregate function as well as the corresponding quantilesBFloat16 and medianBFloat16. It is very simple and fast quantile estimator with relative error not more than 0.390625%. This closes #16641. #23204 (Ivan Novitskiy). Implement sequenceNextNode() function useful for flow analysis. #19766 (achimbab). ClickHouse release v21.8, 2021-08-12 Add support for a part of SQL/JSON standard. #24148 (l1tsolaiki, Kseniia Sumarokova). Collect common system metrics (in system.asynchronous_metrics and system.asynchronous_metric_log) on CPU usage, disk usage, memory usage, IO, network, files, load average, CPU frequencies, thermal sensors, EDAC counters, system uptime; also added metrics about the scheduling jitter and the time spent collecting the metrics. It works similar to atop in ClickHouse and allows access to monitoring data even if you have no additional tools installed. Close #9430. #24416 (alexey-milovidov, Yegor Levankov). Add MaterializedPostgreSQL table engine and database engine. This database engine allows replicating a whole database or any subset of database tables. #20470 (Kseniia Sumarokova). Add new functions leftPad(), rightPad(), leftPadUTF8(), rightPadUTF8(). #26075 (Vitaly Baranov). Add the FIRST keyword to the ADD INDEX command to be able to add the index at the beginning of the indices list. #25904 (xjewer). Introduce system.data_skipping_indices table containing information about existing data skipping indices. Close #7659. #25693 (Dmitry Novik). Add bin/unbin functions. #25609 (zhaoyu). Support Map and UInt128, Int128, UInt256, Int256 types in mapAdd and mapSubtract functions. #25596 (Ildus Kurbangaliev). Support DISTINCT ON (columns) expression, close #25404. #25589 (Zijie Lu). Add an ability to reset a custom setting to default and remove it from the table's metadata. It allows rolling back the change without knowing the system/config's default. Closes #14449. #17769 (xjewer). Render pipelines as graphs in Web UI if EXPLAIN PIPELINE graph = 1 query is submitted. #26067 (alexey-milovidov). ClickHouse release v21.9, 2021-09-09 Do not output trailing zeros in text representation of Decimal types. Example: 1.23 will be printed instead of 1.230000 for decimal with scale 6. This closes #15794. It may introduce slight incompatibility if your applications somehow relied on the trailing zeros. Serialization in output formats can be controlled with the setting output_format_decimal_trailing_zeros. Implementation of toString and casting to String is changed unconditionally. #27680 (alexey-milovidov). Do not allow to apply parametric aggregate function with -Merge combinator to aggregate function state if state was produced by aggregate function with different parameters. For example, state of fooState(42)(x) cannot be finalized with fooMerge(s) or fooMerge(123)(s), parameters must be specified explicitly like fooMerge(42)(s) and must be equal. It does not affect some special aggregate functions like quantile and sequence* that use parameters for finalization only. #26847 (tavplubix). Under clickhouse-local, always treat local addresses with a port as remote. #26736 (Raúl Marín). Fix the issue that in case of some sophisticated query with column aliases identical to the names of expressions, bad cast may happen. This fixes #25447. This fixes #26914. This fix may introduce backward incompatibility: if there are different expressions with identical names, exception will be thrown. It may break some rare cases when enable_optimize_predicate_expression is set. #26639 (alexey-milovidov). Now, scalar subquery always returns Nullable result if it's type can be Nullable. It is needed because in case of empty subquery it's result should be Null. Previously, it was possible to get error about incompatible types (type deduction does not execute scalar subquery, and it could use not-nullable type). Scalar subquery with empty result which can't be converted to Nullable (like Array or Tuple) now throws error. Fixes #25411. #26423 (Nikolai Kochetov). Introduce syntax for here documents. Example SELECT $doc$ VALUE $doc$. #26671 (Maksim Kita). This change is backward incompatible if in query there are identifiers that contain $ #28768. Now indices can handle Nullable types, including isNull and isNotNull. #12433 and #12455 (Amos Bird) and #27250 (Azat Khuzhin). But this was done with on-disk format changes, and even though new server can read old data, old server cannot. Also, in case you have MINMAX data skipping indices, you may get Data after mutation/merge is not byte-identical error, since new index will have .idx2 extension while before it was .idx. That said, that you should not delay updating all existing replicas, in this case, otherwise, if old replica ( Implementation of short circuit function evaluation, closes #12587. Add settings short_circuit_function_evaluation to configure short circuit function evaluation. #23367 (Kruglov Pavel). Add support for INTERSECT, EXCEPT, ANY, ALL operators. #24757 (Kirill Ershov). (Kseniia Sumarokova). Add support for encryption at the virtual file system level (data encryption at rest) using AES-CTR algorithm. #24206 (Latysheva Alexandra). (Vitaly Baranov) #26733 #26377 #26465. Added natural language processing (NLP) functions for tokenization, stemming, lemmatizing and search in synonyms extensions. #24997 (Nikolay Degterinsky). Added integration with S2 geometry library. #24980 (Andr0901). (Nikita Mikhaylov). Add SQLite table engine, table function, database engine. #24194 (Arslan Gumerov). (Kseniia Sumarokova). Added support for custom query for MySQL, PostgreSQL, ClickHouse, JDBC, Cassandra dictionary source. Closes #1270. #26995 (Maksim Kita). Add shared (replicated) storage of user, roles, row policies, quotas and settings profiles through ZooKeeper. #27426 (Kevin Michel). Add compression for INTO OUTFILE that automatically choose compression algorithm. Closes #3473. #27134 (Filatenkov Artur). Add INSERT ... FROM INFILE similarly to SELECT ... INTO OUTFILE. #27655 (Filatenkov Artur). Added complex_key_range_hashed dictionary. Closes #22029. #27629 (Maksim Kita). Support expressions in JOIN ON section. Close #21868. #24420 (Vladimir C). When client connects to server, it receives information about all warnings that are already were collected by server. (It can be disabled by using option --no-warnings). Add system.warnings table to collect warnings about server configuration. #26246 (Filatenkov Artur). #26282 (Filatenkov Artur). Allow using constant expressions from with and select in aggregate function parameters. Close #10945. #27531 (abel-cheng). Add tupleToNameValuePairs, a function that turns a named tuple into an array of pairs. #27505 (Braulio Valdivielso Martínez). Add support for bzip2 compression method for import/export. Closes #22428. #27377 (Nikolay Degterinsky). Added bitmapSubsetOffsetLimit(bitmap, offset, cardinality_limit) function. It creates a subset of bitmap limit the results to cardinality_limit with offset of offset. #27234 (DHBin). Add column default_database to system.users. #27054 (kevin wan). Supported cluster macros inside table functions 'cluster' and 'clusterAllReplicas'. #26913 (polyprogrammist). Add new functions currentRoles(), enabledRoles(), defaultRoles(). #26780 (Vitaly Baranov). New functions currentProfiles(), enabledProfiles(), defaultProfiles(). #26714 (Vitaly Baranov). Add functions that return (initial_)query_id of the current query. This closes #23682. #26410 (Alexey Boykov). Add REPLACE GRANT feature. #26384 (Caspian). EXPLAIN query now has EXPLAIN ESTIMATE ... mode that will show information about read rows, marks and parts from MergeTree tables. Closes #23941. #26131 (fastio). Added system.zookeeper_log table. All actions of ZooKeeper client are logged into this table. Implements #25449. #26129 (tavplubix). Zero-copy replication for ReplicatedMergeTree over HDFS storage. #25918 (Zhichang Yu). Allow to insert Nested type as array of structs in Arrow, ORC and Parquet input format. #25902 (Kruglov Pavel). Add a new datatype Date32 (store data as Int32), support date range same with DateTime64 support load parquet date32 to ClickHouse Date32 Add new function toDate32 like toDate. #25774 (LiuNeng). Allow setting default database for users. #25268. #25687 (kevin wan). Add an optional parameter to MongoDB engine to accept connection string options and support SSL connection. Closes #21189. Closes #21041. #22045 (Omar Bazaraa). ClickHouse release v21.10, 2021-10-16 Now the following MergeTree table-level settings: replicated_max_parallel_sends, replicated_max_parallel_sends_for_table, replicated_max_parallel_fetches, replicated_max_parallel_fetches_for_table do nothing. They never worked well and were replaced with max_replicated_fetches_network_bandwidth, max_replicated_sends_network_bandwidth and background_fetches_pool_size. #28404 (alesapin). Add feature for creating user-defined functions (UDF) as lambda expressions. Syntax CREATE FUNCTION {function_name} as ({parameters}) -> {function core}. Example CREATE FUNCTION plus_one as (a) -> a + 1. Authors @Realist007. #27796 (Maksim Kita) #23978 (Realist007). Added Executable storage engine and executable table function. It enables data processing with external scripts in streaming fashion. #28102 (Maksim Kita) (ruct). Added ExecutablePool storage engine. Similar to Executable but it's using a pool of long running processes. #28518 (Maksim Kita). Add ALTER TABLE ... MATERIALIZE COLUMN query. #27038 (Vladimir Chebotarev). Support for partitioned write into s3 table function. #23051 (Vladimir Chebotarev). Support lz4 compression format (in addition to gz, bz2, xz, zstd) for data import / export. #25310 (Bharat Nallan). Allow positional arguments under setting enable_positional_arguments. Closes #2592. #27530 (Kseniia Sumarokova). Accept user settings related to file formats in SETTINGS clause in CREATE query for s3 tables. This closes #27580. #28037 (Nikita Mikhaylov). Allow SSL connection for RabbitMQ engine. #28365 (Kseniia Sumarokova). Add getServerPort function to allow getting server port. When the port is not used by the server, throw an exception. #27900 (Amos Bird). Add conversion functions between \"snowflake id\" and DateTime, DateTime64. See #27058. #27704 (jasine). Add function SHA512. #27830 (zhanglistar). Add log_queries_probability setting that allows user to write to query_log only a sample of queries. Closes #16609. #27527 (Nikolay Degterinsky). ClickHouse release v21.11, 2021-11-09 Change order of json_path and json arguments in SQL/JSON functions (to be consistent with the standard). Closes #30449. #30474 (Kseniia Sumarokova). Remove MergeTree table setting write_final_mark. It will be always true. #30455 (Kseniia Sumarokova). No actions required, all tables are compatible with the new version. Function bayesAB is removed. Please help to return this function back, refreshed. This closes #26233. #29934 (alexey-milovidov). This is relevant only if you already started using the experimental clickhouse-keeper support. Now ClickHouse Keeper snapshots compressed with ZSTD codec by default instead of custom ClickHouse LZ4 block compression. This behavior can be turned off with compress_snapshots_with_zstd_format coordination setting (must be equal on all quorum replicas). Backward incompatibility is quite rare and may happen only when new node will send snapshot (happens in case of recovery) to the old node which is unable to read snapshots in ZSTD format. #29417 (alesapin). New asynchronous INSERT mode allows to accumulate inserted data and store it in a single batch in background. On client it can be enabled by setting async_insert for INSERT queries with data inlined in query or in separate buffer (e.g. for INSERT queries via HTTP protocol). If wait_for_async_insert is true (by default) the client will wait until data will be flushed to table. On server-side it controlled by the settings async_insert_threads, async_insert_max_data_size and async_insert_busy_timeout_ms. Implements #18282. #27537 (Anton Popov). #20557 (Ivan). Notes on performance: with asynchronous inserts you can do up to around 10 000 individual INSERT queries per second, so it is still recommended to insert in batches if you want to achieve performance up to millions inserted rows per second. Add interactive mode for clickhouse-local. So, you can just run clickhouse-local to get a command line ClickHouse interface without connecting to a server and process data from files and external data sources. Also merge the code of clickhouse-client and clickhouse-local together. Closes #7203. Closes #25516. Closes #22401. #26231 (Kseniia Sumarokova). Added support for executable (scriptable) user defined functions. These are UDFs that can be written in any programming language. #28803 (Maksim Kita). Allow predefined connections to external data sources. This allows to avoid specifying credentials or addresses while using external data sources, they can be referenced by names instead. Closes #28367. #28577 (Kseniia Sumarokova). Added INFORMATION_SCHEMA database with SCHEMATA, TABLES, VIEWS and COLUMNS views to the corresponding tables in system database. Closes #9770. #28691 (tavplubix). Support EXISTS (subquery). Closes #6852. #29731 (Kseniia Sumarokova). Session logging for audit. Logging all successful and failed login and logout events to a new system.session_log table. #22415 (Vasily Nemkov) (Vitaly Baranov). Support multidimensional cosine distance and euclidean distance functions; L1, L2, Lp, Linf distances and norms. Scalar product on tuples and various arithmetic operators on tuples. This fully closes #4509 and even more. #27933 (Alexey Boykov). Add support for compression and decompression for INTO OUTFILE and FROM INFILE (with autodetect or with additional optional parameter). #27135 (Filatenkov Artur). Add CORS (Cross Origin Resource Sharing) support with HTTP OPTIONS request. It means, now Grafana will work with serverless requests without a kludges. Closes #18693. #29155 (Filatenkov Artur). Queries with JOIN ON now supports disjunctions (OR). #21320 (Ilya Golshtein). Added function tokens. That allow to split string into tokens using non-alpha numeric ASCII characters as separators. #29981 (Maksim Kita). Added function ngrams to extract ngrams from text. Closes #29699. #29738 (Maksim Kita). Add functions for Unicode normalization: normalizeUTF8NFC, normalizeUTF8NFD, normalizeUTF8NFKC, normalizeUTF8NFKD functions. #28633 (darkkeks). Streaming consumption of application log files in ClickHouse with FileLog table engine. It's like Kafka or RabbitMQ engine but for append-only and rotated logs in local filesystem. Closes #6953. #25969 (flynn) (Kseniia Sumarokova). Add CapnProto output format, refactor CapnProto input format. #29291 (Kruglov Pavel). Allow to write number in query as binary literal. Example SELECT 0b001;. #29304 (Maksim Kita). Added hashed_array dictionary type. It saves memory when using dictionaries with multiple attributes. Closes #30236. #30242 (Maksim Kita). Added JSONExtractKeys function. #30056 (Vitaly). Add a function getOSKernelVersion - it returns a string with OS kernel version. #29755 (Memo). Added MD4 and SHA384 functions. MD4 is an obsolete and insecure hash function, it can be used only in rare cases when MD4 is already being used in some legacy system and you need to get exactly the same result. #29602 (Nikita Tikhomirov). HSTS can be enabled for ClickHouse HTTP server by setting hsts_max_age in configuration file with a positive number. #29516 (凌涛). Huawei OBS Storage support. Closes #24294. #29511 (kevin wan). New function mapContainsKeyLike to get the map that key matches a simple regular expression. #29471 (凌涛). New function mapExtractKeyLike to get the map only kept elements matched specified pattern. #30793 (凌涛). Implemented ALTER TABLE x MODIFY COMMENT. #29264 (Vasily Nemkov). Adds H3 inspection functions that are missing from ClickHouse but are available via the H3 api: https://h3geo.org/docs/api/inspection. #29209 (Bharat Nallan). Allow non-replicated ALTER TABLE FETCH and ATTACH in Replicated databases. #29202 (Kevin Michel). Added a setting output_format_csv_null_representation: This is the same as output_format_tsv_null_representation but is for CSV output. #29123 (PHO). Added function zookeeperSessionUptime() which returns uptime of current ZooKeeper session in seconds. #28983 (tavplubix). Implements the h3ToGeoBoundary function. #28952 (Ivan Veselov). Add aggregate function exponentialMovingAverage that can be used as window function. This closes #27511. #28914 (alexey-milovidov). Allow to include subcolumns of table columns into DESCRIBE query result (can be enabled by setting describe_include_subcolumns). #28905 (Anton Popov). Executable, ExecutablePool added option send_chunk_header. If this option is true then chunk rows_count with line break will be sent to client before chunk. #28833 (Maksim Kita). tokenbf_v1 and ngram support Map with key of String of FixedSring type. It enhance data skipping in query with map key filter. sql CREATE TABLE map_tokenbf ( row_id UInt32, map Map(String, String), INDEX map_tokenbf map TYPE ngrambf_v1(4,256,2,0) GRANULARITY 1 ) Engine=MergeTree() Order by id With table above, the query select * from map_tokebf where map['K']='V' will skip the granule that doesn't contain key A . Of course, how many rows will skipped is depended on the granularity and index_granularity you set. #28511 (凌涛). Send profile events from server to client. New packet type ProfileEvents was introduced. Closes #26177. #28364 (Dmitry Novik). Bit shift operations for FixedString and String data types. This closes #27763. #28325 (小路). Support adding / deleting tables to replication from PostgreSQL dynamically in database engine MaterializedPostgreSQL. Support alter for database settings. Closes #27573. #28301 (Kseniia Sumarokova). Added function accurateCastOrDefault(x, T). Closes #21330. Authors @taiyang-li. #23028 (Maksim Kita). Add Function toUUIDOrDefault, toUInt8/16/32/64/256OrDefault, toInt8/16/32/64/128/256OrDefault, which enables user defining default value(not null) when string parsing is failed. #21330 (taiyang-li). ClickHouse release v21.12, 2021-12-15 A fix for a feature that previously had unwanted behaviour. Do not allow direct select for Kafka/RabbitMQ/FileLog. Can be enabled by setting stream_like_engine_allow_direct_select. Direct select will be not allowed even if enabled by setting, in case there is an attached materialized view. For Kafka and RabbitMQ direct selectm if allowed, will not commit massages by default. To enable commits with direct select, user must use storage level setting kafka{rabbitmq}_commit_on_select=1 (default 0). #31053 (Kseniia Sumarokova). A slight change in behaviour of a new function. Return unquoted string in JSON_VALUE. Closes #27965. #31008 (Kseniia Sumarokova). Setting rename. Add custom null representation support for TSV/CSV input formats. Fix deserialing Nullable(String) in TSV/CSV/JSONCompactStringsEachRow/JSONStringsEachRow input formats. Rename output_format_csv_null_representation and output_format_tsv_null_representation to format_csv_null_representation and format_tsv_null_representation accordingly. #30497 (Kruglov Pavel). Further deprecation of already unused code. This is relevant only for users of ClickHouse versions older than 20.6. A \"leader election\" mechanism is removed from ReplicatedMergeTree, because multiple leaders are supported since 20.6. If you are upgrading from an older version and some replica with an old version is a leader, then server will fail to start after upgrade. Stop replicas with old version to make new version start. After that it will not be possible to downgrade to version older than 20.6. #32140 (tavplubix). Implemented more of the ZooKeeper Four Letter Words commands in clickhouse-keeper: https://zookeeper.apache.org/doc/r3.4.8/zookeeperAdmin.html#sc_zkCommands. #28981 (JackyWoo). Now clickhouse-keeper is feature complete. Support for Bool data type. #31072 (kevin wan). Support for PARTITION BY in File, URL, HDFS storages and with INSERT INTO table function. Closes #30273. #30690 (Kseniia Sumarokova). Added CONSTRAINT ... ASSUME ... (without checking during INSERT). Added query transformation to CNF (https://github.com/ClickHouse/ClickHouse/issues/11749) for more convenient optimization. Added simple query rewriting using constraints (only simple matching now, will be improved to support ... later). Added ability to replace heavy columns with light columns if it's possible. #18787 (Nikita Vasilev). Basic access authentication for http/url functions. #31648 (michael1589). Support INTERVAL type in STEP clause for WITH FILL modifier. #30927 (Anton Popov). Add support for parallel reading from multiple files and support globs in FROM INFILE clause. #30135 (Filatenkov Artur). Add support for Identifier table and database query parameters. Closes #27226. #28668 (Nikolay Degterinsky). TLDR: Major improvements of completeness and consistency of text formats. Refactor formats TSV, TSVRaw, CSV and JSONCompactEachRow, JSONCompactStringsEachRow, remove code duplication, add base interface for formats with -WithNames and -WithNamesAndTypes suffixes. Add formats CSVWithNamesAndTypes, TSVRawWithNames, TSVRawWithNamesAndTypes, JSONCompactEachRowWIthNames, JSONCompactStringsEachRowWIthNames, RowBinaryWithNames. Support parallel parsing for formats TSVWithNamesAndTypes, TSVRaw(WithNames/WIthNamesAndTypes), CSVWithNamesAndTypes, JSONCompactEachRow(WithNames/WIthNamesAndTypes), JSONCompactStringsEachRow(WithNames/WIthNamesAndTypes). Support columns mapping and types checking for RowBinaryWithNamesAndTypes format. Add setting input_format_with_types_use_header which specify if we should check that types written in WIthNamesAndTypes format matches with table structure. Add setting input_format_csv_empty_as_default and use it in CSV format instead of input_format_defaults_for_omitted_fields (because this setting should not control csv_empty_as_default). Fix usage of setting input_format_defaults_for_omitted_fields (it was used only as csv_empty_as_default, but it should control calculation of default expressions for omitted fields). Fix Nullable input/output in TSVRaw format, make this format fully compatible with inserting into TSV. Fix inserting NULLs in LowCardinality(Nullable) when input_format_null_as_default is enabled (previously default values was inserted instead of actual NULLs). Fix strings deserialization in JSONStringsEachRow/JSONCompactStringsEachRow formats (strings were parsed just until first '\\n' or '\\t'). Add ability to use Raw escaping rule in Template input format. Add diagnostic info for JSONCompactEachRow(WithNames/WIthNamesAndTypes) input format. Fix bug with parallel parsing of -WithNames formats in case when setting min_chunk_bytes_for_parallel_parsing is less than bytes in a single row. #30178 (Kruglov Pavel). Allow to print/parse names and types of colums in CustomSeparated input/output format. Add formats CustomSeparatedWithNames/WithNamesAndTypes similar to TSVWithNames/WithNamesAndTypes. #31434 (Kruglov Pavel). Aliyun OSS Storage support. #31286 (cfcz48). Exposes all settings of the global thread pool in the configuration file. #31285 (Tomáš Hromada). Introduced window functions exponentialTimeDecayedSum, exponentialTimeDecayedMax, exponentialTimeDecayedCount and exponentialTimeDecayedAvg which are more effective than exponentialMovingAverage for bigger windows. Also more use-cases were covered. #29799 (Vladimir Chebotarev). Add option to compress logs before writing them to a file using LZ4. Closes #23860. #29219 (Nikolay Degterinsky). Support JOIN ON 1 = 1 that have CROSS JOIN semantic. This closes #25578. #25894 (Vladimir C). Add Map combinator for Map type. - Rename old sum-, min-, max- Map for mapped arrays to sum-, min-, max- MappedArrays. #24539 (Ildus Kurbangaliev). Make reading from HTTP retriable. Closes #29696. #29894 (Kseniia Sumarokova). ClickHouse Release 22.* ClickHouse release v22.1, 2022-01-18 Implement data schema inference for input formats. Allow to skip structure (or write just auto) in table functions file, url, s3, hdfs and in parameters of clickhouse-local . Allow to skip structure in create query for table engines File, HDFS, S3, URL, Merge, Buffer, Distributed and ReplicatedMergeTree (if we add new replicas). #32455 (Kruglov Pavel). Detect format by file extension in file/hdfs/s3/url table functions and HDFS/S3/URL table engines and also for SELECT INTO OUTFILE and INSERT FROM INFILE #33565 (Kruglov Pavel). Close #30918. #33443 (OnePiece). A tool for collecting diagnostics data if you need support. #33175 (Alexander Burmak). Automatic cluster discovery via Zoo/Keeper. It allows to add replicas to the cluster without changing configuration on every server. #31442 (vdimir). Implement hive table engine to access apache hive from clickhouse. This implements: #29245. #31104 (taiyang-li). Add aggregate functions cramersV, cramersVBiasCorrected, theilsU and contingency. These functions calculate dependency (measure of association) between categorical values. All these functions are using cross-tab (histogram on pairs) for implementation. You can imagine it like a correlation coefficient but for any discrete values (not necessary numbers). #33366 (alexey-milovidov). Initial implementation by Vanyok-All-is-OK and antikvist. Added table function hdfsCluster which allows processing files from HDFS in parallel from many nodes in a specified cluster, similarly to s3Cluster. #32400 (Zhichang Yu). Adding support for disks backed by Azure Blob Storage, in a similar way it has been done for disks backed by AWS S3. #31505 (Jakub Kuklis). Allow COMMENT in CREATE VIEW (for all VIEW kinds). #31062 (Vasily Nemkov). Dynamically reinitialize listening ports and protocols when configuration changes. #30549 (Kevin Michel). Added left, right, leftUTF8, rightUTF8 functions. Fix error in implementation of substringUTF8 function with negative offset (offset from the end of string). #33407 (alexey-milovidov). Add new functions for H3 coordinate system: h3HexAreaKm2, h3CellAreaM2, h3CellAreaRads2. #33479 (Bharat Nallan). Add MONTHNAME function. #33436 (usurai). Added function arrayLast. Closes #33390. #33415 Added function arrayLastIndex. #33465 (Maksim Kita). Add function decodeURLFormComponent slightly different to decodeURLComponent. Close #10298. #33451 (SuperDJY). Allow to split GraphiteMergeTree rollup rules for plain/tagged metrics (optional rule_type field). #33494 (Michail Safronov). ClickHouse release v22.2, 2022-02-17 Projections are production ready. Set allow_experimental_projection_optimization by default and deprecate this setting. #34456 (Nikolai Kochetov). An option to create a new files on insert for File/S3/HDFS engines. Allow to overwrite a file in HDFS. Throw an exception in attempt to overwrite a file in S3 by default. Throw an exception in attempt to append data to file in formats that have a suffix (and thus don't support appends, like Parquet, ORC). Closes #31640 Closes #31622 Closes #23862 Closes #15022 Closes #16674. #33302 (Kruglov Pavel). Add a setting that allows a user to provide own deduplication semantic in MergeTree/ReplicatedMergeTree If provided, it's used instead of data digest to generate block ID. So, for example, by providing a unique value for the setting in each INSERT statement, the user can avoid the same inserted data being deduplicated. This closes: #7461. #32304 (Igor Nikonov). Add support of DEFAULT keyword for INSERT statements. Closes #6331. #33141 (Andrii Buriachevskyi). EPHEMERAL column specifier is added to CREATE TABLE query. Closes #9436. #34424 (yakov-olkhovskiy). Support IF EXISTS clause for TTL expr TO [DISK|VOLUME] [IF EXISTS] 'xxx' feature. Parts will be moved to disk or volume only if it exists on replica, so MOVE TTL rules will be able to behave differently on replicas according to the existing storage policies. Resolves #34455. #34504 (Anton Popov). Allow set default table engine and to create tables without specifying ENGINE. #34187 (Ilya Yatsishin). Add table function format(format_name, data). #34125 (Kruglov Pavel). Detect format in clickhouse-local by file name even in the case when it is passed to stdin. #33829 (Kruglov Pavel). Add schema inference for values table function. Closes #33811. #34017 (Kruglov Pavel). Dynamic reload of server TLS certificates on config reload. Closes #15764. #15765 (johnskopis). #31257 (Filatenkov Artur). Now ReplicatedMergeTree can recover data when some of its disks are broken. #13544 (Amos Bird). Fault-tolerant connections in clickhouse-client: clickhouse-client ... --host host1 --host host2 --port port2 --host host3 --port port --host host4. #34490 (Kruglov Pavel). #33824 (Filippov Denis). Add DEGREES and RADIANS functions for MySQL compatibility. #33769 (Bharat Nallan). Add h3ToCenterChild function. #33313 (Bharat Nallan). Add new h3 miscellaneous functions: edgeLengthKm,exactEdgeLengthKm,exactEdgeLengthM,exactEdgeLengthRads,numHexagons. #33621 (Bharat Nallan). Add function bitSlice to extract bit subsequences from String/FixedString. #33360 (RogerYK). Implemented meanZTest aggregate function. #33354 (achimbab). Add confidence intervals to T-tests aggregate functions. #33260 (achimbab). Add function addressToLineWithInlines. Close #26211. #33467 (SuperDJY). Added #! and # as a recognised start of a single line comment. Closes #34138. #34230 (Aaron Katz). ClickHouse release v22.3-lts, 2022-03-17 Make arrayCompact function behave as other higher-order functions: perform compaction not of lambda function results but on the original array. If you're using nontrivial lambda functions in arrayCompact you may restore old behaviour by wrapping arrayCompact arguments into arrayMap. Closes #34010 #18535 #14778. #34795 (Alexandre Snarskii). Change implementation specific behavior on overflow of function toDatetime. It will be saturated to the nearest min/max supported instant of datetime instead of wraparound. This change is highlighted as \"backward incompatible\" because someone may unintentionally rely on the old behavior. #32898 (HaiBo Li). Make function cast(value, 'IPv4'), cast(value, 'IPv6') behave same as toIPv4, toIPv6 functions. Changed behavior of incorrect IP address passed into functions toIPv4, toIPv6, now if invalid IP address passes into this functions exception will be raised, before this function return default value. Added functions IPv4StringToNumOrDefault, IPv4StringToNumOrNull, IPv6StringToNumOrDefault, IPv6StringOrNull toIPv4OrDefault, toIPv4OrNull, toIPv6OrDefault, toIPv6OrNull. Functions IPv4StringToNumOrDefault , toIPv4OrDefault , toIPv6OrDefault should be used if previous logic relied on IPv4StringToNum, toIPv4, toIPv6 returning default value for invalid address. Added setting cast_ipv4_ipv6_default_on_conversion_error, if this setting enabled, then IP address conversion functions will behave as before. Closes #22825. Closes #5799. Closes #35156. #35240 (Maksim Kita). Support for caching data locally for remote filesystems. It can be enabled for s3 disks. Closes #28961. #33717 (Kseniia Sumarokova). In the meantime, we enabled the test suite on s3 filesystem and no more known issues exist, so it is started to be production ready. Add new table function hive. It can be used as follows hive('', '', '', '', '') for example SELECT * FROM hive('thrift://hivetest:9083', 'test', 'demo', 'id Nullable(String), score Nullable(Int32), day Nullable(String)', 'day'). #34946 (lgbo). Support authentication of users connected via SSL by their X.509 certificate. #31484 (eungenue). Support schema inference for inserting into table functions file/hdfs/s3/url. #34732 (Kruglov Pavel). Now you can read system.zookeeper table without restrictions on path or using like expression. This reads can generate quite heavy load for zookeeper so to enable this ability you have to enable setting allow_unrestricted_reads_from_keeper. #34609 (Sergei Trifonov). Display CPU and memory metrics in clickhouse-local. Close #34545. #34605 (李扬). Implement startsWith and endsWith function for arrays, closes #33982. #34368 (usurai). Add three functions for Map data type: 1. mapReplace(map1, map2) - replaces values for keys in map1 with the values of the corresponding keys in map2; adds keys from map2 that don't exist in map1. 2. mapFilter 3. mapMap. mapFilter and mapMap are higher order functions, accepting two arguments, the first argument is a lambda function with k, v pair as arguments, the second argument is a column of type Map. #33698 (hexiaoting). Allow getting default user and password for clickhouse-client from the CLICKHOUSE_USER and CLICKHOUSE_PASSWORD environment variables. Close #34538. #34947 (DR). ClickHouse release 22.4, 2022-04-19 Do not allow SETTINGS after FORMAT for INSERT queries (there is compatibility setting parser_settings_after_format_compact to accept such queries, but it is turned OFF by default). #35883 (Azat Khuzhin). Function yandexConsistentHash (consistent hashing algorithm by Konstantin \"kostik\" Oblakov) is renamed to kostikConsistentHash. The old name is left as an alias for compatibility. Although this change is backward compatible, we may remove the alias in subsequent releases, that's why it's recommended to update the usages of this function in your apps. #35553 (Alexey Milovidov). Added INTERPOLATE extension to the ORDER BY ... WITH FILL. Closes #34903. #35349 (Yakov Olkhovskiy). Profiling on Processors level (under log_processors_profiles setting, ClickHouse will write time that processor spent during execution/waiting for data to system.processors_profile_log table). #34355 (Azat Khuzhin). Added functions makeDate(year, month, day), makeDate32(year, month, day). #35628 (Alexander Gololobov). Implementation of makeDateTime() and makeDateTIme64(). #35934 (Alexander Gololobov). Support new type of quota WRITTEN BYTES to limit amount of written bytes during insert queries. #35736 (Anton Popov). Added function flattenTuple. It receives nested named Tuple as an argument and returns a flatten Tuple which elements are the paths from the original Tuple. E.g.: Tuple(a Int, Tuple(b Int, c Int)) -> Tuple(a Int, b Int, c Int). flattenTuple can be used to select all paths from type Object as separate columns. #35690 (Anton Popov). Added functions arrayFirstOrNull, arrayLastOrNull. Closes #35238. #35414 (Maksim Kita). Added functions minSampleSizeContinous and minSampleSizeConversion. Author achimbab. #35360 (Maksim Kita). New functions minSampleSizeContinous and minSampleSizeConversion. #34354 (achimbab). Introduce format ProtobufList (all records as repeated messages in out Protobuf). Closes #16436. #35152 (Nikolai Kochetov). Add h3PointDistM, h3PointDistKm, h3PointDistRads, h3GetRes0Indexes, h3GetPentagonIndexes functions. #34568 (Bharat Nallan). Add toLastDayOfMonth function which rounds up a date or date with time to the last day of the month. #33501. #34394 (Habibullah Oladepo). Added load balancing setting for [Zoo]Keeper client. Closes #29617. #30325 (小路). Add a new kind of row policies named simple. Before this PR we had two kinds or row policies: permissive and restrictive. A simple row policy adds a new filter on a table without any side-effects like it was for permissive and restrictive policies. #35345 (Vitaly Baranov). Added an ability to specify cluster secret in replicated database. #35333 (Nikita Mikhaylov). Added sanity checks on server startup (available memory and disk space, max thread count, etc). #34566 (Sergei Trifonov). INTERVAL improvement - can be used with [MILLI|MICRO|NANO]SECOND. Added toStartOf[Milli|Micro|Nano]second() functions. Added [add|subtract][Milli|Micro|Nano]seconds(). #34353 (Andrey Zvonov). ClickHouse release 22.5, 2022-05-19 Enable memory overcommit by default. #35921 (Dmitry Novik). Add support of GROUPING SETS in GROUP BY clause. This implementation supports a parallel processing of grouping sets. #33631 (Dmitry Novik). Added system.certificates table. #37142 (Yakov Olkhovskiy). Adds h3Line, h3Distance and h3HexRing functions. #37030 (Bharat Nallan). New single binary based diagnostics tool (clickhouse-diagnostics). #36705 (Dale McDiarmid). Add output format Prometheus #36051. #36206 (Vladimir C). Add MySQLDump input format. It reads all data from INSERT queries belonging to one table in dump. If there are more than one table, by default it reads data from the first one. #36667 (Kruglov Pavel). Show the total_rows and total_bytes fields in system.tables for temporary tables. #36401. #36439 (xiedeyantu). Allow to override parts_to_delay_insert and parts_to_throw_insert with query-level settings. If they are defined, they will override table-level settings. #36371 (Memo). ClickHouse release 22.6, 2022-06-16 Remove support for octal number literals in SQL. In previous versions they were parsed as Float64. #37765 (Yakov Olkhovskiy). Changes how settings using seconds as type are parsed to support floating point values (for example: max_execution_time=0.5). Infinity or NaN values will throw an exception. #37187 (Raúl Marín). Changed format of binary serialization of columns of experimental type Object. New format is more convenient to implement by third-party clients. #37482 (Anton Popov). Turn on setting output_format_json_named_tuples_as_objects by default. It allows to serialize named tuples as JSON objects in JSON formats. #37756 (Anton Popov). LIKE patterns with trailing escape symbol ('\\') are now disallowed (as mandated by the SQL standard). #37764 (Robert Schulze). If you run different ClickHouse versions on a cluster with AArch64 CPU or mix AArch64 and amd64 on a cluster, and use distributed queries with GROUP BY multiple keys of fixed-size type that fit in 256 bits but don't fit in 64 bits, and the size of the result is huge, the data will not be fully aggregated in the result of these queries during upgrade. Workaround: upgrade with downtime instead of a rolling upgrade. Add GROUPING function. It allows to disambiguate the records in the queries with ROLLUP, CUBE or GROUPING SETS. Closes #19426. #37163 (Dmitry Novik). A new codec FPC algorithm for floating point data compression. #37553 (Mikhail Guzov). Add new columnar JSON formats: JSONColumns, JSONCompactColumns, JSONColumnsWithMetadata. Closes #36338 Closes #34509. #36975 (Kruglov Pavel). Added open telemetry traces visualizing tool based on d3js. #37810 (Sergei Trifonov). Support INSERTs into system.zookeeper table. Closes #22130. #37596 (Han Fei). Support non-constant pattern argument for LIKE, ILIKE and match functions. #37251 (Robert Schulze). Executable user defined functions now support parameters. Example: SELECT test_function(parameters)(arguments). Closes #37578. #37720 (Maksim Kita). Add merge_reason column to system.part_log table. #36912 (Sema Checherinda). Add support for Maps and Records in Avro format. Add new setting input_format_avro_null_as_default that allow to insert null as default in Avro format. Closes #18925 Closes #37378 Closes #32899. #37525 (Kruglov Pavel). Add clickhouse-disks tool to introspect and operate on virtual filesystems configured for ClickHouse. #36060 (Artyom Yurkov). Adds H3 unidirectional edge functions. #36843 (Bharat Nallan). Add support for calculating hashids from unsigned integers. #37013 (Michael Nutt). Explicit SALT specification is allowed for CREATE USER IDENTIFIED WITH sha256_hash. #37377 (Yakov Olkhovskiy). Add two new settings input_format_csv_skip_first_lines/input_format_tsv_skip_first_lines to allow skipping specified number of lines in the beginning of the file in CSV/TSV formats. #37537 (Kruglov Pavel). showCertificate function shows current server's SSL certificate. #37540 (Yakov Olkhovskiy). HTTP source for Data Dictionaries in Named Collections is supported. #37581 (Yakov Olkhovskiy). Added a new window function nonNegativeDerivative(metric_column, timestamp_column[, INTERVAL x SECOND]). #37628 (Andrey Zvonov). Implemented changing the comment for ReplicatedMergeTree tables. #37416 (Vasily Nemkov). Added SYSTEM UNFREEZE query that deletes the whole backup regardless if the corresponding table is deleted or not. #36424 (Vadim Volodin). ClickHouse release 22.7, 2022-07-21 Support expressions with window functions. Closes #19857. #37848 (Dmitry Novik). Add new direct join algorithm for EmbeddedRocksDB tables, see #33582. #35363 (Vladimir C). Added full sorting merge join algorithm. #35796 (Vladimir C). Implement NATS table engine, which allows to pub/sub to NATS. Closes #32388. #37171 (tchepavel). (Kseniia Sumarokova) Implement table function mongodb. Allow writes into MongoDB storage / table function. #37213 (aaapetrenko). (Kseniia Sumarokova) Add SQLInsert output format. Closes #38441. #38477 (Kruglov Pavel). Introduced settings additional_table_filters. Using this setting, you can specify additional filtering condition for a table which will be applied directly after reading. Example: select number, x, y from (select number from system.numbers limit 5) f any left join (select x, y from table_1) s on f.number = s.x settings additional_table_filters={'system.numbers : 'number != 3', 'table_1' : 'x != 2'}. Introduced setting additional_result_filter which specifies additional filtering condition for query result. Closes #37918. #38475 (Nikolai Kochetov). Add compatibility setting and system.settings_changes system table that contains information about changes in settings through ClickHouse versions. Closes #35972. #38957 (Kruglov Pavel). Add functions translate(string, from_string, to_string) and translateUTF8(string, from_string, to_string). It translates some characters to another. #38935 (Nikolay Degterinsky). Support parseTimeDelta function. It can be used like ;-+,: can be used as separators, eg. 1yr-2mo, 2m:6s: SELECT parseTimeDelta('1yr-2mo-4w + 12 days, 3 hours : 1 minute ; 33 seconds'). #39071 (jiahui-97). Added CREATE TABLE ... EMPTY AS SELECT query. It automatically deduces table structure from the SELECT query, but does not fill the table after creation. Resolves #38049. #38272 (Alexander Tokmakov). Added options to limit IO operations with remote storage: max_remote_read_network_bandwidth_for_server and max_remote_write_network_bandwidth_for_server. #39095 (Sergei Trifonov). Add group_by_use_nulls setting to make aggregation key columns nullable in the case of ROLLUP, CUBE and GROUPING SETS. Closes #37359. #38642 (Dmitry Novik). Add the ability to specify compression level during data export. #38907 (Nikolay Degterinsky). Add an option to require explicit grants to SELECT from the system database. Details: #38970 (Vitaly Baranov). Functions multiMatchAny, multiMatchAnyIndex, multiMatchAllIndices and their fuzzy variants now accept non-const pattern array argument. #38485 (Robert Schulze). SQL function multiSearchAllPositions now accepts non-const needle arguments. #39167 (Robert Schulze). Add a setting zstd_window_log_max to configure max memory usage on zstd decoding when importing external files. Closes #35693. #37015 (wuxiaobai24). Add send_logs_source_regexp setting. Send server text logs with specified regexp to match log source name. Empty means all sources. #39161 (Amos Bird). Support ALTER for Hive tables. #38214 (lgbo). Support isNullable function. This function checks whether it's argument is nullable and return 1 or 0. Closes #38611. #38841 (lokax). Added functions for base58 encoding/decoding. #38159 (Andrey Zvonov). Add chart visualization to Play UI. #38197 (Alexey Milovidov). Added L2 Squared distance and norm functions for both arrays and tuples. #38545 (Julian Gilyadov). Add ability to pass HTTP headers to the url table function / storage via SQL. Closes #37897. #38176 (Kseniia Sumarokova). Add clickhouse-diagnostics binary to the packages. #38647 (Mikhail f. Shiryaev). ClickHouse release 22.8, 2022-08-18 Extended range of Date32 and DateTime64 to support dates from the year 1900 to 2299. In previous versions, the supported interval was only from the year 1925 to 2283. The implementation is using the proleptic Gregorian calendar (which is conformant with ISO 8601:2004 (clause 3.2.1 The Gregorian calendar)) instead of accounting for historical transitions from the Julian to the Gregorian calendar. This change affects implementation-specific behavior for out-of-range arguments. E.g. if in previous versions the value of 1899-01-01 was clamped to 1925-01-01, in the new version it will be clamped to 1900-01-01. It changes the behavior of rounding with toStartOfInterval if you pass INTERVAL 3 QUARTER up to one quarter because the intervals are counted from an implementation-specific point of time. Closes #28216, improves #38393. #39425 (Roman Vasin). Now, all relevant dictionary sources respect remote_url_allow_hosts setting. It was already done for HTTP, Cassandra, Redis. Added ClickHouse, MongoDB, MySQL, PostgreSQL. Host is checked only for dictionaries created from DDL. #39184 (Nikolai Kochetov). Prebuilt ClickHouse x86 binaries now require support for AVX instructions, i.e. a CPU not older than Intel Sandy Bridge / AMD Bulldozer, both released in 2011. #39000 (Robert Schulze). Make the remote filesystem cache composable, allow not to evict certain files (regarding idx, mrk, ..), delete old cache version. Now it is possible to configure cache over Azure blob storage disk, over Local disk, over StaticWeb disk, etc. This PR is marked backward incompatible because cache configuration changes and in order for cache to work need to update the config file. Old cache will still be used with new configuration. The server will startup fine with the old cache configuration. Closes https://github.com/ClickHouse/ClickHouse/issues/36140. Closes https://github.com/ClickHouse/ClickHouse/issues/37889. (Kseniia Sumarokova). #36171) Support SQL standard DELETE FROM syntax on merge tree tables and lightweight delete implementation for merge tree families. #37893 (Jianmei Zhang) (Alexander Gololobov). Note: this new feature does not make ClickHouse an HTAP DBMS. Query parameters can be set in interactive mode as SET param_abc = 'def' and transferred via the native protocol as settings. #39906 (Nikita Taranov). Quota key can be set in the native protocol (Yakov Olkhovsky). Added a setting exact_rows_before_limit (0/1). When enabled, ClickHouse will provide exact value for rows_before_limit_at_least statistic, but with the cost that the data before limit will have to be read completely. This closes #6613. #25333 (kevin wan). Added support for parallel distributed insert select with s3Cluster table function into tables with Distributed and Replicated engine #34670. #39107 (Nikita Mikhaylov). Add new settings to control schema inference from text formats: - input_format_try_infer_dates - try infer dates from strings. - input_format_try_infer_datetimes - try infer datetimes from strings. - input_format_try_infer_integers - try infer Int64 instead of Float64. - input_format_json_try_infer_numbers_from_strings - try infer numbers from json strings in JSON formats. #39186 (Kruglov Pavel). An option to provide JSON formatted log output. The purpose is to allow easier ingestion and query in log analysis tools. #39277 (Mallik Hassan). Add function nowInBlock which allows getting the current time during long-running and continuous queries. Closes #39522. Notes: there are no functions now64InBlock neither todayInBlock. #39533 (Alexey Milovidov). Add ability to specify settings for an executable() table function. #39681 (Constantine Peresypkin). Implemented automatic conversion of database engine from Ordinary to Atomic. Create empty convert_ordinary_to_atomic file in flags directory and all Ordinary databases will be converted automatically on next server start. Resolves #39546. #39933 (Alexander Tokmakov). Support SELECT ... INTO OUTFILE '...' AND STDOUT. #37490. #39054 (SmitaRKulkarni). Add formats PrettyMonoBlock, PrettyNoEscapesMonoBlock, PrettyCompactNoEscapes, PrettyCompactNoEscapesMonoBlock, PrettySpaceNoEscapes, PrettySpaceMonoBlock, PrettySpaceNoEscapesMonoBlock. #39646 (Kruglov Pavel). ClickHouse release 22.9, 2022-09-22 Upgrade from 20.3 and older to 22.9 and newer should be done through an intermediate version if there are any ReplicatedMergeTree tables, otherwise server with the new version will not start. #40641 (Alexander Tokmakov). Remove the functions accurate_Cast and accurate_CastOrNull (they are different to accurateCast and accurateCastOrNull by underscore in the name and they are not affected by the value of cast_keep_nullable setting). These functions were undocumented, untested, unused, and unneeded. They appeared to be alive due to code generalization. #40682 (Alexey Milovidov). Add a test to ensure that every new table function will be documented. See #40649. Rename table function MeiliSearch to meilisearch. #40709 (Alexey Milovidov). Add a test to ensure that every new function will be documented. See #40649. The functions lemmatize, synonyms, stem were case-insensitive by mistake. Now they are case-sensitive. #40711 (Alexey Milovidov). For security and stability reasons, catboost models are no longer evaluated within the ClickHouse server. Instead, the evaluation is now done in the clickhouse-library-bridge, a separate process that loads the catboost library and communicates with the server process via HTTP. #40897 (Robert Schulze). Make interpretation of YAML configs to be more conventional. #41044 (Vitaly Baranov). Support insert_quorum = 'auto' to use majority number. #39970 (Sachin). Add embedded dashboards to ClickHouse server. This is a demo project about how to achieve 90% results with 1% effort using ClickHouse features. #40461 (Alexey Milovidov). Added new settings constraint writability kind changeable_in_readonly. #40631 (Sergei Trifonov). Add support for INTERSECT DISTINCT and EXCEPT DISTINCT. #40792 (Duc Canh Le). Add new input/output format JSONObjectEachRow - Support import for formats JSON/JSONCompact/JSONColumnsWithMetadata. Add new setting input_format_json_validate_types_from_metadata that controls whether we should check if data types from metadata match data types from the header. - Add new setting input_format_json_validate_utf8, when it's enabled, all JSON formats will validate UTF-8 sequences. It will be disabled by default. Note that this setting doesn't influence output formats JSON/JSONCompact/JSONColumnsWithMetadata, they always validate utf8 sequences (this exception was made because of compatibility reasons). - Add new setting input_format_json_read_numbers_as_strings that allows to parse numbers in String column, the setting is disabled by default. - Add new setting output_format_json_quote_decimals that allows to output decimals in double quotes, disabled by default. - Allow to parse decimals in double quotes during data import. #40910 (Kruglov Pavel). Query parameters supported in DESCRIBE TABLE query. #40952 (Nikita Taranov). Add support to Parquet Time32/64 by converting it into DateTime64. Parquet time32/64 represents time elapsed since midnight, while DateTime32/64 represents an actual unix timestamp. Conversion simply offsets from 0. #41333 (Arthur Passos). Implement set operations on Apache Datasketches. #39919 (Fangyuan Deng). Note: there is no point of using Apache Datasketches, they are inferiour than ClickHouse and only make sense for integration with other systems. Allow recording errors to specified file while reading text formats (CSV, TSV). #40516 (zjial). ClickHouse release 22.10, 2022-10-25 Rename cache commands: show caches -> show filesystem caches, describe cache -> describe filesystem cache. #41508 (Kseniia Sumarokova). Remove support for the WITH TIMEOUT section for LIVE VIEW. This closes #40557. #42173 (Alexey Milovidov). Remove support for the {database} macro from the client's prompt. It was displayed incorrectly if the database was unspecified and it was not updated on USE statements. This closes #25891. #42508 (Alexey Milovidov). Composable protocol configuration is added. Now different protocols can be set up with different listen hosts. Protocol wrappers such as PROXYv1 can be set up over any other protocols (TCP, TCP secure, MySQL, Postgres). #41198 (Yakov Olkhovskiy). Add S3 as a new type of the destination of backups. Support BACKUP to S3 with as-is path/data structure. #42333 (Vitaly Baranov), #42232 (Azat Khuzhin). Added functions (randUniform, randNormal, randLogNormal, randExponential, randChiSquared, randStudentT, randFisherF, randBernoulli, randBinomial, randNegativeBinomial, randPoisson) to generate random values according to the specified distributions. This closes #21834. #42411 (Nikita Mikhaylov). An improvement for ClickHouse Keeper: add support for uploading snapshots to S3. S3 information can be defined inside keeper_server.s3_snapshot. #41342 (Antonio Andelic). Added an aggregate function analysisOfVariance (anova) to perform a statistical test over several groups of normally distributed observations to find out whether all groups have the same mean or not. Original PR #37872. #42131 (Nikita Mikhaylov). Support limiting of temporary data stored on disk using settings max_temporary_data_on_disk_size_for_user/max_temporary_data_on_disk_size_for_query . #40893 (Vladimir C). Add setting format_json_object_each_row_column_for_object_name to write/parse object name as column value in JSONObjectEachRow format. #41703 (Kruglov Pavel). Add BLAKE3 hash-function to SQL. #33435 (BoloniniD). The function javaHash has been extended to integers. #41131 (JackyWoo). Add OpenTelemetry support to ON CLUSTER DDL (require distributed_ddl_entry_format_version to be set to 4). #41484 (Frank Chen). Added system table asynchronous_insert_log. It contains information about asynchronous inserts (including results of queries in fire-and-forget mode (with wait_for_async_insert=0)) for better introspection. #42040 (Anton Popov). Add support for methods lz4, bz2, snappy in HTTP's Accept-Encoding which is a non-standard extension to HTTP protocol. #42071 (Nikolay Degterinsky). Adds Morton Coding (ZCurve) encode/decode functions. #41753 (Constantine Peresypkin). Add support for SET setting_name = DEFAULT. #42187 (Filatenkov Artur). ClickHouse release 22.11, 2022-11-17 JSONExtract family of functions will now attempt to coerce to the requested type. #41502 (Márcio Martins). Adds support for retries during INSERTs into ReplicatedMergeTree when a session with ClickHouse Keeper is lost. Apart from fault tolerance, it aims to provide better user experience, - avoid returning a user an error during insert if keeper is restarted (for example, due to upgrade). #42607 (Igor Nikonov). Add Hudi and DeltaLake table engines, read-only, only for tables on S3. #41054 (Daniil Rubin, Kseniia Sumarokova). Add table function hudi and deltaLake. #43080 (flynn). Support for composite time intervals. 1. Add, subtract and negate operations are now available on Intervals. In the case where the types of Intervals are different, they will be transformed into the Tuple of those types. 2. A tuple of intervals can be added to or subtracted from a Date/DateTime field. 3. Added parsing of Intervals with different types, for example: INTERVAL '1 HOUR 1 MINUTE 1 SECOND'. #42195 (Nikolay Degterinsky). Added ** glob support for recursive directory traversal of the filesystem and S3. Resolves #36316. #42376 (SmitaRKulkarni). Introduce s3_plain disk type for write-once-read-many operations. Implement ATTACH of MergeTree table for s3_plain disk. #42628 (Azat Khuzhin). Added applied row-level policies to system.query_log. #39819 (Vladimir Chebotaryov). Add four-letter command csnp for manually creating snapshots in ClickHouse Keeper. Additionally, lgif was added to get Raft information for a specific node (e.g. index of last created snapshot, last committed log index). #41766 (JackyWoo). Add function ascii like in Apache Spark: https://spark.apache.org/docs/latest/api/sql/#ascii. #42670 (李扬). Add function pmod which returns non-negative result based on modulo. #42755 (李扬). Add function formatReadableDecimalSize. #42774 (Alejandro). Add function randCanonical, which is similar to the rand function in Apache Spark or Impala. The function generates pseudo random results with independent and identically distributed uniformly distributed values in [0, 1). #43124 (李扬). Add function displayName, closes #36770. #37681 (hongbin). Add min_age_to_force_merge_on_partition_only setting to optimize old parts for the entire partition only. #42659 (Antonio Andelic). Add generic implementation for arbitrary structured named collections, access type and system.named_collections. #43147 (Kseniia Sumarokova). powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Clickhouse/project/check.html":{"url":"common/Clickhouse/project/check.html","title":"问题排查","keywords":"","body":"问题排查 问题排查 2023/03/03 clickhouse CPU打满 2023/01/28 CK版本升级 21.3 -> 22.8 2022/03/07 部分异常及对应配置 2021/06/18 Clickhouse drop table on cluster but not delete on zookeeper 2021/05/03 时间插入后有偏移 2023/03/03 clickhouse CPU打满 查看日志报错 2023.03.03 14:30:56.356211 [ 60975 ] {} xm.member_schema (ReplicatedMergeTreePartCheckThread): Checking part 202109_1_541_105 2023.03.03 14:30:56.356248 [ 60907 ] {} xm.member_schema (b9daa48e-ade2-48e0-92cf-248000210ac9): DB::Exception: No active replica has part 202109_1_541_105 or covering part 2023.03.03 14:30:56.356541 [ 60975 ] {} xm.member_schema (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part 202109_1_541_105 or covering part. 2023.03.03 14:30:56.357000 [ 60975 ] {} xm.member_schema (ReplicatedMergeTreePartCheckThread): Found parts with the same min block and with the same max block as the missing part 202109_1_541_105 on replica 1. Hoping that it will eventually appear as a result of a merge. Parts: 202109_1_534_104, 202109_535_541_2 2023.03.03 14:30:56.503491 [ 60981 ] {} xm.member_schema (ReplicatedMergeTreePartCheckThread): Checking part 202109_1_541_105 2023.03.03 14:30:56.503518 [ 60917 ] {} xm.member_schema (b9daa48e-ade2-48e0-92cf-248000210ac9): DB::Exception: No active replica has part 202109_1_541_105 or covering part 2023.03.03 14:30:56.503916 [ 60981 ] {} xm.member_schema (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part 202109_1_541_105 or covering part. 2023.03.03 14:30:56.504359 [ 60981 ] {} xm.member_schema (ReplicatedMergeTreePartCheckThread): Found parts with the same min block and with the same max block as the missing part 202109_1_541_105 on replica 1. Hoping that it will eventually appear as a result of a merge. Parts: 202109_1_534_104, 202109_535_541_2 2023.03.03 14:30:56.863315 [ 60929 ] {} DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 144.55 GiB. 2023.03.03 14:30:57.013859 [ 60910 ] {} xm.member_schema (b9daa48e-ade2-48e0-92cf-248000210ac9): DB::Exception: No active replica has part 202109_1_541_105 or covering part 2023.03.03 14:30:57.013923 [ 60996 ] {} xm.member_schema (ReplicatedMergeTreePartCheckThread): Checking part 202109_1_541_105 2023.03.03 14:30:57.015119 [ 60996 ] {} xm.member_schema (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part 202109_1_541_105 or covering part. 应该是停机的时候有写入，数据分片出问题了 alter table xm.upload_schema detach partition 76; alter table xm.upload_schema attach partition 76; alter table xm.member_schema detach partition '202109'; alter table xm.member_schema attach partition '202109'; 参考： https://github.com/ClickHouse/ClickHouse/issues/14296 2023/01/28 CK版本升级 21.3 -> 22.8 mkdir /opt/clickhouse/22.8.9.24 cd /opt/clickhouse/22.8.9.24 wget https://packages.clickhouse.com/rpm/lts/clickhouse-client-22.8.9.24.x86_64.rpm wget https://packages.clickhouse.com/rpm/lts/clickhouse-common-static-22.8.9.24.x86_64.rpm wget https://packages.clickhouse.com/rpm/lts/clickhouse-common-static-dbg-22.8.9.24.x86_64.rpm wget https://packages.clickhouse.com/rpm/lts/clickhouse-server-22.8.9.24.x86_64.rpm 关闭clickhouse服务 systemctl stop clickhouse-server.service 确定关闭完成，查看服务状态 systemctl status clickhouse-server.service 升级(运行过程中需要输入default的密码) rpm -Uvh *.rpm 重启clickhouse服务 systemctl daemon-reload systemctl start clickhouse-server.service 查看服务状态 systemctl status clickhouse-server.service 2022/03/07 部分异常及对应配置 code 62，Max query size exceeded: 10000000 Code: 168, AST is too big，Maximum: 50000: 10000000 10000000 2021/06/18 Clickhouse drop table on cluster but not delete on zookeeper 删除zk中的路径是异步操作，可以调节触发时间，或者进行区分 use ordinary database instead of atomic. create database ... Engine=Ordinary. use uniq ZK path using a new var. {uuid} /clickhouse/tables/{layer}-{shard}-{uuid}/streams.streams_apps_ext_log_test reduce database_atomic_delay_before_drop_table_sec = 0; & drop table ... sync 参考： https://github.com/ClickHouse/ClickHouse/issues/18382 https://github.com/ClickHouse/ClickHouse/blob/master/tests/config/config.d/database_atomic.xml 2021/05/03 时间插入后有偏移 需要注意 通过插入字符串的方式插入时间字段， CK会根据配置的当前时区、字段配置的时区等信息对字符串做自动转化 通过插入整形时间戳方式插入时间字段，CK会直接存入不做额外处理 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Redis/":{"url":"common/Redis/","title":"Redis","keywords":"","body":"Redis powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Redis/install.html":{"url":"common/Redis/install.html","title":"安装","keywords":"","body":"安装 安装 docker windows 下载安装 linux 源码安装 yum安装 运行 docker https://hub.docker.com/_/redis docker pull redis docker volume create --name redisdb docker run --name redis -v redisdb:/data -p 6379:6379 -d redis windows 下载安装 https://github.com/microsoftarchive/redis linux 源码安装 wget http://download.redis.io/releases/redis-4.0.2.tar.gz tar xzf redis-4.0.2.tar.gz make # 对应的配置文件位置 # /etc/init.d/redis_6379 yum安装 yum install -y redis 运行 脚本运行 #!/bin/sh #Configurations injected by install_server below.... EXEC=/usr/local/bin/redis-server CLIEXEC=/usr/local/bin/redis-cli PIDFILE=/var/run/redis_6379.pid CONF=\"/etc/redis/6379.conf\" REDISPORT=\"6379\" ############### # SysV Init Information # chkconfig: - 58 74 # description: redis_6379 is the redis daemon. ### BEGIN INIT INFO # Provides: redis_6379 # Required-Start: $network $local_fs $remote_fs # Required-Stop: $network $local_fs $remote_fs # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Should-Start: $syslog $named # Should-Stop: $syslog $named # Short-Description: start and stop redis_6379 # Description: Redis daemon ### END INIT INFO case \"$1\" in start) if [ -f $PIDFILE ] then echo \"$PIDFILE exists, process is already running or crashed\" else echo \"Starting Redis server...\" $EXEC $CONF fi ;; stop) if [ ! -f $PIDFILE ] then echo \"$PIDFILE does not exist, process is not running\" else PID=$(cat $PIDFILE) echo \"Stopping ...\" $CLIEXEC -p $REDISPORT shutdown while [ -x /proc/${PID} ] do echo \"Waiting for Redis to shutdown ...\" sleep 1 done echo \"Redis stopped\" fi ;; status) PID=$(cat $PIDFILE) if [ ! -x /proc/${PID} ] then echo 'Redis is not running' else echo \"Redis is running ($PID)\" fi ;; restart) $0 stop $0 start ;; *) echo \"Please use start, stop, restart or status as first argument\" ;; esac systemctl 运行 systemctl start redis.service systemctl stop redis.service powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Redis/command.html":{"url":"common/Redis/command.html","title":"命令","keywords":"","body":"命令 命令 文档 文档 文档地址 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Redis/project/":{"url":"common/Redis/project/","title":"专题","keywords":"","body":"专题 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Redis/project/version.html":{"url":"common/Redis/project/version.html","title":"version","keywords":"","body":"version powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Redis/project/safe.html":{"url":"common/Redis/project/safe.html","title":"安全设置","keywords":"","body":"安全设置 安全设置 修改配置文件权限 修改端口 指定密码 限制命令 修改配置文件权限 chmod 600 //redis.conf 修改端口 编辑文件redis的配置文件redis.conf，找到包含port的行，将默认的6379修改为自定义的端口号，然后重启redis 指定密码 打开redis.conf，找到requirepass所在的地方，修改为指定的密码，密码应符合复杂性要求： 1、长度8位以上 2、包含以下四类字符中的三类字符: 英文大写字母(A 到 Z) 英文小写字母(a 到 z) 10 个基本数字(0 到 9) 非字母字符(例如 !、$、%、@、^、&等，#除外) 3、避免使用已公开的弱密码，如：abcd.1234 、admin@123等 限制命令 修改 redis.conf 文件，禁用部分危险命令， 或改名 rename-command FLUSHALL \"\" rename-command FLUSHDB \"\" rename-command CONFIG \"\" rename-command KEYS \"\" rename-command SHUTDOWN \"\" rename-command DEL \"\" rename-command EVAL \"\" rename-command FLUSHALL joYAPNXRPmcarcR4ZDgC powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Redis/project/lua.html":{"url":"common/Redis/project/lua.html","title":"lua脚本","keywords":"","body":"lua脚本 lua脚本 读写锁 利用scan进行批量操作 链接 读写锁 读锁 local write_key = 'write'; local mode = redis.call('hget', KEYS[1], 'mode'); if (mode == false) then redis.call('hset', KEYS[1], 'mode', 'read'); redis.call('hset', KEYS[1], write_key, 0); redis.call('hset', KEYS[1], ARGV[2], 1); redis.call('pexpire', KEYS[1], ARGV[1]); return nil; end; if (mode == 'read') then local ind = redis.call('hincrby', KEYS[1], ARGV[2], 1); local remainTime = redis.call('pttl', KEYS[1]); redis.call('pexpire', KEYS[1], math.max(remainTime, ARGV[1])); return nil; end; return redis.call('pttl', KEYS[1]) 释放读锁 local write_key = 'write'; local mode = redis.call('hget', KEYS[1], 'mode'); if (mode == false) then return 1; end; local lockExists = redis.call('hexists', KEYS[1], write_key); if (lockExists == 0) then return nil; end; local counter = redis.call('hincrby', KEYS[1], ARGV[1], -1); if (counter == 0) then redis.call('hdel', KEYS[1], ARGV[1]); end; if (redis.call('hlen', KEYS[1]) > 2) then if mode == 'write' then return 0; end; return nil; end; redis.call('del', KEYS[1]); return 1; 写锁 local write_key = 'write' local mode = redis.call('hget', KEYS[1], 'mode'); if (mode == false) then redis.call('hset', KEYS[1], 'mode', 'write'); redis.call('hset', KEYS[1], write_key, 1); redis.call('pexpire', KEYS[1], ARGV[1]); return nil; end; if (mode == 'write') then if (redis.call('hexists', KEYS[1], write_key) == 1) then redis.call('hincrby', KEYS[1], write_key, 1); local currentExpire = redis.call('pttl', KEYS[1]); redis.call('pexpire', KEYS[1], currentExpire + ARGV[1]); return redis.call('pttl', KEYS[1]); end; end; return redis.call('pttl', KEYS[1]); 释放写锁 local write_key = 'write' local mode = redis.call('hget', KEYS[1], 'mode'); if (mode == false) then return 1; end; local lockExists = redis.call('hexists', KEYS[1], write_key); if (lockExists == 0) then return nil; end; if (mode == 'write') then redis.call('del', KEYS[1]); return 1 end; return 0; 强制释放锁 if (redis.call('hget', KEYS[1], 'mode') == 'read') or (redis.call('hget', KEYS[1], 'mode') == 'write') then redis.call('del', KEYS[1]); return 1; end; return 0; 利用scan进行批量操作 查询符合条件的key数量 local cursor = '0' local count = 0 repeat local result = redis.call('SCAN', cursor, 'MATCH', KEYS[1], 'COUNT', 100) cursor = result[1] count = count + #result[2] until cursor == '0' return count 删除符合条件的key，并返回删除的数量 redis.replicate_commands() local cursor = '0' local delete_count = 0 repeat local result = redis.call('scan', cursor, 'MATCH', KEYS[1], 'COUNT', 100) cursor = result[1] local list = result[2] delete_count = delete_count + #list redis.call('del', unpack(list)) until (cursor == '0') return delete_count 链接 Cuckoo Filter: https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Memcached/":{"url":"common/Memcached/","title":"Memcached","keywords":"","body":"Memcached powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Memcached/install.html":{"url":"common/Memcached/install.html","title":"安装","keywords":"","body":"安装 安装 windows安装 下载 安装 & 运行 连接 windows安装 下载 Memcached-win64.zip 安装 & 运行 memcached.exe -d install memcached.exe -d start 连接 使用各类客户端连接 使用 telnet （在 中设置开启） telnet 127.0.0.1 11211 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Zookeeper/":{"url":"common/Zookeeper/","title":"Zookeeper","keywords":"","body":"Zookeeper powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Zookeeper/install.html":{"url":"common/Zookeeper/install.html","title":"安装","keywords":"","body":"安装 安装 zkCli.sh zkCli.sh # 帮助 h # 退出 quit # 查看列表 ls # 查看信息 stat # 查看内容 get [path] # 创建节点 create [-s|-e] # 设置内容 set [] # 设置节点配额 setquota [-n|-b] # 查看节点配额 listquota # 删除节点配额 delquota [-n|-b] powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Docker/":{"url":"common/Docker/","title":"Docker","keywords":"","body":"Docker powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Docker/install.html":{"url":"common/Docker/install.html","title":"安装","keywords":"","body":"安装 安装 centos 安装 安装 Docker 安装 Docker Compose centos 升级 升级docker 升级docker-compose Centos 6.7 安装 Docker几个坑 windows 安装 centos 安装 安装 Docker yum install epel-release yum install docker-io vi /etc/sysconfig/docker other-args更改为：other_args=\"--exec-driver=lxc --selinux-enabled\" # 修改镜像 mkdir -p /etc/docker tee /etc/docker/daemon.json service docker start 将docker加入开机启动 chkconfig docker on 安装 Docker Compose pip install pip==9.0.1 pip install docker-compose==1.5.2 可能需要修改docker-compose.yml, 具体参考 BLOG centos 升级 升级docker # 停止当前正在运行的Docker服务： sudo systemctl stop docker # 删除旧版本的Docker： sudo yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine # 安装依赖项： sudo yum install -y yum-utils device-mapper-persistent-data lvm2 # 添加Docker存储库： sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # 安装最新版本的Docker： sudo yum install docker-ce docker-ce-cli containerd.io # 启动Docker服务： sudo systemctl start docker 升级docker-compose sudo curl -L \"https://github.com/docker/compose/releases/download/v2.17.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/bin/docker-compose sudo chmod +x /usr/bin/docker-compose Centos 6.7 安装 Docker几个坑 官方文档是针对7以上版本的，不适用 pip版本必须是9.0.1 docker-compose必须是1.5.2 1.5.2版本 只支持 V1 版本的 docker-compose.yml，所以需要修改，大多是删除 version: \"2\" 和services:两行 docker-compose.yml windows 安装 参考: powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Docker/command.html":{"url":"common/Docker/command.html","title":"命令","keywords":"","body":"命令 命令 docker 常用命令 docker-compose 常用命令 目录 docker 常用命令 命令 描述 翻译 attach Attach to a running container 当前 shell 下 attach 连接指定运行镜像 build Build an image from a Dockerfile 通过 Dockerfile 定制镜像 commit Create a new image from a container's changes 提交当前容器为新的镜像 cp Copy files/folders from the containers filesystem to the host path 从容器中拷贝指定文件或者目录到宿主机中 create Create a new container 创建一个新的容器，同 run，但不启动容器 diff Inspect changes on a container's filesystem 查看 docker 容器变化 events Get real time events from the server 从 docker 服务获取容器实时事件 exec Run a command in an existing container 在已存在的容器上运行命令 export Stream the contents of a container as a tar archive 导出容器的内容流作为一个 tar 归档文件[对应 import ] history Show the history of an image 展示一个镜像形成历史 images List images 列出系统当前镜像 import Create a new filesystem image from the contents of a tarball 从 tar 包中的内容创建一个新的文件系统映像[对应 export] info Display system-wide information 显示系统相关信息 inspect Return low-level information on a container 查看容器详细信息 kill Kill a running container kill 指定 docker 容器 load Load an image from a tar archive 从一个 tar 包中加载一个镜像[对应 save] login Register or Login to the docker registry server 注册或者登陆一个 docker 源服务器 logout Log out from a Docker registry server 从当前 Docker registry 退出 logs Fetch the logs of a container 输出当前容器日志信息 port Lookup the public-facing port which is NAT-ed to PRIVATE_PORT 查看映射端口对应的容器内部源端口 pause Pause all processes within a container 暂停容器 ps List containers 列出容器列表 pull Pull an image or a repository from the docker registry server 从 docker 镜像源服务器拉取指定镜像或者库镜像 push Push an image or a repository to the docker registry server 推送指定镜像或者库镜像至 docker 源服务器 restart Restart a running container 重启运行的容器 rm Remove one or more containers 移除一个或者多个容器 rmi Remove one or more images 移除一个或多个镜像[无容器使用该镜像才可删除，否则需删除相关容器才可继续或 -f 强制删除] run Run a command in a new container 创建一个新的容器并运行一个命令 save Save an image to a tar archive 保存一个镜像为一个 tar 包[对应 load] search Search for an image on the Docker Hub 在 docker hub 中搜索镜像 start Start a stopped containers 启动容器 stop Stop a running containers 停止容器 tag Tag an image into a repository 给源中镜像打标签 top Lookup the running processes of a container 查看容器中运行的进程信息 unpause Unpause a paused container 取消暂停容器 version Show the docker version information 查看 docker 版本号 wait Block until a container stops, then print its exit code 截取容器停止时的退出状态值 docker-compose 常用命令 命令 描述 翻译 build Build or rebuild services 构建或重建服务 bundle Generate a Docker bundle from the Compose file config Validate and view the compose file create Create services down Stop and remove containers, networks, images, and volumes events Receive real time events from containers exec Execute a command in a running container help Get help on a command 命令帮助 kill Kill containers 杀掉容器 logs View output from containers 显示容器的输出内容 pause Pause services port Print the public port for a port binding 打印绑定的开放端口 ps List containers 显示容器 pull Pull service images 拉取服务镜像 push Push service images restart Restart services 重启服务 rm Remove stopped containers 删除停止的容器 run Run a one-off command 运行一个一次性命令 scale Set number of containers for a service 设置服务的容器数目 start Start services 开启服务 stop Stop services 停止服务 top Display the running processes unpause Unpause services up Create and start containers 创建并启动容器 version Show the Docker-Compose version information 目录 compose-file：Reference and guidelines: https://docs.docker.com/compose/compose-file/ powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Docker/example.html":{"url":"common/Docker/example.html","title":"实例","keywords":"","body":"实例 实例 简单场景 一般流程 更新容器配置 删除所有container 加速 特殊场景 打包镜像 实例 简单场景 一般流程 pull => [change] => tag => push run(image) => => start(container) stop(container) => restart(container) rmi(images) 更新容器配置 docker-compose up --build --no-start [image] docker-compose restart [image] 删除所有container docker rm -f $(docker ps -aq) 加速 阿里云官方 sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json 特殊场景 打包镜像 # squash its history. docker export 77d9619a7a71 > update.tar docker import - update # keep its history docker save -o update1.tar update docker load 实例 docker cp \"C:\\Users\\user1\\Downloads\\Miniconda3-latest-Linux-x86_64.sh\" \"ffcf4b501621:/home\" yum install -y bzip2 export PATH=\"/root/miniconda3/bin\":PATH docker pull centos powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Git/":{"url":"common/Git/","title":"Git","keywords":"","body":"Git powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Git/install.html":{"url":"common/Git/install.html","title":"安装","keywords":"","body":"安装 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Git/command.html":{"url":"common/Git/command.html","title":"命令","keywords":"","body":"命令 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Git/example.html":{"url":"common/Git/example.html","title":"实例","keywords":"","body":"实例 实例 git pull 将【本地分支】与【远程分支】同步 删除git的追踪 删除远程分支 同步远程分支列表 分支返回到未提交的状态 追加commit git rebase 回退分支, 及回退merge的分支 删除 stash(损坏的) 统计代码行数 大小写敏感 cherry-pick cannot lock ref git pull git pull 将【本地分支】与【远程分支】同步 git branch --set-upstream-to=origin/ 删除git的追踪 git rm --cache [file] 删除远程分支 git -c diff.mnemonicprefix=false -c core.quotepath=false --no-optional-locks branch -D -r origin/ git -c diff.mnemonicprefix=false -c core.quotepath=false --no-optional-locks push origin :refs/heads/ 同步远程分支列表 git remote prune origin 分支返回到未提交的状态 git reset --hard 追加commit git commit --amend git rebase git rebase git rebase 假设当前在分支1， 自动切换到分支2， 一步步提交分支1的修改， 完成后可以提交 git rebase 将从分出后开始的上的修改在重演一遍 回退分支, 及回退merge的分支 git revert git revert -m [1,2] 回退到 状态 （git revert -m 1） [m]----|-----[n]----[n`] | [d]----| 下次如何把develop的修改再提交进来呢？ 下次 再要合并进来的时候 要先git revert n` [m]----|-----[n]----[n`]----[n``] | [d]----| 然后合并 [m]----|-----[n]----[n`]----[n``]----|----[l] | | [d]----|-----[e]---------------------------| 删除 stash(损坏的) git reflog delete --rewrite stash@{0} 统计代码行数 git log --author=\"$(git config --get user.name)\" --pretty=tformat: --numstat | gawk '{ add += $1 ; subs += $2 ; loc += $1 - $2 } END { printf \"added lines: %s removed lines : %s total lines: %s\\n\",add,subs,loc }' - 大小写敏感 git config core.ignorecase false cherry-pick git cherry-pick [hash] git cherry-pick [hash]..[hash] git cherry-pick ^[hash]..[hash] cannot lock ref # 先更新一下reference git update-ref -d refs/remotes/origin/[locked branch name] # 随后同步代码 git pull powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Git/project/":{"url":"common/Git/project/","title":"专题","keywords":"","body":"专题 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Git/project/submode.html":{"url":"common/Git/project/submode.html","title":"子模块","keywords":"","body":"子模块 子模块 使用场景 功能介绍 添加子模块 初始化子模块 .gitmodules 父仓库提交 子模块提交 拉取远程子模块的代码到本地 直接更新 本地子模块修改提交到远程 其他命令 批量操作子模块 查看子模块的签出版本 删除子模块 可能会出现的问题 主模块提交并推送了改动，而子模块并没有推送 在有子模块的项目中切换分支可能会造成麻烦 提交和获取的问题 子模块切换分支 父仓库中子模块的更新 通过http方式拉取代码构建时需要输入子模块账户、密码 其他 参考 使用场景 一个项目里面的代码分为基础代码和定制化代码，定制化代码是针对不同客户的，基础代码需要和定制化代码分开管理，部署的时候是作为一个项目一起跑的。 功能介绍 submodule 是目前对 git 仓库拆分的已有实现之一。它允许将一个Git仓库作为另一个Git仓库的子目录。能够将另一个仓库克隆到自己的项目中，同时还保持独立的提交。 添加子模块 # 克隆仓库 git clone git@xxxxxx/xxx.git # 将子仓库添加至父仓库，[]中为子仓库的git url git submodule add [child1 url] git submodule add [child2 url] 上述操作会将两个子仓库clone到父目录下。 初始化子模块 注意，如果远程父仓库中已经存在子模块，我们clone了父仓库之后，子模块==并不会==一并clone下来，只会有一个目录，我们需要初始化子模块并拉取代码 # 初始化本地配置文件 git submodule init # 拉取子模块的代码（注意，这将拉取在父仓库中签出的版本） git submodule update # 拉取子模块的最新版本, [] 中是可选的 git submodule update --remote [submodule_name] 克隆时子模块的初始化和拉取可以合并成一个操作 git clone git@xxxxxx/xxx.git --recursive .gitmodules 添加子模块之后，除了子模块之外，还多了一个叫.gitmodules的文件，这是一份子模块与路径的映射关系图，git 根据这份文件去识别 submodule。(tips：这个文件是需要被追踪的) 查看一下文件内容： [submodule \"child1_repo\"] path = child1_repo url = git@github.com:xxx/child1_repo.git [submodule \"child2_repo\"] path = child2_repo url = git@github.com:xxx/child2_repo.git 父仓库提交 父仓库下（不涉及子模块）操作==同git== 当子模块存在改动的情况下，父仓库会将子模块当前分支下最新的commitId作为一个目录进行记录，而非将它记录成一个子目录或者一个文件。 父目录记录的子模块的改动信息大致如下 # 子模块当前指向的commitId的信息 Commit 8e182c74182adde49f2b6d192f2a85c50d87f538 Commit Message add child1_repo 子仓库的任何修改反应在parent下就是一次提交的信息，在parent下将这个信息按git流程提交即可。 提交后远端仓库的情况： git页面中子模块的名称格式： 模块名称 @ 签出时的commitId @以及后面签出的commitId在本地不会显示 子模块提交 初始化并拉取子模块内容后，其处于一个称作 “游离的 HEAD”（detached HEAD） 的状态（git 提示的是 commitid 而不是分支名）。这个状态下可以正常的 git 操作，但是此时是没有分支进行跟踪的，也就没办法推送代码。 需要时刻小心子模块是否处于这种状态，在子模块运行 git checkout branch 后即可。 ==对于子模块来说，只有它们的远程 URL 会被记录在父仓库中，以及它们在主项目中的本地路径和签出的版本。== 拉取远程子模块的代码到本地 子仓库在远程有更改，在本地进行同步（如果只要修改主仓库的代码，正常的git操作就可以了），执行 # 同初始化子模块部分的update操作 git submodule update --remote [child_repo] 这条命令将会拉取子模块中最新的提交。 再次强调的是，我们如果单纯的执行git submodule update，我们拉取的将是远程parent下最后一次签出的子仓库的版本。 直接更新 另外一种更新子仓库的方法就是直接进入其目录下，像任何普通的Git那样进行操作即可。 本地子模块修改提交到远程 在子模块目录下就像普通的git那样操作就好了（==注意子模块是否有checkout到某个分支上，没有的话需要先checkout，否则容易丢失你的修改！==） 其他命令 批量操作子模块 可以使用foreach命令对子模块进行批量操作 举例： git submodule foreach 'git checkout -b featureA' 查看子模块的签出版本 git submodule status ... +3557a0e0f7280fb3aba18fb9035d204c7de6344f lib/ToProgress (0.1.1) 通过上述命令，我们可以查看子模块的哪一个版本在当前被签出了。 删除子模块 我们可以手动对子模块进行删除，但这会儿打乱配置文件（除非你很小心） git submodule deinit [child] git rm [child] git status ... modified: .gitmodules deleted: child 使用 git submodule deinit，我们可以确保从配置文件中完全地删除一个子模块。 使用 git rm ，我们可以最终删除这个子模块的文件，包括一些其它废弃的部分。 提交这些改动，这个子模块就会从你的项目中被彻底地删除了。 其他语法传送门 可能会出现的问题 Git对于子模块的管理相对来说比较复杂。当然出现异常的情况也是不可避免的，我们来看看在使用git submodule的过程中可能会出哪些问题。 主模块提交并推送了改动，而子模块并没有推送 如果我们在主仓库中提交并推送但并不推送子模块上的改动，其他人尝试更新子模块的人会遇到麻烦，因为他们无法得到依赖的子模块改动。那些改动只存在于我们本地的拷贝中。 可以使用如下命令检查子模块的改动是否提交 git push --recurse-submodules=check # 如果子模块没有提交，会直接报错 # or git push --recurse-submodules=on-demand # 如果子模块没有提交，会尝试提交，提交不成功同时会阻止主仓库的推送 在有子模块的项目中切换分支可能会造成麻烦 如果你创建一个新分支，在其中添加一个子模块，之后切换到没有该子模块的分支上时，你仍然会有一个还未跟踪的子模块目录，这时候如果不小心提交了这个子模块（git commit -am \"message\"），就会有问题了。 提交和获取的问题 对子模块做了修改，需要先推送子模块再主模块，同时拉取的时候也需要先主模块，再子模块。 子模块切换分支 对子模块做本地修改需要先检出分支，否则有可能在 “游离的 HEAD” 上做修改。 父仓库中子模块的更新 如果你的同事更新了 submodule，然后更新了父项目中依赖的版本号。你需要在 git pull 之后，调用 git submodule update 来更新 submodule 信息。这儿的坑在于，如果你 git pull 之后，忘记了调用 git submodule update，那么你极有可能再次把==旧的==submodule 依赖信息提交上去（使用 git submit -am \"message\" 或者 git add .提交的人会遇到这种事）。 通过http方式拉取代码构建时需要输入子模块账户、密码 查看git 存储凭证 其他 另外还有别的可能出现的问题（e.g.将子目录转换成子模块、git submodule update failed等） 参考 git子模块 git submodule(CSDN) 子模块 - Git Tower powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Git/project/credential.html":{"url":"common/Git/project/credential.html","title":"凭证存储","keywords":"","body":"凭证存储 凭证存储 说明 示例 自定义凭证缓存 说明 如果你使用的是 SSH 方式连接远端，并且设置了一个没有口令的密钥，这样就可以在不输入用户名和密码的情况下安全地传输数据。 然而，这对 HTTP 协议来说是不可能的 —— 每一个连接都是需要用户名和密码的。 这在使用双重认证的情况下会更麻烦，因为你需要输入一个随机生成并且毫无规律的 token 作为密码。 幸运的是，Git 拥有一个凭证系统来处理这个事情。 下面有一些 Git 的选项： 默认所有都不缓存。 每一次连接都会询问你的用户名和密码。 “cache” 模式会将凭证存放在内存中一段时间。 密码永远不会被存储在磁盘中，并且在15分钟后从内存中清除。 “store” 模式会将凭证用明文的形式存放在磁盘中，并且永不过期。 这意味着除非你修改了你在 Git 服务器上的密码，否则你永远不需要再次输入你的凭证信息。 这种方式的缺点是你的密码是用明文的方式存放在你的 home 目录下。 如果你使用的是 Mac，Git 还有一种 “osxkeychain” 模式，它会将凭证缓存到你系统用户的钥匙串中。 这种方式将凭证存放在磁盘中，并且永不过期，但是是被加密的，这种加密方式与存放 HTTPS 凭证以及 Safari 的自动填写是相同的。 如果你使用的是 Windows，你可以安装一个叫做 “winstore” 的辅助工具。 这和上面说的 “osxkeychain” 十分类似，但是是使用 Windows Credential Store 来控制敏感信息。 可以在 https://gitcredentialstore.codeplex.com 下载。 你可以设置 Git 的配置来选择上述的一种方式 git config --global credential.helper cache 部分辅助工具有一些选项。 “store” 模式可以接受一个 --file 参数，可以自定义存放密码的文件路径（默认是 ~/.git-credentials ）。 “cache” 模式有 --timeout 参数，可以设置后台进程的存活时间（默认是 “900”，也就是 15 分钟）。 下面是一个配置 “store” 模式自定义路径的例子： git config --global credential.helper store --file ~/.my-credentials Git 甚至允许你配置多个辅助工具。 当查找特定服务器的凭证时，Git 会按顺序查询，并且在找到第一个回答时停止查询。 当保存凭证时，Git 会将用户名和密码发送给 所有 配置列表中的辅助工具，它们会按自己的方式处理用户名和密码。 如果你在闪存上有一个凭证文件，但又希望在该闪存被拔出的情况下使用内存缓存来保存用户名密码，.gitconfig 配置文件如下： [credential] helper = store --file /mnt/thumbdrive/.git-credentials helper = cache --timeout 30000 ~/git.store 文件的内容类似： https://bob:s3cre7@mygithost 仅仅是一系列包含凭证信息 URL 组成的行。 osxkeychain 和 winstore 辅助工具使用它们后端存储的原生格式，而 cache 使用它的内存格式（其他进程无法读取）。 示例 echo \"http:/uesrname:password@github.com\" >> ~/.git-credentials git config --global credential.helper store 自定义凭证缓存 已经知道 git-credential-store 之类的是和 Git 是相互独立的程序，就不难理解 Git 凭证辅助工具可以是 任意 程序。 虽然 Git 提供的辅助工具覆盖了大多数常见的使用场景，但并不能满足所有情况。 比如，假设你的整个团队共享一些凭证，也许是在部署时使用。 这些凭证是保存在一个共享目录里，由于这些凭证经常变更，所以你不想把它们复制到你自己的凭证仓库中。 现有的辅助工具无法满足这种情况；来看看我们如何自己实现一个。 这个程序应该拥有几个核心功能： 我们唯一需要关注的行为是 get；store 和 erase 是写操作，所以当接受到这两个请求时我们直接退出即可。 共享的凭证文件格式和 git-credential-store 使用的格式相同。 凭证文件的路径一般是固定的，但我们应该允许用户传入一个自定义路径以防万一。 我们再一次使用 Ruby 来编写这个扩展，但只要 Git 能够执行最终的程序，任何语言都是可以的。 这是我们的凭证辅助工具的完整代码： #!/usr/bin/env ruby require 'optparse' path = File.expand_path '~/.git-credentials' (1) OptionParser.new do |opts| opts.banner = 'USAGE: git-credential-read-only [options] ' opts.on('-f', '--file PATH', 'Specify path for backing store') do |argpath| path = File.expand_path argpath end end.parse! exit(0) unless ARGV[0].downcase == 'get' (2) exit(0) unless File.exists? path known = {} (3) while line = STDIN.gets break if line.strip == ''- k,v = line.strip.split '=', 2 known[k] = v end File.readlines(path).each do |fileline| (4) prot,user,pass,host = fileline.scan(/^(.*?):\\/\\/(.*?):(.*?)@(.*)$/).first if prot == known['protocol'] and host == known['host'] then puts \"protocol=#{prot}\" puts \"host=#{host}\" puts \"username=#{user}\" puts \"password=#{pass}\" exit(0) end end 我们在这里解析命令行参数，允许用户指定输入文件，默认是 ~/.git-credentials. 这个程序只有在接受到 get 行为的请求并且后端存储的文件存在时才会有输出。 这个循环从标准输入读取数据，直到读取到第一个空行。 输入的数据被保存到 known 哈希表中，之后需要用到。 这个循环读取存储文件中的内容，寻找匹配的行。 如果 known 中的协议和主机名与该行相匹配，这个程序输出结果并退出。 我们把这个辅助工具保存为 git-credential-read-only，放到我们的 PATH 路径下并且给予执行权限。 一个交互式会话类似： $ git credential-read-only --file=/mnt/shared/creds get protocol=https host=mygithost protocol=https host=mygithost username=bob password=s3cre7 由于这个的名字是 “git-” 开头，所以我们可以在配置值中使用简便的语法： git config --global credential.helper read-only --file /mnt/shared/creds powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Git/project/github.html":{"url":"common/Git/project/github.html","title":"GitHub","keywords":"","body":"GitHub GitHub 生成token actions 文档 生成token actions 文档 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/gitbook/":{"url":"common/gitbook/","title":"gitbook","keywords":"","body":"gitbook powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/gitbook/install.html":{"url":"common/gitbook/install.html","title":"安装","keywords":"","body":"安装 安装 基于包管理工具 命令 基于包管理工具 配置 { \"dependencies\": { \"gitbook-cli\": \"^2.3.2\" } } 命令 # 安装插件 gitbook install # 生成目录 gitbook init powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/gitbook/project/":{"url":"common/gitbook/project/","title":"专题","keywords":"","body":"专题 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/gitbook/project/github.html":{"url":"common/gitbook/project/github.html","title":"github","keywords":"","body":"github github 使用travis-ci自动化构建 配置github 配置travis-ci 配置脚本 使用travis-ci自动化构建 配置github 开放github pages 打开项目 -> settings -> Options -> GitHub Pages -> source: gh-pages: branch 注册服务token 用户 -> settings -> Developer settings -> Personal access tokens -> Generate new token 配置travis-ci https://travis-ci.org/ 配置脚本 创建 .travis.yml 编写以下内容branches: only: - master # 构建的分支 cache: directories: - node_modules # 依赖缓存的目录 before_install: - export TZ='Asia/Shanghai' # 设置时区 install: - npm install -g gitbook-cli # 安装编译工具 - gitbook install script: - gitbook build deploy: provider: pages skip-cleanup: true github-token: $GITHUB_TOKEN # github 上的token环境变量 local-dir: ./_book/ ## 根据情况自定义到静态文件输出目录 target-branch: gh-pages verbose: true on: branch: master powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/gitbook/plugins/":{"url":"common/gitbook/plugins/","title":"插件","keywords":"","body":"插件 插件 相关内容 相关内容 npm还源 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/gitbook/plugins/summary.html":{"url":"common/gitbook/plugins/summary.html","title":"简介","keywords":"","body":"简介 简介 列表 列表 名称 功能 备注 github 跳转到github仓库 github-buttons 支持直接在gitbook中fork等操作 edit-link 直接跳转到编辑页面 prism 代码块高亮，支持各种语法、 prism-themes 配合prism的主题 mermaid mermaid支持 已失效 mermaid-v8 mermaid支持 代码冲突 mermaid-newface mermaid支持 simple-mind-map 代码块实现思维导图 katex 数学公式解析 不好用 code 代码块复制 code-optimize 代码块复制,行号 advanced-emoji 支持更多emoji anchor-navigation-ex 添加Toc到侧边悬浮导航以及回到顶部按 splitter 分割条优化 favicon 修改icon local-video 支持播放视频 sectionx 支持划分段落 expandable-chapters-small 段落的小节点 tbfed-pagefooter 添加文章修改时间 chapter-fold 章节折叠 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/gitbook/plugins/mermaid.html":{"url":"common/gitbook/plugins/mermaid.html","title":"mermaid","keywords":"","body":"mermaid mermaid 示例 相关文档 示例 graph TD; A-->B; A-->C; B-->D; C-->D; {% mermaid %} graph TD; A-->B; A-->C; B-->D; C-->D; {% endmermaid %} 相关文档 文档:https://mermaid-js.github.io/mermaid/#/ 知乎示例:https://zhuanlan.zhihu.com/p/172635547 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/gitbook/plugins/simple-mind-map.html":{"url":"common/gitbook/plugins/simple-mind-map.html","title":"simple-mind-map","keywords":"","body":"simple-mind-map simple-mind-map 安装 配置 示例 markdown txtmap 安装 { \"plugins\": [\"simple-mind-map\"] } 配置 示例 markdown {% simplemindmap %} ```markdown * simplemindmap * config book.json * plugins * others * simple-mind-map * pluginsConfig * others * simple-mind-map * type * preset * linkShape * autoFit * style * custom file.md * markdown * type * preset * linkShape * autoFit * style * txtmap * json * mindmup ``` {% endsimplemindmap %} txtmap {% simplemindmap type=\"txtmap\" %} ```txtmap some example content for you to see the file structure ``` {% endsimplemindmap %} powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/notebook/":{"url":"common/notebook/","title":"notebook","keywords":"","body":"notebook powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/notebook/install.html":{"url":"common/notebook/install.html","title":"安装","keywords":"","body":"安装 安装 python kernel clojure kernel javascript kernel C kernel C++ kernel Go kernel python kernel pip install jupyter pip3 install jupyterlab clojure kernel 准备环境 bash> tree . . ├── deps.edn └── src └── user.clj 1 directory, 2 files bash> cat deps.edn {:deps {clojupyter {:mvn/version \"0.2.3\"}} :aliases {:depstar {:extra-deps {seancorfield/depstar {:mvn/version \"0.3.0\"}}}}} bash> cat src/user.clj (ns user (:require [clojupyter.kernel.version :as ver])) (defn user-ver [] (ver/version-string-long)) 安装编译clojupyter-standalone.jar clojure -A:depstar -m hf.depstar.uberjar clojupyter-standalone.jar clj -m clojupyter.cmdline install --ident clojupyter-1 --jarfile clojupyter-standalone.jar 修改错误的安装目录 查看目录~\\AppData\\Roaming\\jupyter， 修改部分内容，移入kernels， 修改kernel.json文件 javascript kernel npm install -g ijavascript ijsinstall 项目地址: https://github.com/n-riesco/ijavascript C kernel 项目地址: https://github.com/brendan-rius/jupyter-c-kernel C++ kernel 项目地址: https://github.com/jupyter-xeus/xeus-cling Go kernel 项目地址: https://github.com/gopherdata/gophernotes powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/notebook/plugins/":{"url":"common/notebook/plugins/","title":"插件","keywords":"","body":"插件 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/notebook/plugins/summary.html":{"url":"common/notebook/plugins/summary.html","title":"简介","keywords":"","body":"简介 简介 安装 资料 安装 pip install jupyter_contrib_nbextensions jupyter contrib nbextension install --system 资料 文档：https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/install.html powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/notebook/project/":{"url":"common/notebook/project/","title":"专题","keywords":"","body":"专题 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/notebook/project/style.html":{"url":"common/notebook/project/style.html","title":"样式","keywords":"","body":"样式 样式 自定义 链接 自定义 jupyter --config-dir mkdir custom vim custom/custom.css @font-face { font-family: 'Fira Code'; src: url(\"https://cdn.rawgit.com/dunovank/jupyter-themes/1e851888/jupyterthemes/fonts/monospace/firacode/firacode.otf\") format(\"opentype\"); } .CodeMirror { font-family: 'Fira Code'; font-variant-ligatures: initial; } .cm-string { font-variant-ligatures: none; } 链接 修改字体:http://www.abarbon.com/posts/firacode-font-on-jupyter powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Nginx/":{"url":"common/Nginx/","title":"Nginx","keywords":"","body":"Nginx Nginx 图谱 图谱 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Nginx/install.html":{"url":"common/Nginx/install.html","title":"安装","keywords":"","body":"安装 安装 源码安装 配置启动脚本 通用基础设置 centos 安装 源码安装 - wget http://nginx.org/download/nginx-1.12.1.tar.gz - wget ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/pcre-8.41.tar.gz - yum -y install gcc gcc-c++ autoconf automake make - yum -y install openssl openssl-devel - tar -xzf nginx-1.12.1.tar.gz - tar -xzf pcre-8.41.tar.gz - cd nginx-1.12.1 - ./configure --prefix=/usr/local/nginx --with-pcre=../pcre-8.41 --with-http_gunzip_module --with-http_stub_status_module --with-http_ssl_module - make&&make install - vim /etc/rc.d/init.d/nginx 配置启动脚本 #!/bin/sh # chkconfig: - 85 15 # description: nginx is a World Wide Web server. It is used to serve PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin DESC=\"nginx daemon\" NAME=nginx DAEMON=/usr/local/nginx/sbin/nginx CONFIGFILE=/usr/local/nginx/conf/nginx.conf PIDFILE=/usr/local/nginx/logs/nginx.pid SCRIPTNAME=/etc/init.d/nginx set -e [ -x \"$DAEMON\" ] || exit 0 do_start() { $DAEMON -c $CONFIGFILE || echo -n \"nginx already running\" } do_stop() { kill -INT `cat $PIDFILE` || echo -n \"nginx not running\" } do_reload() { kill -HUP `cat $PIDFILE` || echo -n \"nginx can't reload\" } case \"$1\" in start) echo -n \"Starting $DESC: $NAME\" do_start echo \".\" ;; stop) echo -n \"Stopping $DESC: $NAME\" do_stop echo \".\" ;; reload|graceful) echo -n \"Reloading $DESC configuration...\" do_reload echo \".\" ;; restart) echo -n \"Restarting $DESC: $NAME\" do_stop do_start echo \".\" ;; *) echo \"Usage: $SCRIPTNAME {start|stop|reload|restart}\" >&2 exit 3 ;; esac exit 0 通用基础设置 user nobody; worker_processes 4; worker_cpu_affinity 0001 0010 0100 1000; worker_rlimit_nofile 65535; events { use epoll; worker_connections 65535; } http { include mime.types; default_type application/octet-stream; log_format idy '$remote_addr $remote_port $remote_user $time_iso8601 $request_time $status $body_bytes_sent ' '\"$request\" \"$request_body\" \"$http_referer\" \"$http_user_agent\" \"$http_x_forwarded_for\"'; sendfile on; keepalive_timeout 65; gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.0; gzip_comp_level 2; gzip_types text/plain application/x-javascript text/css application/xml text/vnd.wap.wml; gzip_vary on; open_file_cache max=327680 inactive=20s; open_file_cache_min_uses 1; open_file_cache_valid 30s; proxy_ignore_client_abort on; proxy_set_header Host $host; proxy_set_header X-Scheme $scheme; proxy_set_header X-Real-IP $http_x_forwarded_for; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; client_max_body_size 50m; client_body_buffer_size 256k; proxy_connect_timeout 60; proxy_next_upstream error timeout http_502; proxy_send_timeout 60; proxy_read_timeout 60; proxy_buffer_size 512k; proxy_buffers 8 512k; proxy_busy_buffers_size 512k; proxy_temp_file_write_size 512k; #proxy_buffering off; include vhosts/*.conf; } centos 安装 yum install nginx nginx -s reload -c /etc/nginx/nginx.conf powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Nginx/settings.html":{"url":"common/Nginx/settings.html","title":"配置","keywords":"","body":"配置 配置 文件传输 HTTP TCP 文件传输 nginx传输文件注意文件权限问题 nginx传输文件注意文件大小问题 client_max_body_size 2m; HTTP stream { upstream backend { server 10.10.12.45:80 weight=1; server app.example.com:80 weight=2; } server { location / { proxy_pass http://backend; } } } TCP stream { upstream mysql_read { server read1.example.com:3306 weight=5; server read2.example.com:3306; server 10.10.12.34:3306 backup; } server { listen 3306; proxy_pass mysql_read; } } powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/traefik/":{"url":"common/traefik/","title":"traefik","keywords":"","body":"traefik powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/traefik/install.html":{"url":"common/traefik/install.html","title":"安装","keywords":"","body":"安装 安装 docker 安装 docker 安装 https://hub.docker.com/_/traefik docker pull traefik powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/traefik/project/":{"url":"common/traefik/project/","title":"专题","keywords":"","body":"专题 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/traefik/project/Nginx-to-traefik.html":{"url":"common/traefik/project/Nginx-to-traefik.html","title":"私有服务架构迁移","keywords":"","body":"私有服务架构迁移 私有服务架构迁移 历史架构 老架构存在的问题 迁移后架构 http自动跳转https 自动tls证书 证书验证中间件 历史架构 以两层nginx反向代理 graph TD; A[访问网址]-->B(入口nginx代理); B(入口nginx代理)-->|proxy|C(前端nginx代理); C(前端nginx代理)-->|upstream|D(后端服务); 第一层nginx代理域名到内网服务 upstream server_list { server localhost:8051 } server { listen: 80; server_name: password.moveright.top location / { root html; proxy_pass http://server_list; index index.html index.htm; } } 内网nginx通过nginx-proxy动态代理到后端服务 version: '3.9' services: home: container_name: home hostname: home image: ****** restart: unless-stopped network_mode: \"host\" volumes: - /data/docker/mount/home/logs:/home/logs ports: - \"8000:8000\" working_dir: /home command: uvicorn main:app --host 0.0.0.0 --port 8000 environment: # 这些可以被nginx-proxy获取到，动态修改nginx配置 - VIRTUAL_HOST=47.100.32.32 - VIRTUAL_PORT=8000 extra_hosts: - \"xm_redis:127.0.0.1\" - \"xm_mongo:127.0.0.1\" openapi_passwordlock: container_name: openapi_passwordlock hostname: openapi_passwordlock image: registry-vpc.cn-shanghai.aliyuncs.com/swxs/openapi_passwordlock restart: always volumes: - /data/docker/mount/home/nginx/logs/:/etc/nginx/logs/ # 这边需要获取到docker - /var/run/docker.sock:/var/run/docker.sock ports: - \"8051:80\" command: /etc/docker-gen/docker-gen -config /etc/docker-gen/docker-gen.cfg 前端容器内存在的nginx配置模板，动态创建配置 {{ range $host, $containers := groupBy $ \"Env.VIRTUAL_HOST\" }} upstream {{ $host }} { least_conn; {{ range $index, $value := $containers }} server {{ $value.Env.VIRTUAL_HOST }}:{{ $value.Env.VIRTUAL_PORT }} weight=10 max_fails=1 fail_timeout=2; {{ end }} } server { server_name {{ $host }}; server_tokens off; access_log /etc/nginx/logs/{{ $host }}_access.log common; error_log /etc/nginx/logs/{{ $host }}_error.log; root /data/; index index.html; location /api/ { proxy_pass http://{{ $host }}; proxy_set_header Host $host; proxy_set_header X-Real-Ip $http_x_forwarded_for; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Scheme $scheme; #告知tornado当前使用的是http还是https proxy_redirect default; } location /static/ { access_log off; alias /data/static/; } location / { try_files $uri $uri/ /index.html?$query_string; } location @rewrites { rewrite ^(.+)$ /index.html last; } location /ngx_status { stub_status on; access_log off; } location ~* \\.(?:ico|css|js|gif|jpe?g|png)$ { add_header Pragma public; add_header Cache-Control \"public, must-revalidate, proxy-revalidate\"; } } {{ end }} 老架构存在的问题 第二层nginx, 依赖nginx-proxy, 存在前后端耦合问题, 心智要求较高 迁移后架构 graph TD; A[访问网址]-->B(Traefik代理); B(Traefik代理)-->|proxy|C(前端nginx代理); B(Traefik代理)-->|proxy|D(后端服务); 通过traefik动态负载前后端服务 traefik配置 providers: docker: endpoint: \"unix:///var/run/docker.sock\" exposedByDefault: false entryPoints: web: address: \":80\" 具体服务配置 home: labels: - \"traefik.enable=true\" - \"traefik.http.services.home.loadbalancer.server.port=8000\" - \"traefik.http.routers.home.rule=PathPrefix(`/api`)\" - \"traefik.http.routers.home.priority=100\" - \"traefik.http.routers.home.entrypoints=web\" - \"traefik.http.routers.home.service=home\" openapi_passwordlock: labels: - \"traefik.enable=true\" - \"traefik.http.services.openapi_passwordlock.loadbalancer.server.port=80\" - \"traefik.http.routers.openapi_passwordlock.rule=Host(`password.moveright.top`)\" - \"traefik.http.routers.openapi_passwordlock.entrypoints=web\" - \"traefik.http.routers.openapi_passwordlock.service=openapi_passwordlock\" http自动跳转https traefik配置 entryPoints: web: address: \":80\" http: redirections: entryPoint: to: websecure scheme: https websecure: address: \":443\" http: tls: certResolver: \"letsencrypt\" domains: - main: \"*.moveright.top\" 具体服务配置修改 labels: - \"traefik.http.routers.whoami.entrypoints=websecure\" - \"traefik.http.routers.whoami.tls=true\" 自动tls证书 traefik配置 有三种证书获取方式可选，这边使用dnsChallenge，好处是支持泛域名 certificatesResolvers: letsencrypt: acme: email: \"\" storage: \"/letsencrypt/acme.json\" dnsChallenge: provider: \"alidns\" delayBeforeCheck: 0 修改阿里云配置 添加RAM用户，授予所有域名相关权限 获取AccessKey ID并配置docker-compose.yml环境变量 environment: - ALICLOUD_ACCESS_KEY=*** - ALICLOUD_SECRET_KEY=*** - ALICLOUD_REGION_ID=cn-shanghai 重启等待证书生成 证书验证中间件 具体服务配置修改 labels: - \"traefik.http.routers.dashboard.middlewares=dashboard\" - \"traefik.http.middlewares.dashboard.basicauth.users=swxs:$$apr1$$XN0o2eiY$$mJ4mqPzpIuiiA1VRJ3hLi0\" powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/vscode/":{"url":"common/vscode/","title":"vscode","keywords":"","body":"vscode powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/vscode/project/":{"url":"common/vscode/project/","title":"专题","keywords":"","body":"专题 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/vscode/project/settings.html":{"url":"common/vscode/project/settings.html","title":"常用配置","keywords":"","body":"常用配置 常用配置 系统配置 不禁止运行脚本 launch.json 配置 python 系统配置 不禁止运行脚本 set-executionpolicy remotesigned launch.json 配置 python { // 使用 IntelliSense 了解相关属性。 // 悬停以查看现有属性的描述。 // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"RUN[MAIN]\", \"type\": \"python\", \"request\": \"launch\", \"program\": \"__main__.py\", \"args\": [\"8199\"], \"console\": \"integratedTerminal\", \"justMyCode\": false }, { \"name\": \"RUN[RPC]\", \"type\": \"python\", \"request\": \"launch\", \"module\": \"rpc.main\", \"console\": \"integratedTerminal\", } ] } powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/vscode/plugins/":{"url":"common/vscode/plugins/","title":"插件","keywords":"","body":"插件 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/vscode/plugins/summary.html":{"url":"common/vscode/plugins/summary.html","title":"简介","keywords":"","body":"简介 简介 列表 列表 名称 功能 备注 Chinese (Simplified) Language Pack for Visual Studio Code 简体中文支持 GitLens — Git supercharged git 扩展 Git Blame git 扩展 用GitLens就行 Markdown All in One markdown 扩展 Python python 扩展 Pylance python 扩展 Python Docstring Generator python 注释 Calva: Clojure & ClojureScript Interactive Programming Clojure 扩展 SQL Formatter sql formatter Prettier - Code formatter Js Formatter Winter is Coming Theme 效果优化-主题 vscode-icons 效果优化-图标 Better Comments 效果优化-注释 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/vscode/plugins/Python.html":{"url":"common/vscode/plugins/Python.html","title":"Python","keywords":"","body":"Python Python 配置 配置 \"python.showStartPage\": true, \"python.languageServer\": \"Pylance\", \"python.linting.flake8Enabled\": true, \"python.linting.flake8Args\": [ \"--max-line-length=180\", \"--ignore=E402,F841,F401,E302,E305\", ], \"python.linting.pylintArgs\": [ \"--max-line-length=180\" ], \"python.formatting.autopep8Args\": [ \"--max-line-length=180\" ], powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/vscode/plugins/Python-Docstring-Generator.html":{"url":"common/vscode/plugins/Python-Docstring-Generator.html","title":"Python-Docstring-Generator","keywords":"","body":"Python-Docstring-Generator Python-Docstring-Generator 配置 配置 \"autoDocstring.customTemplatePath\": \"D:\\\\\\\\codehouse\\\\\\\\Collection\\\\\\\\store\\\\\\\\vscode\\\\\\\\python.mustache\" 配置文件 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/sonarqube/":{"url":"common/sonarqube/","title":"SonarQube","keywords":"","body":"sonar powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/sonarqube/install.html":{"url":"common/sonarqube/install.html","title":"安装","keywords":"","body":"安装 安装 docker max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] Java heap space error or java.lang.OutOfMemoryError docker docker-compose.yaml version: \"3\" services: sonarqube: image: sonarqube:community depends_on: - db environment: SONAR_JDBC_URL: jdbc:postgresql://db:5432/sonar SONAR_JDBC_USERNAME: sonar SONAR_JDBC_PASSWORD: sonar volumes: - sonarqube_data:/opt/sonarqube/data - sonarqube_extensions:/opt/sonarqube/extensions - sonarqube_logs:/opt/sonarqube/logs ports: - \"9000:9000\" db: image: postgres:12 environment: POSTGRES_USER: sonar POSTGRES_PASSWORD: sonar volumes: - postgresql:/var/lib/postgresql - postgresql_data:/var/lib/postgresql/data volumes: sonarqube_data: sonarqube_extensions: sonarqube_logs: postgresql: postgresql_data: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 修改配置 # /etc/sysctl.conf vm.max_map_count=262144 sysctl -p sysctl -w vm.max_map_count=262144 Java heap space error or java.lang.OutOfMemoryError Increase the memory via the SONAR_SCANNER_OPTS environment variable when running the scanner from a zip file: export SONAR_SCANNER_OPTS=\"-Xmx512m\" In Windows environments, avoid the double-quotes, since they get misinterpreted and combine the two parameters into a single one. set SONAR_SCANNER_OPTS=-Xmx512m powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/sonarqube/project/":{"url":"common/sonarqube/project/","title":"专题","keywords":"","body":"专题 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/sonarqube/project/python-coverage.html":{"url":"common/sonarqube/project/python-coverage.html","title":"Python用例覆盖率","keywords":"","body":"Python用例覆盖率 Python用例覆盖率 coverage 配置 coverage 使用coverage run 代替 python 生成coverage xml 生成 coverage.xml 文件，用来解析 配置 添加sonar-project.properties配置项 sonar.python.coverage.reportPaths=**/coverage.xml sonar.coverage.exclusions=apps/,rpc/,rpc_apps/,**/*.xml powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Jenkins/":{"url":"common/Jenkins/","title":"Jenkins","keywords":"","body":"Jenkins powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Jenkins/install.html":{"url":"common/Jenkins/install.html","title":"安装","keywords":"","body":"安装 安装 Docker安装 Docker安装 docker pull jenkins sudo mkdir /var/jenkins sudo chown 1000:1000 /var/jenkins sudo docker run -p 8080:8080 -p 50000:50000 -v /var/jenkins:/var/jenkins_home --name my_jenkins -d jenkins powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/":{"url":"common/Book/","title":"读书体会","keywords":"","body":"读书体会 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/words.html":{"url":"common/Book/words.html","title":"生僻字","keywords":"","body":"生僻字 生字 读音 又 [yòu] 双 [shuāng] 叒 [ruò] 叕 [zhuó][yǐ][lì][jué] 彡 [shān][xiǎn] powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/terminology.html":{"url":"common/Book/terminology.html","title":"术语","keywords":"","body":"术语 ELB Elastic Load Balancing - 弹性负载均衡 LB、 EC2 Elastic Compute Cloud - 弹性云计算 ECS Elastic Container Service - 弹性容器服务 RDS Relational Database Service - 关系型数据库服务 RT Reaction Time - 响应时间 QPS Queries-per-second - 每秒查询率 WAL Write-Ahead Logging - 预写日志系统 VCSs 版本控制系统 PVC Persistent Volume Claim - 持久化卷声明 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/Introductory-Combinatorics.html":{"url":"common/Book/Introductory-Combinatorics.html","title":"组合数学","keywords":"","body":"组合数学 组合数学 排列与组合 加法原理 乘法原理 减法原理 除法原理 组合数学所关心的问题就是把某个集合中的对象排列成某种模式，使其满足一些指定的规则。下面时两种反复出现的通用问题： 排列的存在性。当我们想排列一个集合的对象使其满足特定条件时，这样的排列是否存在 排列的列举和分类。当指定的排列可行时，我们需要计数和分类不同类型的排列 组合数学是研究离散构造的存在、计数、分析和优化等问题的一门学科 例子：棋盘的完美覆盖、幻方、四色问题、36军官问题、最短路径问题、相互重叠的圆、Nim游戏 排列与组合 加法原理 $$ |S|=\\left|S{1}\\right|+\\left|S{2}\\right|+\\cdots+\\left|S_{m}\\right| $$ 乘法原理 $$ |S|= p \\times q $$ 减法原理 $$ |A|= |U| - |\\overline{A}| $$ 除法原理 $$ k = \\frac{ |S| }{在一个部分中的对象数目} $$ 定理2.2.1 $$ P(n, r)=\\frac{n !}{(n-r) !} $$ 定理2.3.1 $$ P(n, r) = r !\\left(\\begin{array}{l} n \\ r \\end{array}\\right) $$ 因此 $$ \\left(\\begin{array}{l} n \\ r \\end{array}\\right) = \\frac{n !}{r !(n-r) !} $$ 推论2.3.2 $$ \\left(\\begin{array}{l} n \\ r \\end{array}\\right) = \\left(\\begin{array}{l} n \\ n - r \\end{array}\\right) $$ 定理2.3.3(帕斯卡公式) $$ \\left(\\begin{array}{l} n \\ k \\end{array}\\right) = \\left(\\begin{array}{l} n-1 \\ k \\end{array}\\right) + \\left(\\begin{array}{l} n-1 \\ k-1 \\end{array}\\right) $$ 定理2.3.4 $$ \\left(\\begin{array}{l} n \\ 0 \\end{array}\\right)+\\left(\\begin{array}{l} n \\ 1 \\end{array}\\right)+\\left(\\begin{array}{l} n \\ 2 \\end{array}\\right)+\\cdots+\\left(\\begin{array}{l} n \\ n \\end{array}\\right)=2^{n} $$ 定理2.4.1 设S是有k种不同类型对象的多重集合，每一个元素都有无限重复数。那么，S的r排列的数目是 $$k^{r}$$ 定理2.4.2 设S是多重集合，它有k种不同类型的对象，且每一种类型的有限重复数分别是 $$n{1}, n{2},\\cdots, n_{k}$$ 设S的大小为 $$n = n{1} + n{2} +\\cdots+ n_{k}$$ 则S的排列数目等于 $$ \\frac{n !}{n{1}!n{2}!\\cdots n_{k}!} $$ 定理2.5.1 设S是有k种类型对象的多重集合，每种元素均具有无线的重复数。那么S的r组合的个数等于 $$ \\left(\\begin{array}{l} r + k -1 \\ r \\end{array}\\right) = \\left(\\begin{array}{l} r + k -1 \\ k - 1 \\end{array}\\right) $$ powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/Code-Complete.html":{"url":"common/Book/Code-Complete.html","title":"代码大全","keywords":"","body":"代码大全 代码大全 什么是软件构建 什么是软件构建 定义问题 需求分析 规划构建 软件架构 详细设计 编码与调试 单团测试 集成测试 集成 系统测试 保障维护 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/Refactoring-Improving-the-Design-of-Existing-Code.html":{"url":"common/Book/Refactoring-Improving-the-Design-of-Existing-Code.html","title":"重构","keywords":"","body":"重构 重构 流程 时机 难题 代码的坏味道 重构技术 重新组织函数 对象之间搬移特性 重新组织数据 简化条件表达式 简化函数调用 处理概括关系 大型重构 流程 测试 小步修改 时机 添加功能时 修补错误时 review代码时 难题 数据库 已对外暴露的方法 代码的坏味道 重复的代码 过长的函数 过大类 过长的参数 发散式变化 多种修改会需要一个类进行修改 散弹式变化 一个修改需要在多处进行 依恋情节 一个类经常因为另一个类的变化而变化 数据泥团 经常需要在一起被处理的数据 基本型别偏执 不使用类而是使用基本类型表示 switch 平行继承体系 一个类添加子类需要其他类添加对应子类 冗余类 面向没有的功能 不被使用的特性 暂时值域 只在特定逻辑中才会使用的field 过度耦合的消息链 中间人转手 一个类的方法都委托于其他类 狎昵关系 两个类需要对方的私有属性 不完美的程序库类 纯数据类 只是用了一小部分父类方法 过多的注释 重构技术 重新组织函数 ExtractMethod 函数过长 单独创建函数 InlineMethod 函数过于简单 直接取消函数；可能是为了重新拆分 InlineTemp 临时变量只被使用了一次 ReplaceTempWithQuery 将临时变量写为查询式 为了在外面复用 IntroduceExplainingVariable 将一个复杂表达式替换为变量 SplitTemporaryVariable 一个临时变量在多处被不同的目的使用 RemoveAssignmentsToParaments 修改了参数 ReplaceMethodWithMethodObject 局部变量的使用导致长函数无法拆分 SubstituteAlgorithm 重写部分方法对象之间搬移特性 MoveMethod MoveField ExtractClass 函数职责有多个 InlineClass 函数职责过于简单不完善 HideDelegate 委托会复杂调用的已知内容 RemoveMiddleMan 委托过多 IntroduceForeignMethod IntroduceLocalExtension重新组织数据 SelfEncapsulateField 通过函数取值\\设值 ReplaceDataValueWithObject 基本数据类型 ChangeValueToReference 统一修改 ChangeRefencceToValue 在不会改变的情况下可以简化在分布式\\并发系统中的逻辑 ReplaceArrayWithObject DuplicateObservedData ChangeUniderectionalAssociationToBidirectional 本重构需要测试访问函数 通常让单一那端控制 ChangeBidirectionalAssociationToUniderectional ReplaceMagicNumberWithSymbolicConstant EncapsulateField EncapsulateCollection ReplaceRecordWithDataClass ReplaceTypeCodeWithClass TypeCode没有行为 ReplaceTypeCodeWithSubclass TypeCode有行为， 不会修改 ReplaceTypeCodeWithState\\Strategy TypeCode有行为， 会修改 ReplaceSubclassWithFields 子类没有新特性简化条件表达式 DecomposeConditional 条件过长 整合到一个函数 ConsolidateConditionalExpression 多个条件作用目的相同 ConsolidateDuplicateConditionalFragments 不同条件下相同的执行内容 RemoveControlFlag ReplaceNestedConditionalWithGuardClauses 提前返回 ReplaceConditionalWithPolymorphism IntroduceNullObject 过多的检查对象是否为空 IntroduceAssertion 加断言 目的是为了替代注释进行说明简化函数调用 RenameMethod 修改实现之后名字不在适合 AddParameter RemoveParameter SeparateQueryFromModifier 保证无副作用 ParameterizeMethod 功能相同只有依赖于参数不同的函数， 通过参数整合 ReplaceParameterWithExplicitMethods PreserveWholeObject 如果不希望产生依存就不要用 ReplaceParameterWithMethods IntroduceParameterObject DataClump 通过额外的数据类代替 RemoveSettingMethod 对对象不可修改属性, 不提供setter HadeMethod ReplaceConstructorWithFactoryMethod EncapsulateDowncast ReplaceErrorCodeWithException ReplaceExceptionWithTest 添加测试的目的是为了提供给调用者处理概括关系 PullUpField PullUpMethod PullUpConstructorBody PullDownMethod PullDownField ExtractSubclass ExtractSuperclass ExtractInterface CollapseHierarchy FormTemplateMethod 子类大致相同的逻辑可以提炼为模板方法 ReplaceInheritanceWithDelegation ReplaceDelegationWithInheritance 大型重构 TeaseApartInheritance 梳理并分解继承体系 ConvertProduralDesignToObjects 将过程化设计转换为对象化设计 SeparateDomainFromPresentation 将领域和表述/显示分离 ExtractHierarchy 提炼继承体系 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/Compilers.html":{"url":"common/Book/Compilers.html","title":"编译原理","keywords":"","body":"编译原理 编译原理 引论 引论 编译器 解释器 符号表管理 词法分析 语法分析 语义分析 中间代码生成 代码优化 代码生成 组合 token 标记 context free grammar 上下文无关文法 syntax directed translation 语法制导的翻译 ambiguity 二义 terminals 终结符 non-terminals 非终结符 shift-reduce 移进-规约 Empty Productions 空产生式 Panic mode recovery 悲观恢复模式 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/Seven-Concurrency-Models-in-Seven-Weeks.html":{"url":"common/Book/Seven-Concurrency-Models-in-Seven-Weeks.html","title":"七周七并发模型","keywords":"","body":"七周七并发 七周七并发 并行or并发 锁模型 函数式编程 Future 模型 Promise 模型 Clojure之道——分离标识与状态 资料 并行or并发 并发程序含有多个逻辑上的独立执行块，它们可以独立地并行执行，也可以串行执行。 并行程序解决问题的速度往往比串行程序快得多，因为其可以同时执行整个任务的多个部分。并行程序可能有多个独立执行块，也可能仅有一个。 并发是同一时间应对（dealing with）多件事情的能力； 并行是同一时间动手做（doing）多件事情的能力。 锁模型 线程与锁模型其实是对底层硬件运行过程的形式化。这种形式化既是该模型最大的优点，也是它最大的缺点。 线程与锁模型的最大优点是其适用面很广。它是本书介绍的其他许多技术的基础，适用于解决很多类型的问题。同时，线程与锁模型更接近于“本质”——近似于对硬件工作方式的形式化——正确使用时，其效率很高。 互斥————用锁保证某一时间仅有一个线程可以访问数据，可能导致竞态条件和死锁。 编译器的静态优化可以打乱代码的执行顺序； JVM的动态优化也会打乱代码的执行顺序； 硬件可以通过乱序执行来优化其性能。 Java内存模型定义了何时一个线程对内存的修改对另一个线程可见。基本原则是，如果读线程和写线程不进行同步，就不能保证可见性。 同步的方法： 通过获取对象的内置锁 使用ReentrantLock（可中断、可以设置超时、显示加锁解锁、支持条件变量） 原子变量 让多线程代码安全运行的方法只能是让所有的方法都同步。然而，这也会带来问题。首先这样做效率低下。如果每个方法都同步，大多数线程会频繁阻塞，使程序失去了并发的意义。问题不止于此，当使用多把锁时（Java中每一个对象都有自己的内置锁），线程之间可能发生死锁。 解决死锁的方案： 总是按照一个全局的固定的顺序获取多把锁 避免持有锁时调用外星方法 遍历之前对可迭代对象进行保护性复制，再针对这份副本进行遍历 写入时复制 持有锁的时间应尽可能短 函数式编程 函数式编程与命令式编程（Imperative Programming）不同。命令式编程的代码由一系列改变全局状态的语句构成，而函数式编程则是将计算过程抽象成表达式求值。这些表达式由纯数学函数构成，而这些数学函数是第一类对象，并且没有副作用。由于没有副作用，函数式编程可以更容易做到线程安全，因此特别适合于并发编程。 不使用可变状态 引用透明性 Future 模型 Promise 模型 Clojure之道——分离标识与状态 资料 非公平锁与公平锁: https://www.jianshu.com/p/f584799f1c77 虚假唤醒（spurious wakeup）https://www.jianshu.com/p/0eff666a4875 Fork-join模型https://zh.wikipedia.org/wiki/Fork-join%E6%A8%A1%E5%9E%8B 阿姆达尔定律: https://zh.wikipedia.org/wiki/%E9%98%BF%E5%A7%86%E8%BE%BE%E5%B0%94%E5%AE%9A%E5%BE%8B powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/Pro-Git.html":{"url":"common/Book/Pro-Git.html","title":"Pro Git","keywords":"","body":"Pro Git Pro Git 简介 直接记录快照，而非差异比较 三种状态 配置 GIT 基础 GIT 分支 简介 直接记录快照，而非差异比较 在 Git中，每当你提交更新或保存项目状态时，它基本上就会对当时的全部文件创建一个快照并保存这个快照的索引。为了效率，如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件。 Git 对待数据更像是一个 快照流。 三种状态 现在请注意，如果你希望后面的学习更顺利，请记住下面这些关于 Git 的概念。 Git 有三种状态，你的文件可能 处于其中之一： 已提交（committed）、已修改（modified） 和 已暂存（staged）。 已修改表示修改了文件，但还没保存到数据库中。 已暂存表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。 已提交表示数据已经安全地保存在本地数据库中。 这会让我们的 Git 项目拥有三个阶段：工作区、暂存区以及 Git 目录。 配置 --system: 系统级 --global: 全局级 --local: 仓库级 git config --list --show-origin git config --global user.name git config --global user.email # 查看配置 git config --list git config GIT 基础 命令 用途 git init 初始化GIT仓库 git clone [] 获取远程GIT仓库 git status 检查状态 git add 跟踪新文件 git add 暂存已修改的文件 git diff 查看修改内容 git commit 提交更新 git rm 移除文件 git log 查看提交历史 git commit --amend 合并当前暂存内容与上次提交作为一次新提交 git reset HEAD 取消暂存的文件 git checkout -- 撤消对文件的修改 git remote 查看远程仓库 GIT 分支 Git 仓库中有五个对象：三个 blob 对象（保存着文件快照）、一个 树 对象 （记录着目录结构和 blob 对象索引）以及一个 提交 对象（包含着指向前述树对象的指针和所有提交信息）。 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/Influence-The-Psychology-of-Persuasion.html":{"url":"common/Book/Influence-The-Psychology-of-Persuasion.html","title":"影响力","keywords":"","body":"影响力 影响力 互惠和退让 互惠的触发与获取都不需要对方同意 如何利用 如何防范 承诺和一致 从小的改变开始无法修改立场 思考是不是最开始的立场了 社会认同 从重心理 很多人在场会导致无法实行正确的决策 喜好 外在的影响很大 权威 权威的影响很大 短缺 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/The-Effective-Executive.html":{"url":"common/Book/The-Effective-Executive.html","title":"卓有成效的管理者","keywords":"","body":"卓有成效的管理者 卓有成效的管理者 卓有成效是可以学到的 为什么需要卓有成效的管理者 什么样的人才算是管理者 管理者必须面对的现实 作为—个卓有成效的管理者，必须在思想上养成如下五种习惯 掌握自己的时间 我能作哪些贡献 如何充分发挥自身的优势 要用人所长 如何调动上司的积极性 充分发挥自己的长处 重要的事情先做 决策的要素 有效的决策 读后思考 部分练习 卓有成效是可以学到的 为什么需要卓有成效的管理者 社会变革：体力劳动->脑力劳动 我们无法对知识工作者进行严密和细致的监督。我们可以帮助他们，但是他们必须自己管理自己，自觉地完成任务，自觉地作出贡献，自觉地追求工作效益。 知识工作者的工作动力取决于他的工作效益，取决于他在工作中是否能有所成就。 什么样的人才算是管理者 管理者->(负责决策:促进机构有效运转)->\"管理者\"(知识的权威/职务的权威) 管理者必须面对的现实 管理者的工作时间往往只属于别人，而不属于自己。 管理者往往被迫按照“老一套办法”开展工作，除非他们敢于采取行动来改变他们周围的一切。(管理者需要的是一些标准，以帮助他识别哪些工作对他真正重要，哪些工作可以帮他提高效益，哪些工作有利于他多作贡献) 是因为他身处机构之内 管理者身处组织之内，受到组织之局限 作为—个卓有成效的管理者，必须在思想上养成如下五种习惯 卓有成效的管理者必须懂得如何有效地利用他们的时间。 卓有成效的管理者重视对外界的贡献。 卓有成效的管理者善于利用长处，不光善于利用他们自己的长处，而旦也知道如何利用上司、同事及下属的长处。他们还善于抓住形势提供的机会做他们想做的事。 卓有成效的管理考知道如何将自己的精力集中在一些重要的领域里。 最后，卓有成效的管理者善于做出有效的决策。 掌握自己的时间 要使用好他的时间，他首先必须要知道自己的时间实际上是怎么花掉的。 至少每年两次、每次3—4个星期要亲自来做自己的工作记录 是要对时间进行有序的管理 砍掉浪费时间的活动 必须用好自己可以支配的时间。他知道自己需要的是整块的时间，零答碎碎的时间是派不上用场的。 我能作哪些贡献 重视贡献便能使管理者的注意力从自己狭隘的部门、专业及技能转移到整个机构的经营业绩上来．使他更加重视外部世界。只有外部世界才是产生效益的地方。 知识分子有责任让别人了解自己。需要了解别人需要什么、发现了什么以及能理解些什么。 正确的人际关系的基本要求：互相沟通，共同协作，自我提高，培养他人。 清楚开会、汇报、介绍情况应该达到什么目的 根据经验，下属给自己定的目标往往与上级想的不一样。这是因为下属和新手看问题的角度与他们的上司不一样。 下属越是能干，就越愿意自已承担责任，对客观现实、机会及需要的看法和认识与上级的差距也就越大，他的结论与上级的期望往往是明显地对立的。 重视贡献可以帮助管理者解决面临的一个重要问题：可以让你在一团乱麻似的事务中理出轻重缓急来。以贡献为主就会给你一条安排事情的原则，使各项工作获得一种相关性。 如何充分发挥自身的优势 须懂得如何充分地使用一切现有的力量，包括周围同事的力量、上级的力量以及自己的力量。 要用人所长 在设计职位时，得要非常小心，都设置得合情合理 是确保每个职位既有很高的要求，又有较宽广的范围。 在决定将某人安置到某个职位上去之前，需要对其条件进行充分的考虑 若想利用某人的长处，那你也得容忍他的短处。 真正可以评估的倒是工作成效，真正应该去评估的也是工作成效。 如何调动上司的积极性 要抓住上司的长处，了解他们可以起什么作用。它要求人们把注意力放到上司的优势和长处上 充分发挥自己的长处 敢于树立正确的态度 重要的事情先做 会集中机构的一切资源以及他们自己的时间和精力，坚持把重要的事情放在前面先做，每次做完一件事情。 善于摆脱已经不再有价值的过去 对哪些事情需要优先处理，哪些问题可以缓一缓，管理者必须作出明确决断 1.要看将来，不能只看过去； 2.要重视机会，不能只看到存在的问题， 3.要选择自己的方向，不能只赶浪头，人云亦云； 4.目标要高，要有新意，不能只求“安全”和方便。 决策的要素 1．思想明确。如果问题是一般性的，那么就只能通过一项确立规则或原则的决策来加以解决； 2．划定界限。划定在解决问题时必须要满足的界限．这就是所谓的“界限条件”； 3．反复推敲。必须反复推敲什么是“对的”，解决问题的方案必须满足哪些条件。只有将这些问题考虑清楚之后，才可以考虑采取适当的妥协、让步以及适度变动等一系列其他措施。采取那些措施的目的，就是为了使决策能被大家所接受； 4．落实措施，让决策变成可以被贯彻的行动； 5．重视“反馈”，以验证决策的正确性和有效性。 有效的决策 机构需要各种类型的人才，否则就会缺乏应变能力，会失去表达不同意见的能力 决策必须听取不同的意见 读后思考 本书主要讨论了作为—个卓有成效的管理者，必须在思想上养成的五种习惯。所有习惯的基础是效率优先（以正确的方式来使用时间）；所有习惯的终点是有所输出（发挥组织所有人员的长处，以向外提供价值） 其中有部分方法论可以进行实践，例如可以统计目前对时间的使用，确认是否可以改进；也有部分内容需要在更长期的实践过程中才能有所练习 部分练习 时间使用明细 日期 时间 内容 220713 10:00~11:00 评审会 220713 11:00~12:00 code review 220713 13:30~14:00 问题修复 220713 14:00~14:20 核酸 220713 14:20~15:00 功能开发 220713 15:00~16:10 评审会 220713 16:10~16:30 功能开发 220713 16:30~17:00 评审会 220713 17:00~19:20 排查内存问题 进行问题： 对时间的统计过于宽泛，不能准确对应实际使用情况，可能要寻找一种更好的方式进行统计 例如在实际情况中，大块的工作会被经常性打断，而零散的打断无法被有效统计 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/Measure-What-Matters.html":{"url":"common/Book/Measure-What-Matters.html","title":"这就是OKR","keywords":"","body":"这就是OKR 这就是OKR OKR的基本特征与实践 聚焦 承诺 协同 连接 责任追踪 挑战不可能 CFR 对话 反馈 认可 文化 总结 OKR的基本特征与实践 OKR(Objectives and Key Results): 目标与关键结果法 目标就是你想要实现的东西，不要将其夸大 或缩小。根据定义，目标应该是重要的、具体的、具有行动导向并且能 鼓舞人心的。如果设计合理并且实施得当，目标能够有效地防止思维和执行过程中出现模糊不清的情况 关键结果是检查和监控我们如何达到目标的标准。有效的关键结果 应该是具体的、有时限的且具有挑战性的，但又必须是能够实现的。最重要的是，它们必须是可衡量、可验证的。 利器1——对优先事项的聚焦和承诺：高绩效组织应该聚焦重要的工作，同时清楚什么是不重要的。领导层面临艰难抉择时，OKR可推动其做出选择。对于部门、团队和个人来说， OKR是一种精准沟通的工具，能消除困惑，让我们进一步明确目标，聚焦到关键的成功要素上。 利器2——团队工作的协同和联系：OKR 具有透明性，上自首席执行官，下至一般员工，每个人的目标都是公开的。每个员工都将个人目标与公司计划紧密地联系起来，进而明确两者之间的依赖关系，并与其他团队展开通力协作。这种自上而下的协同，将个人贡献与组织成功联系起来，为工作赋予了特定的意义。自下而上的OKR，则通过加深员工的主人翁意识，促进了个人的参与和创新。 利器3——责任追踪：OKR是由数据驱动的。 定期检查、目标评分和持续的重新评估可以让OKR充满生机——所有这一切都是基于客观、负责的精神。危险的关键结果会引发某些行动，应使其回到正轨，或者在必要时对其进行修改或替换。 利器4——充分延展进而挑战不可能：OKR激励我们不断超越之前设定的各种可能，甚至超出我们的想象力。通过挑战极限和允许失败，OKR能够促使我们释放出最具创造力和雄心的自我。 聚焦 3~5个主要目标, 需要明确具体 承诺 管理者必须公开对其目标做出承诺，并一以贯之; 当目标不合适之后可以放弃 协同 目标应该是透明的, 从上而下对目标层层分解, 相互合作; 同时也鼓励目标自下而上地涌现 连接 上下之间的合作以外还有跨部门的合作 责任追踪 需要明确得对目标的完成情况进行衡量 挑战不可能 目标应该是有挑战的 CFR 对话（Conversation）：经理与员工之间真实的、高质量的交流， 旨在对绩效提升起到驱动作用。 反馈（Feedback）：同事之间面对面进行双向沟通或通过网络进行交流，以评估工作进展情况并探讨未来的改进方向。 认可（Recognition）：根据个体所做贡献的大小施以对等的表彰。 对话 领导同员工之间持续跟进的前瞻性对话，通常以五个问题为中心： 你正在做什么？ 你做得怎么样？你的OKR进展如何？ 你的工作有什么阻碍吗？ 你需要我提供什么来帮助你实现目标？ 你需要什么帮助来实现你的职业目标？ 管理者和下属之间的一对一定期会谈对工作改进有很大的价值。 它应该被看作一个以下属为主导的会议，会议的内容和整 个基调是由下属决定的，而主管的作用是倾听并做出指导。 反馈 反馈是一种基于观察和经验的意见，可以帮助我们了解自己给别人留下的印象。 反馈可以是非常有建设性的，但前提是它必须足够具体。 认可 现代的认可是基于绩效表现和员工横向之间的对比 鼓励同事间认可 建立明确的标准 分享有利于增加认同感的故事 提高认同发生的频率和可获得性 对公司目标和战略的认同 文化 坚持公司文化价值观的员工将在各种相似的条件下表现出一致的行为，这就意味着管理者无须担心由常规制度、程序和规章等带来的低效率。管理需要发展和培养共同的价值观、目标和构建信任的方法。那么我们该如何做呢？一种方式是以口头或文字的形式传达，另外一种更重要的方式则是树立标杆。 总结 对优先事项的聚焦和承诺 为自己的OKR循环设置合适的节奏。我推荐双重追踪，即季度OKR（用于短期目标）和年度OKR（用于长期策略）并行部署。 为了制订实施计划并加强领导者的承诺，在OKR推出阶段中应该以高层管理为主。在征召个别员工加入之前，让这个过程获得动力。 指定一个OKR领头人，确保每个人在每个周期中花一定的时间来选择什么最重要。 在每个循环周期，需要承诺完成3~5个最高目标。太多的OKR会淡化和分散员工的努力。通过决定不做什么，放弃、推迟或相应减少什么来提升有效性。 在选择OKR时，要尽量寻找那些对杰出绩效最有影响力的目标。 在组织的使命宣言、战略计划或由领导者明确的广泛主题中寻找设定最高层OKR的依据。 强调部门目标并争取横向支持，需要把OKR提升到公司层面。 每个目标的关键结果都不要超过5个，而且这些关键结果是可以衡量的、明确的及有时间限制的。关键结果也即目标如何实现？根据定义，完成了所有关键结果就等于实现了目标。 为了平衡和质量控制，将定性和定量关键结果进行匹配。 当某个关键结果需要额外关注时，将其提升为一个或多个周期的目标。 OKR成功的唯一最重要元素，是组织领导者的信念和支持。 团队工作的协同和联系 通过展示团队的目标与领导者的愿景及公司首要任务的联系来激励员工。实现卓越运营的捷径是透明的、公开的目标，员工直至首席执行官均是如此。 在全体大会上解释为什么OKR对于组织如此重要。然后，不断重复这个重要的信息，直到你自己都感到厌烦为止。 当部署由高层驱动的垂直层级OKR时，应欢迎一线员工的加入，与其针对关键结果进行相互交流并适当接受他们的意见。创新往往产生于公司的基层边缘，而很少产生于权力中心。 鼓励公司内自下而上的OKR应占合适比例（大约一半）。 通过将团队与横向的共享OKR联系起来，打破部门间的隔阂。跨部门、跨职能的操作使快速和协调的决策成为可能，而这是获取竞争优势的基础。 明确所有横向的、跨功能的依赖关系。 当修改或删除OKR目标时，请确保所有利益相关者都了解这一情况。 责任追踪 建立一种问责文化。植入持续的重新评估和诚实客观的分级制度，并且从顶层开始。当领导者公开承认他们的错误时，员工就会更轻松地承担犯错的风险。 要少用外在奖励来激励员工，更多采取公开的、切实的措施来衡量他们的成就。 为了使OKR具有及时性和相关性，指定专人来进行定期检查和进度更新。经常性检查使得团队和个人能够及时纠正错误或快速放弃。 为了保持高绩效，每周都鼓励员工与管理者之间进行一对一的OKR会议，以及召开每月的部门会议。 随着环境条件的改变，只要适合组织的发展，可随时更改、添加或删除OKR指标，即使是在考核周期的中期阶段。谷歌公司有句名言：目标不是写在石头上的（目标并非一成不变的）。如果固执地坚持不相关或不可能实现的目标，只会适得其反。 在考核周期结束时，使用OKR等级加上主观自我评估法来评估过去的表现、庆祝业绩的取得，以及对未来进行规划和改进。在进入下一个考核循环周期之前，花点时间反思上一个周期完成的工作。 为了让OKR紧跟时代潮流，需要投资建设一个专用的、自动化的、基于云的平台。公共、协作、实时的目标设置系统最为有效。 充分延展进而挑战不可能 在每个周期的开始，区分一下必须达到百分之百的目标（承诺型OKR）和那些要非常努力才能完成的目标（BHAG或愿景型OKR）。 建立一种自由的环境。在这种环境下，允许每个人出现失败，而无须受到批判。 为了刺激人们解决问题并激励其取得更大的成就，要设定一些挑战性的目标——即使这意味着有些季度目标可能无法完成。但是，不要把目标门槛设得太高，以至OKR变得不现实。当人们知道他们不可能完成目标时，士气将会受到影响，甚至低落。 要想在生产力或创新上取得飞跃，请遵循谷歌公司的“10倍速”原则，并且用指数级来替换增量式的OKR。这就是企业被颠覆、品类被推新的原因。 设计具有延展性的OKR以符合组织的文化。一个公司的最佳“拓展”界限会随着企业发展周期的不同需求而有所变化。 当一个团队没能完成延展性OKR，假如目标仍是相关的，需要考虑把目标转到下一个考核周期。 持续性绩效管理 为了解决变成真正问题之前的“问题”，并为陷入困境的员工提供所需的支持，将年度绩效管理转为持续性绩效管理。 把前瞻性的OKR与事后反馈的年度评价区分开来，有助于实现那些野心勃勃的目标。将目标达成与奖金支票等同起来，会招致欺诈和规避风险的行为。 用透明的、基于强度的、多维度的绩效评估取代竞争性评级和员工排名。在这些数字背后，考虑员工的团队合作能力、沟通情况和目标设定的雄心。 依靠内在动机来激励员工，如提供有目的的工作和成长机会，而非单纯的财务激励，这些激励要素的作用将更为强大。 为了强化积极的商业成果，在制定结构性目标的同时，贯彻正在实施的CFR计划。透明的OKR使得指导变得更加具体而有效。持续的CFR计划保证每天的工作准时完成，并促进员工之间的真诚合作。 在管理者和员工之间的绩效驱动沟通中，允许员工设置工作计划，而管理者的角色则是学习和指导。 通过两种方式进行绩效反馈，一种是临时性的反馈，另一种则是多方向的、不受组织架构约束的反馈。 使用匿名“动向”调查，对专项工作或员工士气进行实时反馈。 在跨职能的OKR中，通过点对点的反馈，加强团队与部门之间的联系。 利用同行的认可来提升员工参与度和绩效。为了获得最大的影响，绩效的识别应该是频繁、具体及高度可见的，并与顶层的OKR绑定在一起。 文化的重要性 让顶层的OKR与组织的使命、愿景及核心价值保持一致。 通过语言传达文化价值固然重要，但最重要的是通过行动来实现企业文化的价值。 通过协作和问责来提升最佳绩效。OKR是总体目标，把这些关键结果分配到个人，并让他们对其负起责任。 为了发展高激励文化，在支持工作行为的“催化剂”（OKR）和人与人之间的支持行为甚至随机善举这样的“营养液”（CFR）之间构建平衡。 使用OKR，提高透明度、清晰度、目的性和大局方向；开展CFR，培养积极、热情、拓展性思维，并每天都有所改进。 在执行OKR之前，要注意解决文化障碍的必要性，尤其是其中的问责和信任问题。 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/The-One-Minute-Manager.html":{"url":"common/Book/The-One-Minute-Manager.html","title":"一分钟经理人","keywords":"","body":"一分钟经理人 一分钟经理人 管理风格的不同结果 一分钟目标 一分钟称赞 前半分钟 停顿一会儿 后半分钟 一分钟批评 前半分钟 停顿一会儿 后半分钟 管理风格的不同结果 不同的性格特点会形成不同的管理风格。 铁腕的经理人，他们会严格要求下属，强调工作结果和利润。会给下属安排高强度的工作任务，这就会导致离职率比较高，但是很多上司认为他们是不错的经理人，因为可以给企业带来不错的业绩。 好心的经理人，他们很关心自己的下属，很有人情味，做事民主，员工比较喜欢这样的经理人。但效率看起来要比铁腕经理人低，业绩也不那么好。 如何平衡处理？ 一分钟目标 目标不是领导给员工单方面布置的，一个目标的制定过程，应该是经理人和下属、同事一起商讨达成一致的。其次，经理人制定目标，还必须理解，一个好的目标要包括三部分内容：那就是目标本身、衡量标准和完成期限。 对于相对复杂的重要目标，要把每个目标单独用一页纸来写下，在这页纸上把目标、衡量标准和截止时间都用几句话说明白。每个目标都花大概１分钟的时间就能看完。 通过一分钟目标这样的工具把预期清晰化，才能够减少不必要的浪费，经理人还必须要通过清晰的目标管理好大家彼此的预期。这样做可以让员工自己管理自己，可以经常反思自己做的对不对，而不是等经理人告诉你。这样做工作进度和效率都会有很大的提高。 好的目标管理方法就能让员工自己管理自己，进一步让项目经理人能够腾出时间来思考更重要的事情。所以，作者认为作为经理人第一个要培养的能力，就是构建一分钟目标的管理能力，核心是和员工之间达成统一明确的工作预期。这样无论是经理人要帮助员工，还是员工自己完成工作，都有了统一的标准。 一分钟称赞 如果达成了目标，就需要进行一分钟称赞了。 应当在工作中去主动发现员工做对了的事情，可能一开始他们只是在大方向上做对了一些事儿，你就应该给出鼓励。鼓励他在后面一点一点改进，一点一点完美，关键是你要在员工做得正确的时候，及时地给出正面的反馈。 前半分钟 一开始的时候，要及时称赞下属。而且要告诉他们对在哪里，要说得非常具体。并且告诉他们这件事情做对之后，会让你感到多么高兴，对整个团队和其他同事又会有多么大的帮助。 停顿一会儿 在这个部分，你要沉默一会，让他们静静的体会做对事带来的喜悦。 后半分钟 要趁热打铁，给员工建立以后做得更好的信心，鼓励他们以后继续这样做。明确说明你对他们有信心，并会支持他们获得成功。 一分钟批评 当员工没有按预期的目标完成任务以后，我们要给他做出反馈，让他去改进。工作出了差错，不更正只称赞于事无补。当然谁都不喜欢经常被人指出错误，但一次更正可以帮我们重回工作轨道并完成工作目标，公司和个人才能实现双赢。 我们的目的是帮助人们建立自尊自信，而不是贬低他们的价值 前半分钟 在一开始，当错误发生后就要立刻进行更正。然后对事不对人，确认既有的事实，分析错在哪里，并且要说得非常具体。告诉他们这件事带给你的感受，以及对工作成果可能造成的影响。 停顿一会儿 告诉下属错在哪里之后，就是沉默几秒钟，让员工理解和感受审视他们自己犯的错。 后半分钟 在这个时候你得聚焦在人身上，对人不对事。这个过程你要告诉对方，其实他的实际能力比表现出来的要强，即使犯了错但是你对他们是有信心的，你仍然信任他，希望他能够改正错误，之后做得更好。 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/働き方.html":{"url":"common/Book/働き方.html","title":"干法","keywords":"","body":"干法 干法 为什么要工作 如何投入工作 持续付出不亚于任何人地努力 抓紧今天这一天 怎样才能出色工作 每天都要钻研创新 既然定了计划，就一定要实现 乐观构思、悲观计划、乐观实行 人生*工作的结果 = 思维方式 X 热情 X 能力 思考 为什么要工作 人工作的目的是为了提升自己的心志，工作能够锻炼人性、磨练心志，工作是人生最尊贵、最重要、最有价值的行为 认真工作、努力获得回报，才能让你感受到人生的快乐和时间的可贵 社会的进步 -> 社会逐步西方化 -> 人性中好逸恶劳的倾向 如何投入工作 与其寻找自己喜欢的工作，不如先喜欢上已有的工作，脚踏实地，从眼前开始 当工作进展顺利时，要直率地表达出快乐 必须要有明确地目标 持续付出不亚于任何人地努力 高目标就是促使个人和组织进步地最大动力 不去想，不认真思考，就什么都实现不了。 抓紧今天这一天 与其中途就要作废，不如一开始就不要建立。我只用心于建立一年的年度经营计划 在建立目标时，要设定“超过自己能力之上的指标”; 要相信经过努力学习，获取知识，掌握技术，人的能力就会增长 当项目遇上难以克服的困难，认为“已经不行了”的时候，其实并不是终点，而是重新开始的起点。在成功之前，要绝不罢休，不屈不挠，坚韧不拔；不能给自己设置界限，要不厌其烦，持续挑战。这样才有可能变“危机”为“机会”，让“失败”转为“成功”。 怎样才能出色工作 必须注重细节 比道理更重要的是重视经验 不厌其烦，持续、专业地工作 想要成就某项事业，就应该时时描绘这一事业的理想状态。同时，对于实现这个理想地过程也要反复思考，知道“看得见”这个过程为止。 追求“完美”而非“最佳” 每天都要钻研创新 既然定了计划，就一定要实现 乐观构思、悲观计划、乐观实行 人生*工作的结果 = 思维方式 X 热情 X 能力 思考 通过本书可以看到东西方的哲学思辨，西方以人性本恶，通过完善制度法规以此规范人的行为，东方以人性本谦，通过相信人的能力来激励人的行为。然而作者也看到，社会的逐步西化，人性的逐渐浮躁，作为一种社会现象深刻地改变了大众的思潮。 但是我们也能看出一些相似的东西，首先就是要有确定的目标，而且目标最好是有部分超出当前能力的；其中本书明确了超出能力的目标的依据是要相信人的努力会带来能力的进步。 此外当我们取得小小的成功时需要对次感到喜悦，这与《一分钟管理人》中推荐的积累小的奖励也不谋而合 是故我们可以这样考虑，东西方的成功经验看似分道扬镳其实殊途同归；他们的底层逻辑截然不同，但他们取得成功的方法论是相同的。在东方文化中，人应该对自己进行修炼，首先以正确的方式管理自己本身，以此影响他人；在西方文化中，需要有一个管理人地制度，以此限制人的行为在正确的道路上 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/Trust-Factor.html":{"url":"common/Book/Trust-Factor.html","title":"零内耗-打造一支彼此信任的高效团队","keywords":"","body":"零内耗-打造一支彼此信任的高效团队 信任的力量 被信任的程度越高，产生的催产素越多 将外在激励转化为内在动力 影响团队成员之间相互信任的8种因素 OXYTOCIN: O: Ovation-喝彩 X: eXpectation-期望 Y: Yield-高产 T: Transfer-放权 O: Openness-开放 C: Caring-关爱 I: Invest-投资 N: Natural-自然 打造一支零内耗的高效团队 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Book/How-Money-Became-Dangerous.html":{"url":"common/Book/How-Money-Became-Dangerous.html","title":"危险的金钱","keywords":"","body":"危险的金钱 危险的金钱 电子表格 电子表格 计算机电子表格的引入释放了金融业的创造力，同时有助于消除人类的主观性和偏见。然而计算机电子表格导致了对分析真实性的侵蚀和品质的缺失。 从程式中去除对道德品质的评估，使得金融世界失去了人性。 powered by GitbookFile Modify: 2023-05-10 09:34:39 "},"common/Tool/":{"url":"common/Tool/","title":"工具","keywords":"","body":"工具 工具 FastStone FastStone MaxView FastStone Capture calibre-ebook Just Manager 向日葵 SourceTree Visual Studio Code CPU-Z AIDA64 Extreme Python NodeJS Clojure 7-zip Foxmail GIT redis mongodb Postman Apifox xmind freemind docker chromedriver FastStone An image browser, converter and editor 图片查看器，会创建索引 下载: https://www.faststone.org/FSIVDownload.htm FastStone MaxView A fast, compact and innovative image viewer 图片查看器， 可以查看压缩包内图片 下载: https://www.faststone.org/FSMaxViewDownload.htm FastStone Capture screen capture tool 截图工具 下载: https://www.faststone.org/FSCaptureDownload.htm calibre-ebook a powerful and easy to use e-book manager 电子书管理 下载: https://calibre-ebook.com/download_windows64 Just Manager a feature packed, versatile, multi-tabbed multi-pane file manager for the Windows operating system 文件系统辅助 下载: http://justmanager.ru/downloads/ 向日葵 远程管理 下载: https://sunlogin.oray.com/download/ SourceTree Simplicity and power in a beautiful Git GUI 源代码管理辅助 下载: https://www.sourcetreeapp.com/ Visual Studio Code Free. Built on open source. Runs everywhere. 文本编辑器 下载: https://code.visualstudio.com/ CPU-Z gathers information on some of the main devices of your system 下载: https://www.cpuid.com/softwares/cpu-z.html AIDA64 Extreme system information tool 下载: https://www.aida64.com/downloads Python Python 下载: https://www.python.org/downloads/ MiniConda 下载: https://docs.conda.io/en/latest/miniconda.html C++ build tools 下载: https://download.microsoft.com/download/2/E/6/2E61CFA4-993B-4DD4-91DA-3737CD5CD6E3/vcredist_x64.exe https://www.microsoft.com/en-ph/download/confirmation.aspx?id=40784 http://go.microsoft.com/fwlink/?LinkId=691126 https://download.visualstudio.microsoft.com/download/pr/3e542575-929e-4297-b6c6-bef34d0ee648/639c868e1219c651793aff537a1d3b77/vs_buildtools.exe NodeJS NodeJS 下载 https://nodejs.org/zh-cn/ Clojure Clojure 下载 https://github.com/clojure/tools.deps.alpha/wiki/clj-on-Windows 7-zip a file archiver with a high compression ratio. 解压缩软件 下载: https://www.7-zip.org/ Foxmail 邮件管理 下载: https://www.foxmail.com/ GIT 版本管理 下载: https://git-scm.com/download/win redis 下载: windows: https://github.com/microsoftarchive/redis/releases mongodb mongod、mongo数据处理工具(mongodump, mongorestore)、robo3T 下载: https://www.mongodb.com/try/download/community?tck=docs_server https://www.mongodb.com/try/download/database-tools https://robomongo.org/download Postman 下载: https://www.postman.com/downloads/ Apifox 下载: https://apifox.com/ xmind 下载: https://www.xmind.net/download/ freemind 下载: http://freemind.sourceforge.net/wiki/index.php/Download https://sourceforge.net/projects/freemind/files/latest/download docker docker on windows 下载: https://hub.docker.com/editions/community/docker-ce-desktop-windows chromedriver 下载: https://chromedriver.chromium.org/ powered by GitbookFile Modify: 2023-05-10 09:34:39 "}}